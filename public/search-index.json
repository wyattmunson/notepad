[{"content":"Kubernetes Kubernetes is a container orchistration platform.\nContents Core concepts kubectl commands Kubectx \u0026amp; Kubens Secrets - external secrets Core Concepts Deployment Set things like:\nreplicas Update strategy: rolling update, ect. environment variables, Image location Healthcheck and liveliness probe endpoints Pods Like a pod of whales, one or more containers that should be treated as a single application. A pod encapsulates application containers, storage resrouces, a unique network Id, and configuration on how to run the containers.\nService Logical set of pods that acts as a gateway for the client to send requests to the pods. Pods are volatile in k8s. The replication controller may kill pods. Service abstracts the finding of the physical pods and acts as a gateway.\nSet configurations like what port the service listens on and what port that maps to the port the container is listening on.\nNamespace Analgous to an Amazon VPC, a namspace is a virtual cluster. A physical cluster can run multiple namespaces (virtual clusters) to isolate components, projects, or secure access.\nKubectx \u0026amp; Kubens Kubectx is a command line tool to help you quickly swtich clusters. Use Kubens for namespaces.\nInstall kubectx here: https://github.com/ahmetb/kubectx\nUse kubectl here:\nUSAGE: kubectx : list the contexts kubectx \u0026lt;NAME\u0026gt; : switch to context \u0026lt;NAME\u0026gt; kubectx - : switch to the previous context kubectx \u0026lt;NEW_NAME\u0026gt;=\u0026lt;NAME\u0026gt; : rename context \u0026lt;NAME\u0026gt; to \u0026lt;NEW_NAME\u0026gt; kubectx \u0026lt;NEW_NAME\u0026gt;=. : rename current-context to \u0026lt;NEW_NAME\u0026gt; kubectx -d \u0026lt;NAME\u0026gt; [\u0026lt;NAME...\u0026gt;] : delete context \u0026lt;NAME\u0026gt; (\u0026#39;.\u0026#39; for current-context) (this command won\\\u0026#39;t delete the user/cluster entry that is used by the context)\rSecrets (ExternalSecrets Method) NOTE: This requires the GoDaddy External Secrets for K8s is installed, you can see the yaml kind is ExternalSecret.\nStore secret in AWS Secrets Manager Write ExternalSecret k8s yaml to reference AWS Secret and deploy to k8. The cluster will automatically fetch the secret(s) from AWS and give them a name you specify Reference secret key/name You can store multiple secrets under main secret. In the example below, connectionString is a secrets stored under the parent webserver-secret.\nCreate ExternalSecret yaml\napiVersion: \u0026#34;kubernetes-client.io/v1\u0026#34; kind: ExternalSecret metadata: name: webserver-secret # name in kubernetes namespace: finance-qa secretDescriptor: backendType: secretsManager data: - key: prod/rds/connectionString # name in AWS secrets manager name: connectionSting # key for secret in kubernetes - key: prod/additional/secret name: anotherApiKey\rThis creates a secret key of webserver-secret that is equal to a key/value pair of the secrets pulled into AWS. You can sort of represent it with the JSON below:\n{ \u0026#34;webserver-secret\u0026#34;: { \u0026#34;connectionString\u0026#34;: \u0026#34;postgres:password//posgres@172.0.0:5432/database\u0026#34;, \u0026#34;anotherApiKey\u0026#34;: \u0026#34;asdfef;asldkf;eSECRET_TEXTlskdjfla\u0026#34; } }\rReference secret in deployment yaml\nNow that the secret is populated and named in k8s, you can reference it like you would normally.\n--- . . . env: - name: DB_CONNECTION_STRING valueFrom: secretKeyRef: key: connectionString name: webserver-secret\rkubectl \u0026amp; secrets # related kubectl commands $ kubectl apply -f secret.yml -n some-namespace $ kubectl delete secret secret-name -n some-namespace # Get a list of all secret names in a namespace $ kubectl get secrets -n some-namespace # Get details of a particular secret $ kubectl get secrets webserver-secret -n some-namespace -o yaml # This will return the base64 encoded secrets $ echo \u0026#39;SECRET_TEXT\u0026#39; | base64 --decode\rKubernetes API Resources and Shorthand All k8s resources that can get \u0026ldquo;gotten\u0026rdquo; and shorthand if applicable.\n$ k api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service mutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io false CustomResourceDefinition apiservices apiregistration.k8s.io false APIService controllerrevisions apps true ControllerRevision daemonsets ds apps true DaemonSet deployments deploy apps true Deployment replicasets rs apps true ReplicaSet statefulsets sts apps true StatefulSet tokenreviews authentication.k8s.io false TokenReview localsubjectaccessreviews authorization.k8s.io true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io false SubjectAccessReview horizontalpodautoscalers hpa autoscaling true HorizontalPodAutoscaler cronjobs cj batch true CronJob jobs batch true Job certificatesigningrequests csr certificates.k8s.io false CertificateSigningRequest leases coordination.k8s.io true Lease eniconfigs crd.k8s.amazonaws.com false ENIConfig events ev events.k8s.io true Event daemonsets ds extensions true DaemonSet deployments deploy extensions true Deployment ingresses ing extensions true Ingress networkpolicies netpol extensions true NetworkPolicy podsecuritypolicies psp extensions false PodSecurityPolicy replicasets rs extensions true ReplicaSet ingresses ing networking.k8s.io true Ingress networkpolicies netpol networking.k8s.io true NetworkPolicy runtimeclasses node.k8s.io false RuntimeClass poddisruptionbudgets pdb policy true PodDisruptionBudget podsecuritypolicies psp policy false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io false ClusterRoleBinding clusterroles rbac.authorization.k8s.io false ClusterRole rolebindings rbac.authorization.k8s.io true RoleBinding roles rbac.authorization.k8s.io true Role priorityclasses pc scheduling.k8s.io false PriorityClass csidrivers storage.k8s.io false CSIDriver csinodes storage.k8s.io false CSINode storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment\rGet Nodes. View pods within a given node.\n\u0026gt; kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-3-0-215.ec2.internal Ready \u0026lt;none\u0026gt; 200d v1.14.7-eks-1861c5 ip-10-3-1-138.ec2.internal Ready \u0026lt;none\u0026gt; 27h v1.14.7-eks-1861c5 ip-10-3-2-194.ec2.internal Ready \u0026lt;none\u0026gt; 137d v1.14.7-eks-1861c5 ip-10-3-3-208.ec2.internal Ready \u0026lt;none\u0026gt; 14m v1.14.7-eks-1861c5 [10:06:44] wyatt.munson@WCF-0047 /Users/wyatt.munson/code/gloomberg (0) \u0026gt; kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName Error from server (BadRequest): Unable to find \u0026#34;/v1, Resource=pods\u0026#34; that match label selector \u0026#34;\u0026#34;, field selector \u0026#34;spec.nodeName\u0026#34;: invalid selector: \u0026#39;spec.nodeName\u0026#39;; can\u0026#39;t understand \u0026#39;spec.nodeName\u0026#39; [10:06:53] wyatt.munson@WCF-0047 /Users/wyatt.munson/code/gloomberg (1) \u0026gt; kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=ip-10-3-0-215.ec2.internal\ruserarn: \u0026lsquo;arn:aws:iam::468667468385:user/branden.dodge@westcreekfin.com\u0026rsquo; username: branden.dodge@westcreekfin.com groups: - \u0026lsquo;system:masters\u0026rsquo;\n","date":"2023-02-13","id":0,"permalink":"/docs/kubernetes/kubernetes-overview/","summary":"Kubernetes Kubernetes is a container orchistration platform.\nContents Core concepts kubectl commands Kubectx \u0026amp; Kubens Secrets - external secrets Core Concepts Deployment Set things like:","tags":[],"title":"Kubernetes Overview"},{"content":"\rPostgres is an open source relational database manager.\nPostgres Convetions Postgres connection stting Case sentitive column names PSQL CLI Logging into psql Basic psql commands Using psql wtih schemas Postgres flavored SQL Creating tables Drop table Alter table and delete rows Limit and offset Enums Schemas Postgres data types Postgres Conventions Postgres connection string # basic example\rpostgres://USERNAME:PASSWORD@DATABASE_URL:5432/DATABSE_NAME\r# AWS database\rpostgres://postgres@some-database.us-east-1.amazonaws.com:5432/secret_database\r# bad example - password in connection string is security risk\rpostgres://postgres:exposed_password@10.0.0.5:5432/secret_database\rUnsecure method You can alternately pass in the password using USERNAME:PASSWORD but this is considered unsecure. The password should be omtted from the connection sting, which will then prompt the user to enter it.\nCase sensitive column names Using case sensitive column names in Postgres is optional. By default, all column names are lower case.\nUse \u0026quot; double quotes around column names when creating tables for case sentitive columns. The double quotes must be used whenever referencing the column in a SQL statement.\nCREATE TABLE people ( last_name VARCHAR(64), \u0026#34;firstName\u0026#34; VARCHAR(64) );\rThe advantage is the column names can be formatted in camel case, meaning one less transformation before returning it to the frontend. The disadvantage is the double quotes must be used on the column name in every SQL statement.\nDatabase Schemas Using schemas\nA Postgres contains multiple databases. A database contains multiple schemas. Schemas in turn contain tables. Schemas are a way to segment a single database and allow multiple user to use the same database without stepping on each other.\nBy default talbes use the public schema, which does not have to be referenced in SQL queries.\nPostgres flavored SQL Postgres implements the SQL language as expected for the most part. Postgres layers on additional datatypes and functions.\nCreating tables Postgres datatype reference\n-- Create a sequence to be used in a table (e.g., id field) CREATE SEQUENCE box_sequence start 2000 increment 1; CREATE TABLE boxes ( -- sequence reference uses \u0026#39;single quotes\u0026#39; \u0026#34;boxId\u0026#34; INTEGER NOT NULL PRIMARY KEY DEFAULT nextval(\u0026#39;box_sequence\u0026#39;), -- column names use \u0026#34;double quotes\u0026#34; \u0026#34;boxName\u0026#34; VARCHAR(32), -- lower case column names don\u0026#39;t need quotes notes VARCHAR(128) fixed_text CHAR(32) long_text TEXT, can_delete BOOLEAN, price FLOAT(2), metadata JSONB, id_code UUID, create_time TIME, create_date DATE, created_on TIMESTAMP, with_timezone TIMESTAMP WITH TIME ZONE ); CREATE TABLE items ( -- SERIAL is auto-incremented auto-populated integer \u0026#34;itemId\u0026#34; SERIAL PRIMARY KEY, -- foreign key reference to another table \u0026#34;box\u0026#34; INTEGER REFERENCES boxes (\u0026#34;boxId\u0026#34;), );\rSERIAL is an auto-incrementing, auto-populating type. It starts at one and is automatically add when the record is created. It\u0026rsquo;s handy for a primary key ID when you\u0026rsquo;re not concerned about the value. Drop table Delete a table, handle errors, handle object dependencies.\n-- drop (delete) a table DROP TABLE table_name; -- prevent SQL error if talbe doesn\u0026#39;t exist DROP TABLE IF EXISTS table_name; -- drop table and dependant objects in foreign table DROP TABLE table_name CASCADE;\rAlter table and delete rows Add columns, change column data types, delete columns.\nALTER TABLE items ADD \u0026#34;itemDescription\u0026#34; VARCHAR(64); ALTER TABLE items ALTER COLUMN \u0026#34;lastMoved\u0026#34; TYPE TIMESTAMP WITH TIME ZONE; ALTER TABLE DROP COLUMN description;\rInsert, Update, and Delete rows Insert new rows, update existing rows, and delete rows.\n-- INSERT (strings use \u0026#39;single quotes\u0026#39;) INSERT INTO users (\u0026#34;userId\u0026#34;, notes) VALUES (500, \u0026#39;Greg Benish\u0026#39;); -- UPDATE ROW(S) UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition; -- DELETE ROW(S) DELETE FROM table_name WHERE condition;\rLimit and Offset Limit number of returned rows, skip a number of rows before returning the retult set.\n-- Limiting the rows returned SELECT * FROM users ORDER BY create_date LIMIT 200; -- Skip a number of rows before returning limit SELECT * FROM users LIMIT 200 OFFSET 20;\rEnums Create predefined values for a column.\n-- Create an enum CREATE TYPE item_category AS ENUM (\u0026#39;Electronic\u0026#39;, \u0026#39;Camping\u0026#39;, \u0026#39;Other\u0026#39;); -- Update existing enum ALTER TYPE item_category ADD VALUE \u0026#39;None\u0026#39;; -- Use as column type ALTER TABLE items ADD \u0026#34;itemCategory\u0026#34; ITEM_CATEGORY; -- List enum values \\dT+ ENUM_NAME \\dT+ item_category\rArray Datatypes Column data types can be arrays\nALTER TABLE items ADD \u0026#34;itemList\u0026#34; TEXT[]; INSERT INTO items (\u0026#34;itemList\u0026#34;) VALUES (\u0026#39;{foo,bar}\u0026#39;);\rSchemas Schemas are effectively namespaces that help segment data within a Postgres database.\nCREATE SCHEMA some_schema; CREATE TABLE some_schema.some_table ( ... ); SELECT * FROM some_schema.some_table;\rPSQL Tid Bits Find rows with duplicate records in itinerary, e.g., pointing to a foreign key. Matching records are combined into a JSON array. Returns each row with a unique value for itinerary and a combined JSON array of matching values.\nSELECT s.\u0026#34;itinerary\u0026#34;, jsonb_agg(s) AS segments FROM segments s GROUP BY s.\u0026#34;itinerary\u0026#34;\ritinerary | segments\r__________|___\r1 | [{ itinerary: 1, stops: 3 }, { itinerary: 1, stops: 6 }]\r2 | [{ itinerary: 2, stops: 7 }]\r## Postgres Data Types\r| Type | Code | Desc. |\r| ------------- | --------------------------- | -------------------------------------------------------------------------------------------------------- |\r| Boolean | `BOOLEAN` or `BOOL` | `0`, `true`, `t`, `yes`, `y` evaluates to `true`. \u0026lt;br\u0026gt;`1`, `false`, `f`, `no`, `n` evaluates to `false`. |\r| Charachter | `CHAR(n)` | Fixed length string. Strings less than `n` are padded with spaces. |\r| Charachter | `VARCHAR(n)` | Variable length string. Limited to up to `n` charachters. |\r| Charachter | `VARCHAR` | Variable length string. Unlimited charachters. |\r| Charachter | `TEXT` | Variable length string. Unlimited length. |\r| Small Integer | `SMALLINT` | 2-byte signed integer from -32,768 to 32,767 |\r| Integer | `INT` or `INTEGER` | 4-byte singed integer from -2,147,483,648 to 2,147,483,647 |\r| Serial | `SERIAL` | Similar to integer, but Postgres will automatically generate and populate the value. |\r| Float | `FLOAT(n)` | Floating-point number with precision of `n`. Maximum of 8 bytes. |\r| Float | `real` or `float8` | 4-byte floating point number |\r| Float | `numeric` or `numeric(p,s)` | Real number with `p` digits and `s` number after the decimal place. |\r| Date | `DATE` | Date only. |\r| Date | `TIME` | Time only. (without timezone) |\r| Date | `TIME WITH TIMEZONE` | Time only. Includes timezone. |\r| Date | `TIMESTAMP` | Time and date. |\r| Date | `TIMESTAMPZ` | Time and date with timezone. |\r| Date | `INTERVAL` | Periods of time. |\r| UUID | `UUID` | RFC 4122 compliant UUIDs. |\r| JSON | `JSON` | Plain JSON types that require for each processing. |\r| JSON | `JSONB` | Plain JSON types that are faster to inster but slower to insert. Supports indexing. |\r| Array | `???` | Plain JSON types that are faster to inster but slower to insert. Supports indexing. |\r# Assorted Troubleshooting\r## Delete `postmaster.pid` on certain hard restarts\rThe `postmaster.pid` file points to the process ID running Postgres.\rIf the computer crashes the `postmaster.pid` may not get cleaned up and point to an old process Id. The existance of this file will prevent Postgres from starting again and may need to be deleted if the process it\u0026#39;s referencing is confirmed to be outdated.\r### Check Porcess ID\r```bash\r# inspect postmaster.pid\r# this location may be different\rcat /opt/homebrew/var/postgres/postmaster.pid\r# check processes with that process id\rlsof | grep PROCESS_ID\rlsof | grep 1023\rThe top line of the postmaster.pid file should refer to the process id. Use lsof and grep to check that process id.\nNo processes may be associated with that process id, confirming the process id is outdated.\nIf processes return, verify they are unrelated to Postgres. The process id may have been reassigned to a new, unrelated process, confirming the postmaster.pid file is outdate and can be deleted.\nRunning Postgres Commands # check if Postgres is running ps auxwww | grep postgres\rInstall Location Install location with Homebrew on new M1 Macs: /opt/homebrew/var/postgres.\nOtherwise may be located in /usr/local/var/postgres.\nLog Location New Homebrew installations in M1 Macs: /opt/homebrew/var/log/postgres.log\n","date":"2023-01-04","id":1,"permalink":"/docs/postgres/postgres-basics/","summary":"Postgres is an open source relational database manager.\nPostgres Convetions Postgres connection stting Case sentitive column names PSQL CLI Logging into psql Basic psql commands Using psql wtih schemas Postgres flavored SQL Creating tables Drop table Alter table and delete rows Limit and offset Enums Schemas Postgres data types Postgres Conventions Postgres connection string # basic example\rpostgres://USERNAME:PASSWORD@DATABASE_URL:5432/DATABSE_NAME\r# AWS database\rpostgres://postgres@some-database.","tags":[],"title":"Postgres Basics"},{"content":"Docker is a OS virtualization software. Similar to virtual machines in their outcome, their differ in their function.\nWhen setting up a VM, there is a greater focus on hardware: what processor, how many CPUs, how much RAM. Then a OS is installed on top of that.\nWhen setting up a Docker image, the software is the main question: what OS, adding application code or config, installing dependencies, or building binaries.\nWhat\u0026rsquo;s it purpose The general idea is to promote interoperability. If a Docker container runs on your local machine, it should run on someone else\u0026rsquo;s machine, or a Kubernetes cluster, or Elastic Container Service cluster.\nWhat problem it solves Let\u0026rsquo;s say your developing a Node.js application running Node v12 on your computer. If someone else tries to run said program, but they\u0026rsquo;re running Node v8, it may not run correctly.\nWith Docker, the version of Node is built directly into the Docker image. So if we\u0026rsquo;re using the same Docker image, we\u0026rsquo;re using the same Node version.\nIn this case what you want to do is run the Node.js application, not spend time figure out dependencies and ensuring your \u0026ldquo;development evnironment\u0026rdquo; is set up to run the Node.js application.\nContainer vs Images An image is like a DVD in a case. It can be downloaded, copied, or renamed. It has all information, but it\u0026rsquo;s not running in a DVD player.\nDocker images are built with docker build Docker images are downloaded to a machine with docker pull List all images with docker images A container is a running version of the image.\nDocker images are started with docker run List all running containers with docker ps Who created Docker Docker is created by Docker, Inc. (see: recursion).\nDocker vs. VMs In short:\nVMs abstract the hardware layer so you can focus on the OS level Docker abstracts the OS level, so you can focus on the app ","date":"2023-01-04","id":2,"permalink":"/docs/docker/docker-overview/","summary":"Overview of Docker","tags":[],"title":"Docker Overview"},{"content":"Convert Data Convert CSV - Convert CSV to JSON, YAML, XML, SQL, ect. Link City Bootstrap - Bootstrap ","date":"2023-01-05","id":3,"permalink":"/docs/web-dev/online-tools/","summary":"Convert Data Convert CSV - Convert CSV to JSON, YAML, XML, SQL, ect. Link City Bootstrap - Bootstrap ","tags":[],"title":"Online Tools"},{"content":" File Commands File Commands ls Use ls to list files and directories.\nls # list current directory files ls -lah # show metadata and hidden files ls /dir # list specified direcotry\u0026#39;s file contents\rChanging directories pwd # print working directory - get current location cd dir # move into \u0026#34;dir\u0026#34; directory cd .. # move up one directory cd - # return to previous directory cd ~ # move to user\u0026#39;s home directory cd / # move to root directory\rCreating a file touch FILE # create a file vim FILE # create/open file in vim text editor nano FILE # create/open file in nano text editor mktemp -t FILE # make temp file in /tmp (deleted on boot) mkdir DIR # create a directory\rMoving a file # if destination is a dir, target is placed in dir # if destination is a file, target overwrites file mv source_file target_file mv file.txt /home/usr/file.txt mv -f # force move and overwrite mv -i # interactive prompt befor everwrite mv -u # update - move when source is newer than target mv -v # verbose - print source and target files # rename file foo to bar mv foo bar # move all JSON files to subdirectory bar mv *.json bar # move all files in subdirectory foo to current directory mv foo/* .\rRsync to move.\n# local to local rsync [OPTIONS] SOURCE TARGET # local to remote (push) rsync [OPTIONS] SOURCE USER@HOST:TARGET # remote to local (pull) rsync [OPTIONS] USER@HOST:SOURCE TARGET # basic usage rsync -a /source/foo/ /target/foo/ # flags -a # archive mode - sync recursively -z # force compression to send to destination machine -P # progress bar and keep partially transferred files -q # quiet to supress non-error messages -v # verbose -h # human readable output # copy local source to remote target rsync -a /foo/ user@remote_host:/foo/ # copy from remote source to local target rsync -a user@remote_hostname:/foo/ /foo/ # use for large transfers and/or unstable internet connections rsync -aP source target # exclude \u0026#34;directory\u0026#34; and \u0026#34;another_dir\u0026#34; rsync -a --exclude=directory --exlucde=another_dir source target # include json files and no others rsync -ae ssh --include=\u0026#34;*.json\u0026#34; --exlucde=\u0026#34;*\u0026#34; source target # delete files that are not in source directory rsync -a --delete source target # rsync over ssh rsync -ae ssh user@remote_hostname:/foo/ /foo/ # SSH other than port 22 rsync -a -e \u0026#34;ssh -p 123\u0026#34; /source/ user@remote_hostname:/target/ # specify max file size for transfer rsync -a --max-size=\u0026#34;200k\u0026#34; source target # do dry run rsync --dry-run --remove-source-files -v source target # set bandwidth limit rsync --bw-limit=100 -a source target\rCopying a file cp source_file target_file # copy directory, and create target_dir if it doesn\u0026#39;t exist cp -r source_dir target_dir # copy source file to target file cat SOURCE_FILE \u0026gt; TARGET_FILE # merge to files together cat FILE_1 \u0026gt;\u0026gt; FILE_2\rDeleting a file rm file_name # remove directory rm -r directory_name # force remove file rm -f file_name # display deletion status confirmation rm -v file_name # delete a directory (fails if directory is not empty) rmdir DIRECTORY_NAME # delete a non-empty directy (and its contents) rmdir -r DIRECTORY_NAME # delete directory and its contents rm -r DIRECTORY_NAME # force delete directory and its contents rm -rf DIRECTORY_NAME\rReading a file # display contents of file to terminal window cat FILE_NAME # display number of lines cat -n FILE_NAME # display number of lines (alternate) nl FILE_NAME wc FILE_NAME # display matching lines cat FILE_NAME | grep -i \u0026#34;search text\u0026#34; # get line count of matching lines cat FILE_NAME | grep -i \u0026#34;search text\u0026#34; | wc # display first 10 lines of file to terminal window head FILE_NAME # display last 10 lines of file to terminal window tail FILE_NAME # display last XX lines of file to terminal window tail -n XX FILE_NAME # display last 10 lines of file and keep open for changes tail -f FILE_NAME more FILE_NAME less FILE_NAME # find differences in two files diff file_name1 file_name2 # print every row of 2nd column awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; airports.csv # print distinct values of 2nd column awk -F \u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39; airports.csv | sort -u # print each value in 10th column with count (duplicates only) cut -f10 -d, \u0026#34;airports.csv\u0026#34; | sort | uniq -cd | sed \u0026#39;s/ *//\u0026#39; # print each value in 10th column with count cut -f10 -d, \u0026#34;airports.csv\u0026#34; | sort | uniq -c | sed \u0026#39;s/ *//\u0026#39; # removes lines where given column is empty awk \u0026#39;$1!=\u0026#34;\u0026#34;\u0026#39; input_file # partial match of text in given column awk -F \u0026#39;,\u0026#39; \u0026#39;$6~/no longer/\u0026#39; reduced_airlines.csv # awk variable awk \u0026#39;/\u0026#39;$VAR\u0026#39;/\u0026#39; airlines.csv awk -F \u0026#39;,\u0026#39; \u0026#39;$3~/\u0026#39;$VAR\u0026#39;/\u0026#39; airlines.csv\rFinding a file # seach for file or dir at current dir and all subdirs find . -name NAME [TYPE] find . -iname search_text # ignore text case find . -iname search_text # search by file size (c, k, M, G, T, P) find . -name search_text -size 10M # print out directory tree structure tree\rCreating a Symbolic Link # create symbolic link ln -s FILE_NAME LINK # check where symbolic link points to readlink FILE_NAME\rCheck file information file FILE_NAME file package.json # use file_list.txt to read file names (separated by line) file -f file_list.txt # check inside compressed files file -z file_name # check file line count (display: lines, words, bytes) wc file_name\rText sed - Stream EDitor Sed is a stream editor for filtering and replacing text. Sed probably has a shit tonne of stuff going on, used commonly for REGEX string matching and replacement.\nUse sed to find and replace text sed -i \u0026#34;s,TEXT_TO_FIND,TEXT_TO_REPLACE_WITH,g\u0026#34; # use a sigial charachter in a file and replce the text sed -i \u0026#34;s,\u0026lt;VERSION_NUMBER\u0026gt;,10,g\u0026#34; someFile.txt # formatting for macOS 10.9+ sed -i\u0026#39;.original\u0026#39; -e \u0026#34;s,\u0026lt;SOMETHING_TO_FIND\u0026gt;,REPLACE_TEXT,g\u0026#34; someFile.txt\rAlias sed The default sed binary on MacOS behaves differently from sed on linux machines. Install gnu-sed from homebrew.\nInstall gnu-sed with homebrew on MacOS machines to get the version of sed that has similar behavior to linux machines.\nAlias gnu-sed to sed so it will be used instead of the default sed command.\nbrew install gnu-sed alias sed=gnu-sed\rCutting # cut 1 \u0026amp; 3rd field from file (Tab delimeted) cut file_name -f 1,3 # cut up to 3rd field from file (Tab delimeted) cut file_name -f -3 # specify alternate delimeter cut file_name -d \u0026#34;:\u0026#34; -f -3\rSystem Information date # get date cal # show current month\u0026#39;s calendar uptime # show current uptime w # display who\u0026#39;s online whoami # display current logged in user finger user # display infomation about \u0026#34;user\u0026#34; uname -a # show kernel information df # show disk usage du # show directory space usage # show disk quota quota -v free # show memory and swap usage cat /proc/cpuinfo # CPU information cat /proc/meminfo # memory information # switch to given user su USER # swtich to root user (may need: sudo su -) su - # execute command as super user sudo COMMAND\rVariables # set a variable VARIABLE_NAME=some_text # reference a variable echo $VARIABLE_NAME\rBash Builtin Variables $0 # Name of the Bash script $1 - $9 # First 9 arguments to the script $# # Number of arguments were passed to script $@ # All the arguments supplied to the Bash script. $? # The exit status of the most recently run process. $$ # The process ID of the current script. $USER # The username of the user running the script. $HOSTNAME # The hostname of the machine the script is running on. $SECONDS # The number of seconds since the script was started. $RANDOM # Returns a different random number each time is it referred to. $LINENO # Returns the current line number in the Bash script.\rReading Input # set a variable read VARIABLE_NAME # set a variable, with prompt statement read -p \u0026#39;Enter name: \u0026#39; VARIABLE_NAME # set a variable, with prompt statement, password read -sp \u0026#39;Enter password: \u0026#39; VARIABLE_NAME\rIf Statements if [ $1 -gt 100] then echo \u0026#34;Some text\u0026#34; elif [ $2 == \u0026#39;no\u0026#39;] then echo \u0026#34;Alternate matching condition\u0026#34; else echo \u0026#34;Final catch condition\u0026#34; fi\rWildcards if [[ $1 == *\u0026#34;$SUBSTRING\u0026#34;* ]]\rBoolean Operators \u0026amp;\u0026amp; # and || # or\rif [[ $1 -gt 100 \u0026amp;\u0026amp; $2 == \u0026#39;yes\u0026#39;]] elif [[ $1 -gt 50 || $2 == \u0026#39;no\u0026#39;]]\rEquality Operators = == # string comparisons -eq # equals, numeric comparisons -gt # greater than -ge # greater than or equal to -lt # less than -le # less than or equal to -ne # not equal\rNetworking # ping ping google.com # get DNS information about domain dig google.com\rcURL curl https://google.com # download file and set name to file_name curl https://google.com --output file_name curl https://google.com -o file_name # silence output curl https://google.com --silent curl https://google.com -s # follow a location (instead of returning redirect) curl --location https://google.com # username/password authentication curl http://user:password@example.com/ # OR curl -u user:password http://example.com/ # curl POST using application/x-www-form-urlencoded curl --data \u0026#34;birthyear=1905\u0026amp;press=%20OK%20\u0026#34; http://example.com/api # curl POST using application/json curl --header \u0026#34;Content-Type: application/json\u0026#34; \\ --request POST \\ --data \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;}\u0026#39; \\ http://example.com/api # curl https://mysubdomain.zendesk.com/api/v2/groups.json \\ -v -u myemail@example.com:mypassword # Certificates curl --cert mycert.pem https://secure.example.com # use your own CA cert store to verify server\u0026#39;s certificate curl --cacert ca-bundle.pem https://example.com/ # ignore certificate verification curl --insecure https://secure.example.com # OR curl -k https://secure.example.com # curl resolve (point to different address for a hostname) curl --resolve www.example.org:80:127.0.0.1 http://www.example.org/\rwget # download file with wget wget http://example.com/file.txt # specify custom file name (also use to overwrite) wget -O custom_name.txt http://example.com/file.txt # download to a specific directory wget -P Documents/ http://example.com/file.txt # silence output wget -p http://example.com/file.txt # show progress bar wget -q --show-progress http://example.com/file.txt\rSend REST requests with wget\n# send GET request wget -O- http://example.com/posts # send POST request wget -O- --method=post -q --body-data=\u0026#39;{\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;}\u0026#39; \\ --header=Content-Type:application/json \\ --header=\u0026#34;Authorization: Bearer abc123\u0026#34; http://example.com/posts\rCompressing # untar tar.gz files tar -zxvf file.tar.gz # specify output file name tar -zxvf file.tar.gz output_file.txt # tar using gzip compression tar -cvzf target.tar.gz source_file_or_directory # tar using bzip2 compression tar -cvjf target.tar.bz2 source_file_or_directory tar -cf file.tar files # create a tar named file.tar containing files tar xf file.tar # extract the files from file.tar tar czf file.tar.gz files # create a tar with Gzip compression tar xzf file.tar.gz # extract a tar using Gzip tar cjf file.tar.bz2 # create a tar with Bzip2 compression tar xjf file.tar.bz2 # extract a tar using Bzip2 gzip file # compresses file and renames it to file.gz gzip -d file.gz # decompresses file.gz back to file\rSearching # search file for matching text grep search_text file_name grep -i \u0026#34;phrase\u0026#34; file.txt # case-insensitive search grep -i \u0026#34;phrase\u0026#34; file_name # search current directory and all subdirectories: grep -R \u0026#34;phrase\u0026#34; . # find matching file names ls | grep \u0026#34;phrase\u0026#34; # use wildcards ls | grep *report.txt ls | grep *report*\rRunning Commands whereis command # show possible locations of \u0026#34;command\u0026#34; which command # show default location of \u0026#34;command\u0026#34; to be used # see manual page for grep man grep # measure execution time to run grep time grep\rHomebrew # install formula brew install FORMULA # uninstall formula brew uninstall FORMULA # list all installed formulae brew list # get info on formula brew info FORMULA # display all locally available forumae for brewing brew search # substring match on forumae for brewing brew search SEARCH_TEXT # check version brew --version # print help brew help # print help for sub command brew help SUB_COMMAND # check for problems brew doctor # delete old versions of all installed formulae brew cleanup # dry-run of delete old versions of all installed formulae brew cleanup -n # delete old versions of a given installed formulae brew cleanup FORMULA\rUpgrading Homebrew and Formulas # get latest version of homebrew and formula brew update # upgrade out of date and unpinned brews brew upgrade # upgrade specified brew brew upgrade FORMULA # show formulae with updates available brew outdated # lock version and prevent updates for given forumlae brew pin FORMULA # unlock version and allow updates for given formulae brew unpin FORMULA\rExternal Repositories Using Tap Add additional homebrew repositories with brew tap.\n# list current taps (tapped repositories) brew tap # tap a formula repository from github brew tap USER/REPO # tap a formula repository from given URL brew tap USER/REPO URL # untap a formula repository brew untap USER/REPO\rCask This is outdated! Use updated brew \u0026lt;command\u0026gt; --cask instead.\n# tap the Cask repository in GitHub brew tap homebrew/cask # install a given cask brew cask install CASK # reinstall a given cask brew cask reinstall CASK # uninstall a given cask brew cask uninstall CASK # list all installed casks brew cask list\rBash Input and Output Redirection cmd1 | cmd2 # pipe output of cmd1 to input of cmd2 \u0026lt; file # use standard input of a file \u0026gt; file # redirect standard output to file \u0026gt;\u0026gt; file # redirect standard output to file, (append) \u0026lt;\u0026gt; file # use file as stdin and stdout \u0026amp;\u0026gt; file # redirect stdout and stderr 2\u0026gt; file # redirect stderr only\rSelect last charachters in string Extract the last charachters in a string\nstringer=\u0026#34;tester\u0026#34; echo \u0026#34;${stringer: -1}\u0026#34; # ⮑ r\rRemove last n charachters in string Exclude the last charachters in a string\nstringer=\u0026#34;someFile.md\u0026#34; short_str=${stringer::-3} echo $short_str # ⮑ someFile\r","date":"2023-01-03","id":4,"permalink":"/docs/bash/bash-cheatsheet/","summary":"File Commands File Commands ls Use ls to list files and directories.\nls # list current directory files ls -lah # show metadata and hidden files ls /dir # list specified direcotry\u0026#39;s file contents\rChanging directories pwd # print working directory - get current location cd dir # move into \u0026#34;dir\u0026#34; directory cd .","tags":[],"title":"Bash Cheatsheet"},{"content":"The Docker CLI is used to communicate with the Docker Engine to build, run, and maintain images.\nBasic Commands # View all docker images saved locally docker images # Download a docker image onto local machine docker pull \u0026lt;IMAGE_NAME\u0026gt;:\u0026lt;IMAGE_TAG\u0026gt; docker pull library/nginx:latest # View all running containers docker ps\rBuild Docker Image Use docker build to build a Docker image from a Dockerfile, from a specified location (. specifies the current directory).\nDocker images file the syntax \u0026lt;VENDOR\u0026gt;/\u0026lt;REPO\u0026gt;:\u0026lt;TAG\u0026gt;.\n\u0026lt;VENDOR\u0026gt; - the organization, company, or user. Set to the DockerHub username if pushing to that location. \u0026lt;REPO\u0026gt; - the repository or application name for this image. All versions of an application share the same repo name. \u0026lt;TAG\u0026gt; - reference to the version number or specific image. # build a docker image docker build . # build docker image and specify tag docker build . -t munsonwf/express-sample:1.2.3 # feed env variables in a command line docker build . --build-arg DB_PASSWORD=hunter2\rRun Docker container # run docker container (using image name and tag) docker run \u0026lt;IMAGE_NAME\u0026gt;:\u0026lt;IMAGE_TAG\u0026gt; docker run alpine:latest # run docker container (using container Id) docker run \u0026lt;CONTAINER_ID\u0026gt; docker run 183582d52a03 docker run 18 # run with env vars docker run -e DB_PASSWORD=hunter2 -it ubuntu:latest some-command # exec into running container docker exec -it \u0026lt;CONTAINER_ID\u0026gt; \u0026lt;COMMAND\u0026gt; docker exec -it fe93b2ca2 /bin/bash # run exec against stopped container docker run --rm -it \u0026lt;REPO_NAME\u0026gt; \u0026lt;EXEC_COMMAND\u0026gt; docker run --rm -it gitlab/gitlab-runner:2.0.3 bin/bash # run exec against stopped container (override entrypoint) docker run --rm -ti --entrypoint=\u0026#39;\u0026#39; munsonwf/nslookup:1.0.0 /bin/ash\rLogs from Docker container # View container logs sudo docker logs CONTAINER-NAME # log flags sudo docker logs --follow CONTAINER-NAME sudo docker logs --since=1h CONTAINER-NAME sudo docker logs --tail=20 CONTAINER-NAME # save logs to file sudo docker logs CONTAINER-NAME \u0026gt; ./file/path.txt sudo systemctl start docker service docker status # install docker compose sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose # add docker compose to path for sudo by creating symlink sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\rTake Snapshot of Container Take snapshot of running container\r# take snapshot of running container (get id with `docker ps`) docker commit \u0026lt;CONTAINER_ID\u0026gt;\rTag Docker Image Tag image\r# tag docker image (get id with `docker images`) docker tag \u0026lt;IMAGE_ID\u0026gt; \u0026lt;NEW_NAME\u0026gt;\rInspect Running Container Inspect running container\r# inspect container (see entrypoint, CMD) docker inspect \u0026lt;IMAGE ID\u0026gt; docker inspect 2bf89faaab70\rInspect Network Configuration Inspect network configuration\r# list docker networks docker network ls # inspect given docker network docker network inspect \u0026lt;IMAGE ID\u0026gt; docker network inspect 74e\rPush to DockerHub # login to docker docker login # push sepcific image and tag to docker push docker push vendor/app:ver\rBuild for different platforms The platform for an M1 MacBook is not the same as the Linux container it may be running in on the cloud.\ndocker buildx build --platform=linux/amd64 -t \u0026lt;image-name\u0026gt; .\rDocker Volumes Volumes are a way to store data outside of a container. When a container is stopped, all the data inside it is destroyed. A volume allows the container to save data to the system it runs on.\ndocker run -it -v LOCATION_ON_FILESYSTEM:LOCATION_IN_CONTAINER ubuntu:22.06\rDocker will automatically create a volume when referenced in a docker run command.\n# create a volume docker volume create VOLUME_NAME # list all volumes docker volume ls # inspace a given volume docker volume inspect VOLUME_NAME # remove a given volume docker volume rm VOLUME_NAME # force remove volume docker volume rm VOLUME_NAME -f # clean up all unused volumes (not mounted to a container) docker volume prune\rVolumes in Docker Compose services: app: image: IMAGE_NAME:TAG volumes: - VOLUME_NAME:LOCATION_IN_CONTAINER volumes: VOLUME_NAME:\rIf using an existing volume, set the external flag to true:\nvolumes: VOLUME_NAME: external: true\r","date":"2023-01-04","id":5,"permalink":"/docs/docker/docker-cli-commands/","summary":"Docker CLI command reference for building and running images, and managing Docker","tags":[],"title":"Docker CLI Commands"},{"content":"Install Mac?\nbrew install terraform\rBasic Usage Sample Backend terraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;terraform-core-lle-state\u0026#34; key = \u0026#34;A/UNIQUE/KEY\u0026#34; region = \u0026#34;us-east-1\u0026#34; dynamodb_table = \u0026#34;terraform-lock\u0026#34; } }\rThe key is the S3 key, that is the path to the file. Do not duplicate keys between projects, or you will destroy existing the existing state file.\nTerraform\u0026rsquo;s backend is a statefile, with a representation of it provisioned. For more information about Terraform state, go here: Terraform State\nParamterizing the Backend You can set the bucket name to be a variable, by passing it in during terraform init:\n$ terraform init -backend-config=\u0026#34;bucket=${your-bash-variable-that-has-bknd-s3-bucket-name}\u0026#34; \\ -backend-config=\u0026#34;key=some-key\u0026#34; \\ -backend-config=\u0026#34;region=${your-bash-variable-that-has-aws-region}\u0026#34; \\ -backend-config=\u0026#34;access_key=${your-access-key-in-variable}\u0026#34; \\ -backend-config=\u0026#34;secret_key=\u0026#34;${your-secret-key-in-variable}\u0026#34;\rStore Secrets in AWS You can store secrets in AWS and have terraform retrieve the value from AWS at runtime.\nThere are two AWS services for secrets storage\nAWS Secrets Manager Parameter Store (part of AWS Systems Manager) Retrieving a Secret in Terraform Secrets can be stored in AWS Secrets Manager or AWS Parameter Store\n# Import secret from Parameter Store data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;token-db-password\u0026#34; { name = \u0026#34;prod-token-db-pw-${terraform.workspace}\u0026#34; } # Use secret in new resource resource \u0026#34;aws_rds_cluster\u0026#34; \u0026#34;token-cluster\u0026#34; { engine = \u0026#34;aurora-postgresql\u0026#34; master_password = \u0026#34;${data.aws_ssm_parameter.token-db-password.value}\u0026#34; master_username = \u0026#34;postgres\u0026#34; }\rCommon Commands # import $ terraform import $ terraform import aws_s3_bucket.prod bucket-name # Remove from state $ terraform state rm aws_s3_b # Remove state lock $ terraform force-unlock \u0026lt;LOCK_ID\u0026gt;\rTerraform Import $ terrform import ADDRESS ID $ terraform import aws_s3_bucket.prod_bucket aws-resource-id\rNOTE: Use terraform refresh to get resource addresses and ID\u0026rsquo;s. See here\nTerraform Refresh $ terraform refresh | ----- ADRESS ------ | | ---- ID ---- | aws_s3_bucket.prod-bucket: Refreshing state... [id=prod-bucket-name]\rTerraform in the Pipeline This is where it gets tricky.\nThere are three main stages:\nValidate: verify terraform is syntactically correct through static anlysis Connection to backend is not required here Role assumption in pipeline is not required here Terraform must be inialized, but backed connection not required at this stage (terraform init -backend=false). Plan: generate an execution plan with details about what Terraform will update or change Use this stage to verify Terraform is not altering resources you do not expect Plan file is saved as an artifact and used during the apply stage, so Terraform will apply exactly what it plans to do on this stage. Connection to backend is established here Apply: provision resources as planned. Terraform will call out to the AWS CLI and provision the resources you requested Apply stage uses plan file from previous stage to be certain there is no deveation between the plan and apply stage. Sample .gitlab-ci.yml image: docker.internal.westcreekfin.com/terraform-aws:1a8cfe02b6 stages: - validate - plan - apply - destroy validate:lle: stage: validate tags: - aws-public-cloud-privileged script: - terraform init -backend=false - terraform validate except: variables: - $CI_COMMIT_MESSAGE =~ /chore/ plan:lle: stage: plan tags: - aws-public-cloud-privileged script: - source lle.sh - aws eks --region us-east-1 update-kubeconfig --name ${EKS_LLE_CLUSTER_NAME} - terraform init -backend-config=\u0026#34;bucket=terraform-core-lle-state\u0026#34; - terraform workspace select lle - terraform plan -out=./tf-plan artifacts: untracked: false paths: - ./tf-plan expire_in: 3 days except: variables: - $CI_COMMIT_MESSAGE =~ /chore/ apply:lle: stage: apply when: manual tags: - aws-public-cloud-privileged script: - source lle.sh - aws eks --region us-east-1 update-kubeconfig --name ${EKS_LLE_CLUSTER} - kubectl config current-context - kubectl get po -A - helm repo add ${HELM_REPO_NAME_AND_URL} - helm repo update - terraform init -backend-config=\u0026#34;bucket=terraform-core-lle-state\u0026#34; - terraform workspace select lle - terraform apply -auto-approve ./tf-plan dependencies: - plan:lle except: variables: - $CI_COMMIT_MESSAGE =~ /chore/ DESTROY_ALL_RESOURCES:lle: stage: destroy when: manual tags: - aws-public-cloud-privileged script: - source lle.sh - aws eks --region us-east-1 update-kubeconfig --name ${EKS_LLE_CLUSTER} - terraform init -backend-config=\u0026#34;bucket=terraform-core-lle-state\u0026#34; - terraform workspace select lle - terraform destroy -auto-approve\r","date":"2024-07-01","id":6,"permalink":"/docs/devops/terraform/","summary":"This is a terraform summary","tags":[],"title":"Terraform"},{"content":"Full Bash reference Command Name Description cat concatenate Display contents of text file; combine multiple files. cd change directory Traverse the directory tree and move to different folders chmod change mode Change file permissions cp copy Copy files or directories curl curl HTTP utility cut cut dig domain information grouper Show DNS information for a URL file file Display file type find find Find a file echo echo Print output to the terminal exit exit Exit current script or shell session export export Export a variable grep global regular expression print Use regex to find files kill kill Stop a running process ls list List contents of a directory lsof list open files See running processes, check ports mkdir mkdir Create a directory ps ps See status of running process pwd print working directory Show current directory rm remove Removes files or directories rsync rsync Sync files between locations or remotely sudo super user do Elevates a command to super user privileges scp secure copy Securely copy file between hosts sed stream editor Edit a text stream ssh ssh HTTP utility tail tail Print the end of a file touch touch Create a file type type See file or directory type xargs xargs Yarr Redirect Operators Redirect Syntax Reference command ARGUMENT [OPTIONS] [FILE_NAME...]\rcat - ConcATenate cat\ncat combines files.\nIntended to combine or concatenate multiple text files into one. Used for a variety of things, like the simple cat filename.txt to print out a files contents.\nUse cat to print out contents of file to terminal:\n$ cat FILE_NAME $ cat log.txt\rUse the more or less command to print out one page at a time:\n$ cat file | more $ cat file | less\rUse cat to combine multiple files:\n# combine file1 and file2 into megaFile.txt $ cat file1 file2 \u0026gt; megaFile.txt # combine all files in current directory to monsterFile.txt $ cat * \u0026gt; monsterFile.txt # combine all text files cat *.txt \u0026gt; combined.txt\rCat multiline variable to file Use cat to send a multiline string to a file.\nSend multiline string to a file\rcat \u0026lt;\u0026lt;EOF \u0026gt; file_name.yaml version: 1 type: prod settings: status: active online: false EOF\rcd - Change Directory Probably the most rudimentary command. Allows you to traverse the file system.\n$ pwd # print working directory - get current location ↪ /users/benish/code $ ls # list out content of current directory ↪ file.txt lounge-frontend/ lounge-backend/ $ cd lounge-frontend/ # move into \u0026#34;lounge-frontend\u0026#34; # pwd /users/benish/code/lounge-frontend $ cd .. # move up one directory # pwd /users/benish/code $ cd - # return to previous directory # pwd /users/benish/code/lounge-frontend\rchmod - CHange MODe chmod\nchmod changes file ownership.\nChanges file permissions: who can read, write, and execute the file.\nBasic chmod usage example\rchmod +x FILE_NAME chmod 744 FILE_NAME\rThe file permissions can be specified with either the symbolic or numeric method.\nFile Permissions\nUnderstand file permissions\nEach file has three personas that can access the file: the file owner (or creator), the group, and all other users. Each persona then has different levels of permissions for that file: read, write, execute, or some combination.\nWhen using chmod you are specifying two things:\nThe persona: file owner, file group, or all users The permissions: read, write, execute, or some combination A single command can specify a single persona or multiple personas. Each persona can (and usually does) have different levels of permissions.\nSymobilc Method chmod\nThe symolic method users letters to refer to groups and permissions, e.g., chmod +x FILE_NAME\nSymbolic method uses letters to refer to the groups and permissions, e.g., g+x.\nGroups:\nu - file owner (user) g - group, users in the same linux group o - all other users a - all users (u, g, o) Attachments:\n+ - add specified permission - - remove specified permission = - change current permission to specified permission (if no permissions are specified, all permissions are removed) Permissions:\nr - read w - write x - execute Use chmod with symbolic method:\nChange permissions with chmod symbolic method\r# give file owner execute permissions chmod u=x FILE_NAME # remove group permissions to write to file chmod g-w FILE_NAME # add execute to file owner, remove all permissions from group chmod u+x,g= FILE_NAME\rNumeric Method Numeric Method\nThe numeric method uses numbers to identify permissions and their position to identify the linux user, e.g., chmod 744 FILE_NAME.\nIn the numeric method, each persona is assigned a single digit that represents their permissions: read, write, execute, or some combination thereof.\nr - read - 4 w - write - 2 x - execute - 1 No permission - 0 To combine permissions, add the value for each permission:\nRead (4) + Write (2) + Execute (1) =\u0026gt; 7\rRead (4) + Execute (1) =\u0026gt; 5\rRead (4) =\u0026gt; 4\rPermissions for all groups are given in a three digit code. The position of each digit refers to the three linux user group. The order is file owner + file group + all users.\n# numeric group order syntax chmod [FILE_OWNER][FILE_GROUP][ALL_USERS] FILE_NAME chmod 755 FILE_NAME ^^^ |||_ all other users (o) ||__ group (g) |___ file owner\rUse chmod with numeric method:\nExample use of numeric method\r# give file owner - read, write, execute; group - read, write; all other users - read chmod 764 FILE_NAME # give all permissions to all groups chmod 777 FILE_NAME\rChange permissions for multiple files Use wildcards (*) to specify multiple files in a single command.\nUse wildcard with chmod to target multiple files\r# give file owner execute permissions for all text files in current dir chmod u=x *.txt\rChange permissions in subdirectories Use the recursive (-R) flag to also include subdirectories in the chmod command. This will also target all file as well as the target directory\u0026rsquo;s permissions.\nChange permissions for all subdirectories and files\r# give full permissions to all users chmod -R 777 DIR_NAME\rUse wildcard and target subdirectories\r# give file owner execute permissions for all text files chmod -R u=x *.txt\rcp - CoPy Command: Copy\nThe cp command copies a file or directory from a source location to a new or existing target location.\nCopy quick reference\r# syntax cp [OPTIONS] \u0026lt;sourceFile\u0026gt; \u0026lt;destinationFile\u0026gt; cp [OPTIONS] SOURCE_DIRECTORY DESTINATION_DIRECTORY # basic usage cp sourceFile targetFile # copy a dir and its contents to a new dir cp -R directory1 directory2 # copy file and add to an existing target dir cp fileName ../targetDirectory # copy all files within source directory to an existing target dir cp -R sourceDirectory/* targetDirectory/\rCopy a file Copies content of file1 to file2. Creates file2 if it doesn\u0026rsquo;t exist, overwrites it if it does.\nCopy file to directory A file can be copied to an existing directory. The file will be added to the target directory without affecting the other files in the target. If the source file exists in the target, it will be overwritten.\nCopy a file to an existing directory\rcp file1 dir1\rCopy multiiple files to a directory Overwrites contents of directory if it exits, creates one if it does not. Can supply 1..n files.\nCopy multiple files to a target directory\rcp file1 file2 file3 targetDirectory\rCopy directories Copies entire contents of source directory to target directory.\nThe behavior depends on if the target directory exists:\nIf the target directory does not exist, it is created. If the target directory does exist, it is copied as a subdirectory into the target directory with the name of source directory Copy a directory\rcp -R sourceDirectory targetDirectory\rIf the target directory exists, the source directory becomes a subdirectory in the target directory.\nOther copy flags Wildcards Use wildcards to target multiple source files to copy.\nUse wildcards\rcp *.txt targetDirectory\rForce copying Force copying, even in cases when user lacks write permissions, delete destination file if necessary.\nForce copy a file\rcp -f file1 file2\rCreate backup Create a backup of the target file in the same folder with a different name and format.\nCreate a backup when copying\rcp -b file1 file2\rPreserve file charachteristics Preserve file charachteristics like modification time, access time, ownership, and permissions.\nPreserve charachteristics\r# preserve charachteristics cp -p file1 file2\rInteractive mode Interactive mode to prompt before overwriting\r# interactive - promt before overwriting $ cp -i s.txt f.txt\rcurl - Client URL Transfer an URL.\ncurl https://example.com # get file using FTP curl ftp://ftp.example.com/README\rSilence output\r# silence output curl https://example.com --silent curl https://example.com -s\rUse -L to follow redirects if the first response is a 3xx.\nFollow redirects\r# follow a location (instead of returning redirect) curl --location https://example.com curl -L https://example.com\rDownload file with cURL Download a file with cURL\r# download file and set name to file_name curl https://example.com --output file_name curl https://example.com -o file_name # download file and use same file name curl https://example.com -O\rUsename and password authentication # username/password authentication curl http://user:password@example.com/ # OR curl -u user:password http://example.com/\rHTTP POST request By default, cURL uses a GET request. Use the -X flag to specify a different method.\nSpecify HTTP method\r# follow a location (instead of returning redirect) curl -X POST https://example.com\r# curl POST using application/json curl --header \u0026#34;Content-Type: application/json\u0026#34; \\ --request POST \\ --data \u0026#39;{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;}\u0026#39; \\ http://example.com/api # curl POST using application/json (alternate flags) curl -X POST \u0026#34;http://example.com\u0026#34; \\ -H \u0026#34;Content-Type:application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;magentorocks1\u0026#34;}\u0026#39; # curl POST using application/x-www-form-urlencoded curl --data \u0026#34;birthyear=1905\u0026amp;press=%20OK%20\u0026#34; http://example.com/api\rSet paramaterized HTTP POST response to variable Set HTTP POST response body to variable, using parameterized data payload and values.\n# curl POST with parameters API_RESPONSE=$(curl -s --location \u0026#34;https://example.com/api/$VAR_ID?ticketId=\u0026#34;$VAR_2\u0026#34;\u0026amp;anotherParamKey=paramValue\u0026#34; -X POST \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;accept: application/json\u0026#39; \\ --header \u0026#39;Authorization: ApiKey \u0026#39;$VAR_3\u0026#39;\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;id\u0026#34;: \u0026#34;\u0026#39;$VAR_4\u0026#39;\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;benish@example.com\u0026#34; }\u0026#39;) # access response object key of \u0026#34;id\u0026#34;, set to variable TEAM_ID=$(echo $API_RESPONSE | jq -r \u0026#39;.id\u0026#39;)\rSee redirect path curl -sILk google.com\rCertificates # Certificates curl --cert mycert.pem https://secure.example.com # use your own CA cert store to verify server\u0026#39;s certificate curl --cacert ca-bundle.pem https://example.com/ # ignore certificate verification curl --insecure https://secure.example.com # OR curl -k https://secure.example.com # curl resolve (point to different address for a hostname) curl --resolve www.example.org:80:127.0.0.1 http://www.example.org/\rcut - CUT Cut stuff.\nCut by Field Cut by field using the -f flag. The default delimeter is TAB.\n# sales.txt 2022-20-11 11:34 134.23 Electronics 2022-20-10 10:23 500.34 Appliances\rUse cut to extract the first and third column:\n$ cut sales.txt -f 1,3 # output 2022-20-11 134.23 2022-20-10 500.34\rDisplay first through thrid field:\n$ cut sales.txt -f -3 # output 2022-20-11 11:34 134.23 2022-20-10 10:23 500.34\rCut with a Delimeter Use the -d flag to specify a delimeter.\ncut sales.txt -d \u0026#34;:\u0026#34; -f 1,2\rdig - Domain Information Grouper Get information about DNS name servers. Commonly used to see what domain names resolve to.\n# run dig on github.com $ dig github.com # results # # ... # # ;; ANSWER SECTION: # github.com.\t42\tIN\tA\t140.82.113.3 # # ...\rUse the ANSWER SECTION to see the DNS records of the search. If there are no answers, the requested DNS record could not be found or does not exist.\n# quick dig $ dig github.com +short ↪ 140.82.113.3 # get trace route $ dig github.com +trace # bring your own nameserver $ dig @ns1.you-specify.com github.com # trim dig output $ dig github.com +nostats # no stats $ dig github.com +nocomments # no header $ dig github.com +noall +answer # remove everything, show answer # Request different types of records $ dig github.com NS\rRequest different types of records\nA = Internet Address, default (IP address) TXT = Text annotations MX = Mail eXchange (mail servers) NS = Name Server echo command Prints text to the terminal window.\n$ echo some text ↪ some text\rexit command Exit current shell session with a given exit code.\n# exit current shell with exit code 1 exit 1\rexport - EXPORT export\nThe export command makes variables and functions available to child shells and processes.\nMark variables and functions to be passed to child shells and processes.\nExport a variable:\nBash variables are capatalized by convention.\nexport VARIABLE_NAME=VARIABLE_VALUE export NODE_VERSION=14\rGet text from file and insert into new file:\nexport VERSION=`grep \u0026#39;\u0026#34;version\u0026#34;:\u0026#39; package.json | cut -d\\\u0026#34; -f4` export IMAGE_NAME_AND_TAG=$(cat .dev-image-name-and-tag) echo $IMAGE_NAME_AND_TAG \u0026gt; .stage-image-name-and-tag\rWhen sourcing a file, the chile process inherits all the variables of the parent process. If the child process sets variables with export, these variables are now available in the parent process.\nfile command Indicates file type.\nUsage\n$ file cats.md # basic usage ↪ ./cats.md: ASCII text, with very long lines $ file cats.md # get MIME type ↪ ./cats.md: text/plain; charset=utf-8 $ file some-directory/* # entire contents of \u0026#34;some-directory\u0026#34; $ file -z flash. # content of compressed file file /dev/sda file -s /dev/sda file /dev/sda5 file -s /dev/sda5\rfind command Find a file.\nBasic usage:\nfind . -name search_text\rUsage\n# ignore text case find . -iname search_text # follow subdirectories to level X find . -name search_text -maxdepth X # only find files modified in last 4 days find . -name search_text -mtime 4 # only find empty find . -name search_text -type f\rgrep - Global Regular Expression Print Search a file to see if it contains a phrase.\nSearch file contents Search contents of a file with grep\rgrep SEARCH_TEXT FILE_NAME grep phrase log.txt\rCase insensitive search:\nUse case insensitive search with -i\rgrep -i \u0026#34;phrase\u0026#34; file_name.txt\rSearch current directory and all subdirectories:\nSearch files in all subdirectories with -R\rgrep -R \u0026#34;phrase\u0026#34; .\rSearch output of other commands Commands can be \u0026ldquo;piped in\u0026rdquo; to grep using the | operator.\n# display only matching files ls | grep SEARCH_TEXT ls | grep log.txt # use wildcards ls | grep *report.txt ls | grep *report* echo \u0026#34;tester\u0026#34; | grep \u0026#34;test\u0026#34;\rGet count of matching keyword grep -c \u0026#34;keyword\u0026#34; log.txt\rkill command Stop a process by process ID.\n# see processes running in terminal $ ps PID TTY TIME CMD 27894 ttys001 0:00.08 -bash # see process for all users and outside of terminal $ ps aux $ ps aux | grep chrome $ kill SIGNAL PID $ kill -9 ./applications-service # see all signal commands # 9 (SIGKILL) - is the most common command to terminate a process $ kill -l\rls - LiSt List contents of a directory.\nList current directory:\nls\rList a specified directory:\nls DIRECTORY ls /local/usr/bin\rls flags:\nls -l ls -lah # flags # list/table view with date, ownership, and size -l # list hidden files like .git (starting with a dot \u0026#34;.\u0026#34;) -a # human readable file sizes -h # list subdirectories -R # order by last modified -t # order by file size -lS # list in reverse order (combine with other commands) -r # show directories with \u0026#34;/\u0026#34; and executables with \u0026#34;*\u0026#34; -F # display inode number of file or directory -i\rlsof - LiSt Open Files Use it to check if a process is running on a given port.\nlsof -i :5432\rlsof may be in /usr/sbin and needs to be invoked directly (/usr/sbin/lsof -i :5432).\nsudo - Super User DO sudo allows a user to execute a command as the superuser or another user, within the specified security policy.\nIf a command fails because of insufficient prviledges, the sudo command can help. This does elevate permissions and should be used with caution.\nsudo COMMAND sudo rm -rf /data\rMore:\n# add a new user sudo useradd # create password for new user sudo passwd # add to a group sudo groupadd # delete user sudo userdel # delete group sudo groupdel # add user to a primary group sudo usermod -g\rman - MANual Show manual page for a given command. Use to show the manual for any bash command.\nman COMMAND man ls\rmkdir - MaKe DIRectory Creates a directory\nmkdir DIRECTORY_NAME mkdir some directory # copy subdirectories -p\rps - Process Status Use to see running processes.\n# show running terminal sessions ps\rShow all running processes ps aux # use grep to find process by keyword ps aux | grep keyword\r# check if a process is running ps auxwww | grep postgres\rpwd - Print Working Directory Displays the current directory you\u0026rsquo;re in.\n$ pwd ↪ /home/greg/documents\rrm - ReMove Delets a file or directory.\nrm FILE_NAME rm file.txt # remove directory rm -r directory_name # force remove file rm -f file_name\rrsync - Rsync rsync\nThe numeric method uses numbers to identify permissions and their position to identify the linux user, e.g., chmod 744 FILE_NAME.\nSync files locally Use rsync locally\rrsync -avh source/ target/\rThis will sync all the files in source/ to target/. If target/ does not exist it will be created.\n# local to local rsync [OPTIONS] SOURCE TARGET # local to remote (push) rsync [OPTIONS] SOURCE USER@HOST:TARGET # remote to local (pull) rsync [OPTIONS] USER@HOST:SOURCE TARGET # basic usage rsync -a /source/foo/ /target/foo/ # flags -a # archive mode - sync recursively -z # force compression to send to destination machine -P # progress bar and keep partially transferred files -q # quiet to supress non-error messages -v # verbose -h # human readable output # copy local source to remote target rsync -a /foo/ user@remote_host:/foo/ # copy from remote source to local target rsync -a user@remote_hostname:/foo/ /foo/ # use for large transfers and/or unstable internet connections rsync -aP source target # exclude \u0026#34;directory\u0026#34; and \u0026#34;another_dir\u0026#34; rsync -a --exclude=directory --exlucde=another_dir source target # include json files and no others rsync -ae ssh --include=\u0026#34;*.json\u0026#34; --exlucde=\u0026#34;*\u0026#34; source target # delete files that are not in source directory rsync -a --delete source target # rsync over ssh rsync -ae ssh user@remote_hostname:/foo/ /foo/ # SSH other than port 22 rsync -a -e \u0026#34;ssh -p 123\u0026#34; /source/ user@remote_hostname:/target/ # specify max file size for transfer rsync -a --max-size=\u0026#34;200k\u0026#34; source target # do dry run rsync --dry-run --remove-source-files -v source target # set bandwidth limit rsync --bw-limit=100 -a source target\rscp - Secure CoPy Secure copy is used to securely transfer files from one host to another.\nscp -i SOURCE TARGET scp -i /home/file.txt user@host:/app/data/file.txt\rsed - Stream EDitor Sed is a stream editor for filtering and replacing text. Sed probably has a shit tonne of stuff going on, used commonly for REGEX string matching and replacement.\n# syntax sed -i \u0026#34;s,TEXT_TO_FIND,TEXT_TO_REPLACE_WITH,g\u0026#34; # example usage sed -i \u0026#34;s,\u0026lt;VERSION_NUMBER\u0026gt;,2.1.15,g\u0026#34; someFile.txt\rssh - openSSH client Remote login program\nUse SSH with password prompt\rssh ubuntu@172.16.0.105\rUse SSH with key file\rssh -i keyName.pem ec2-user@whatever.amazonaws.com\rtail command Displays content of files; prints the last 10 lines of a file by default.\ntail FILE_NAME\rFollow a file: do not close tail when end of file is reached, but rather wait for additional input.\ntail -f FILE_NAME # also check if file is renamed or rotated tail -F FILE_NAME\rSpecify length: specify amount of file returned in lines, blocks, or bytes.\n# specify 50 lines tail -n 50 FILE_NAME # specify 50 blocks tail -b 50 FILE_NAME # specify 50 bytes tail -c 50 FILE_NAME\rtouch command Touch command creates a new file.\ntouch FILE_NAME\r# create multiple files touch FILE_NAME_1 FILE_NAME_2\rFull argument list\n-A Adjust access and modification timestamps of value with specified value. -a Change access time of file. Modification time is not changed, unless -m flag also specified. -c Do not create the file if it does not exist -d Change access and modification times -h If file has symbolic link, change time of the link itself, rather than the file the link points to. Using -h implies -c and will not create a new file. m Change modification time of file. Access time is not changed unless -a is also specified. r Use access and modification times from specified file instead of the current time of day. t Change access and modification times to the specified time instead of the current time of day. # Adjust access and modification timestamps of value with specified value touch # Change access time of file. Modification time is not changed, unless `-m` flag also specified. touch -a FILE_NAME\rtype command Type give the user information about the command type.\ntype [OPTIONS] FILE_NAME...\rxargs Can remove quotes form a\necho \u0026#39;\u0026#34;text\u0026#34;\u0026#39; | xargs ↪ text\rRandom Commands xargs\nCan remove quotes form a\necho \u0026#39;\u0026#34;text\u0026#34;\u0026#39; | xargs ↪ text\rRedirect Operators \u0026gt; - Redirect Output Operator Redirects the output of standard output (stdout) and writes it to a given file. If the file exists, it\u0026rsquo;s overwritten.\nRedirect standard output to a file:\n# list current directory and write results to file_directory.txt ls \u0026gt; file_directory.txt\rTypes of Output Different types of output\nNumber Type Abbreviation 0 Standard input stdin 1 Standard output stdout 2 Standard error stderr With the \u0026gt; operator, standard output (1) is assumed. Using \u0026gt; is the same as 1\u0026gt;.\nRedirect Standard Error Use 2\u0026gt; to redirect stderr:\ncommand 2\u0026gt; /dev/null\rRedirect Standard Error and Standard Output Use \u0026amp;\u0026gt; to redirect stderr and stdout:\ncommand \u0026amp;\u0026gt; /dev/null\rIn Bash, \u0026amp;\u0026gt; is the same as \u0026gt;\u0026amp;; however, the former is preferred.\nMore complex solutions achieve the same results\ncommand \u0026gt;file 2\u0026gt;\u0026amp;1\rWhat happened:\nThe first redirection \u0026gt;file prints stdout to file The second redirection 2\u0026gt;\u0026amp;1 duplicates file descriptor 2 to be the same as 1 Discard Output of Command command \u0026gt; /dev/null\rThe location /dev/null immediately deletes all data written to it. Also known as the \u0026ldquo;bit bucket.\u0026rdquo;\nDiscard output of stdout and stderr:\ncommand \u0026amp;\u0026gt; /dev/null # OR command \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\r\u0026gt;\u0026gt; - Redirect Output Append Operator Redirects the output of standard output (stdout) and writes it to a given file. If the file exists, it\u0026rsquo;s appended to the contents of the file.\n# list current directory and write results to file_directory.txt ls \u0026gt;\u0026gt; file_directory.txt\rRedirect Standard Error and Standard Output` Use \u0026amp;\u0026gt;\u0026gt; to redirect stderr and stdout:\ncommand \u0026amp;\u0026gt;\u0026gt; file.txt\r\u0026lt; - Regular Input Operator The input redirector pulls data in a stream from a given source.\nTake the given text file for example:\n# states.txt California Alaska New York\rUse above text file as input to sort command:\n$ sort \u0026lt; states.txt Alaska California New York\r| - Pipe Operator Takes the output of a given program and redirects it as input for another program. This is known as \u0026ldquo;piping\u0026rdquo;.\nUse ls to print directory contents and use grep to find matching regex pattern:\n# show files names that match \u0026#34;2004\u0026#34; ls | grep 2004\rhttps://catonmat.net/bash-one-liners-explained-part-three\n","date":"2023-01-03","id":7,"permalink":"/docs/bash/bash-reference/","summary":"Full Bash reference Command Name Description cat concatenate Display contents of text file; combine multiple files. cd change directory Traverse the directory tree and move to different folders chmod change mode Change file permissions cp copy Copy files or directories curl curl HTTP utility cut cut dig domain information grouper Show DNS information for a URL file file Display file type find find Find a file echo echo Print output to the terminal exit exit Exit current script or shell session export export Export a variable grep global regular expression print Use regex to find files kill kill Stop a running process ls list List contents of a directory lsof list open files See running processes, check ports mkdir mkdir Create a directory ps ps See status of running process pwd print working directory Show current directory rm remove Removes files or directories rsync rsync Sync files between locations or remotely sudo super user do Elevates a command to super user privileges scp secure copy Securely copy file between hosts sed stream editor Edit a text stream ssh ssh HTTP utility tail tail Print the end of a file touch touch Create a file type type See file or directory type xargs xargs Yarr Redirect Operators Redirect Syntax Reference command ARGUMENT [OPTIONS] [FILE_NAME.","tags":[],"title":"Bash Reference"},{"content":"Telegraf makes monitoring your infrastrcture drop dead simple. With a few configuration commands, this can be added.\nInputs Telegraf inputs are data sources, like information from a linux system, kubernetes cluster, docker container, ect.\nOutputs are where Telegraf sends that data to. We will use Influx as the database.\nSetup Influx DB Telegraf needs some sort of database to send data to, and we\u0026rsquo;ll use Influx DB. The Infflux db needs to be created before Telegraf is configured.\ndocker-compose.yml\rversion: \u0026#34;3.6\u0026#34; networks: default: driver: bridge ipam: driver: default services: influxdb: container_name: influxdb image: \u0026#34;influxdb:1.8\u0026#34; restart: unless-stopped ports: - \u0026#34;8086:8086\u0026#34; environment: - TZ=Etc/UTC - INFLUXDB_HTTP_FLUX_ENABLED=false - INFLUXDB_REPORTING_DISABLED=false - INFLUXDB_HTTP_AUTH_ENABLED=false - INFLUXDB_MONITOR_STORE_ENABLED=FALSE # - INFLUX_USERNAME=user # - INFLUX_PASSWORD=pass # - INFLUXDB_UDP_ENABLED=false # - INFLUXDB_UDP_BIND_ADDRESS=0.0.0.0:8086 # - INFLUXDB_UDP_DATABASE=udp volumes: - ./volumes/influxdb/data:/var/lib/influxdb - ./backups/influxdb/db:/var/lib/influxdb/backup healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;http://localhost:8086\u0026#34;] interval: 30s timeout: 10s retries: 3 start_period: 30s\rStart docker-compose stack\rdocker-compose up -d\rInstall Telegraf Install Telegraf on Ubuntu\r# enable Telegraf to start on boot sudo systemctl enable telegraf\r","date":"2024-07-05","id":8,"permalink":"/blog/telegraf/","summary":"Telegraf makes infrastructure monitoring simple","tags":["telegraf","observability"],"title":"Telegraf"},{"content":"Well-thought-through product announcements will help increase feature awareness and engage users with new functionality. Just like sharing your public roadmap, it\u0026rsquo;s also a great way to let potential customers see that you\u0026rsquo;re constantly improving.\nFurther reading Read How to announce product updates and features ","date":"2023-09-07","id":9,"permalink":"/blog/example-post/","summary":"You can use blog posts for announcing product updates and features.","tags":[],"title":"Example Post"},{"content":"","date":"2023-09-07","id":10,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"IP Addresses IPv4 addresses are like 192.168.10.55 are decimal representations of binary form.\nThey are composed of four binary blocks. Each block is 8 bits (or 1 byte), which specifies a number from 0 to 255. An IPv4 addresses is represented by four blocks for 32 bits (or 4 bytes).\n172 . 16 . 50 . 1\r10101100.0001000.00110010.00000001\r| 8 bits |\r|------- 32 bits (4 bytes) --------|\rConvert decimal to binary form:\n128 62 32 16 8 4 2 1 X X X X X X X X Private IP Ranges Private IP addresses are used for residential and enterprise networks. They cannot be seen outside the private network.\nIn a typical home network, an ISP allocates a single public (routable) IPv4 address. In turn, each device on the network is assigned a private IP address, allowing the single public IP address to be used across multiple devices.\nStart End 10.0.0.0 10.255.255.255 172.16.0.0 172.31.255.255 192.168.0.0 192.168.255.255 Subnet Masks CIDR Subnet mask Wildcard mask No. IP addres No. usable IP addresses /32 255.255.255.255 0.0.0.0 1 1 /31 255.255.255.254 0.0.0.1 2 2* /30 255.255.255.252 0.0.0.3 4 2 /29 255.255.255.248 0.0.0.7 8 6 /28 255.255.255.240 0.0.0.15 16 14 /27 255.255.255.224 0.0.0.31 32 30 /26 255.255.255.192 0.0.0.63 64 62 /25 255.255.255.128 0.0.0.127 128 126 /24 255.255.255.0 0.0.0.255 256 254 /23 255.255.254.0 0.0.1.255 512 510 /22 255.255.252.0 0.0.3.255 1,024 1,022 /21 255.255.248.0 0.0.7.255 2,048 2,046 /20 255.255.240.0 0.0.15.255 4,096 4,094 /19 255.255.224.0 0.0.31.255 8,192 8,190 /18 255.255.192.0 0.0.63.255 16,384 16,382 /17 255.255.128.0 0.0.127.255 32,768 32,766 /16 255.255.0.0 0.0.255.255 65,536 65,534 /15 255.254.0.0 0.1.255.255 131,072 131,070 /14 255.252.0.0 0.3.255.255 262,144 262,142 /13 255.248.0.0 0.7.255.255 524,288 524,286 /12 255.240.0.0 0.15.255.255 1,048,576 1,048,574 /11 255.224.0.0 0.31.255.255 2,097,152 2,097,150 /10 255.192.0.0 0.63.255.255 4,194,304 4,194,302 /9 255.128.0.0 0.127.255.255 8,388,608 8,388,606 /8 255.0.0.0 0.255.255.255 16,777,216 16,777,214 /7 254.0.0.0 1.255.255.255 33,554,432 33,554,430 /6 252.0.0.0 3.255.255.255 67,108,864 67,108,862 /5 248.0.0.0 7.255.255.255 134,217,728 134,217,726 /4 240.0.0.0 15.255.255.255 268,435,456 268,435,454 /3 224.0.0.0 31.255.255.255 536,870,912 536,870,910 /2 192.0.0.0 63.255.255.255 1,073,741,824 1,073,741,822 /1 128.0.0.0 127.255.255.255 2,147,483,648 2,147,483,646 /0 0.0.0.0 255.255.255.255 4,294,967,296 4,294,967,294 ","date":"2024-06-30","id":11,"permalink":"/docs/networking/networking-basics/","summary":"IP Addresses IPv4 addresses are like 192.168.10.55 are decimal representations of binary form.\nThey are composed of four binary blocks. Each block is 8 bits (or 1 byte), which specifies a number from 0 to 255.","tags":[],"title":"Networking Basics"},{"content":"","date":"2023-06-26","id":12,"permalink":"/docs/bash/","summary":"","tags":[],"title":"Bash"},{"content":"","date":"2020-10-06","id":13,"permalink":"/docs/help/","summary":"","tags":[],"title":"Help"},{"content":"","date":"2020-10-06","id":14,"permalink":"/docs/js/","summary":"","tags":[],"title":"Javascript"},{"content":"","date":"2020-10-06","id":15,"permalink":"/docs/linux/","summary":"","tags":[],"title":"Linux"},{"content":"\rCheck for outdated packages The npm outdated command will check the registry to see if any (or, specific) installed packages are currently outdated:\nnpm outdated [[\u0026lt;@scope\u0026gt;/]\u0026lt;pkg\u0026gt; ...]\rUpdate packages The npm update command will update all the packages listed to the latest version (specified by the tag config), respecting semver:\nnpm update [\u0026lt;pkg\u0026gt;...]\r","date":"2020-11-12","id":16,"permalink":"/docs/help/how-to-update/","summary":"Check for outdated packages The npm outdated command will check the registry to see if any (or, specific) installed packages are currently outdated:","tags":[],"title":"How to Update"},{"content":"Problems updating npm packages Delete the ./node_modules folder, and run again:\nnpm install\rProblems with cache Delete the temporary directories:\nnpm run clean\r","date":"2020-11-12","id":17,"permalink":"/docs/help/troubleshooting/","summary":"Problems updating npm packages Delete the ./node_modules folder, and run again:\nnpm install\rProblems with cache Delete the temporary directories:","tags":[],"title":"Troubleshooting"},{"content":"Hyas? Doks is a Hyas theme build by the creator of Hyas.\nFooter notice? Please keep it in place.\nKeyboard shortcuts for search? focus: Ctrl + / select: ↓ and ↑ open: Enter close: Esc Other documentation? Netlify Hugo Can I get support? Create a topic:\nNetlify Community Hugo Forums Doks Discussions Contact the creator? Send h-enk a message:\nNetlify Community Hugo Forums Doks Discussions ","date":"2020-10-06","id":18,"permalink":"/docs/help/faq/","summary":"Hyas? Doks is a Hyas theme build by the creator of Hyas.\nFooter notice? Please keep it in place.\nKeyboard shortcuts for search?","tags":[],"title":"FAQ"},{"content":"","date":"2023-09-07","id":19,"permalink":"/docs/guides/","summary":"","tags":[],"title":"Guides"},{"content":"Guides lead a user through a specific task they want to accomplish, often with a sequence of steps. Writing a good guide requires thinking about what your users are trying to do.\nFurther readinghh Read about how-to guides in the Diátaxis framework ","date":"2023-09-07","id":20,"permalink":"/docs/guides/test/","summary":"Guides lead a user through a specific task they want to accomplish, often with a sequence of steps. Writing a good guide requires thinking about what your users are trying to do.","tags":[],"title":"Test"},{"content":"","date":"2023-09-07","id":21,"permalink":"/docs/docker/","summary":"","tags":[],"title":"Docker"},{"content":"","date":"2023-09-07","id":22,"permalink":"/docs/reference/","summary":"","tags":[],"title":"Reference"},{"content":"Reference pages are ideal for outlining how things work in terse and clear terms. Less concerned with telling a story or addressing a specific use case, they should give a comprehensive outline of what your documenting.\nFurther reading Read about reference in the Diátaxis framework ","date":"2023-09-07","id":23,"permalink":"/docs/reference/example-reference/","summary":"Reference pages are ideal for outlining how things work in terse and clear terms. Less concerned with telling a story or addressing a specific use case, they should give a comprehensive outline of what your documenting.","tags":[],"title":"Example Reference"},{"content":"Webfonts: https://gwfh.mranftl.com/fonts/roboto?subsets=latin\nBasic /content/about.md\r--- title: About --- Basic about page.\rBasic Layout Create a single layout page\n/layouts/_default/list.html\r\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;{{ .Page.Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\rUse multiple blocks /layouts/_default/baseof.html\r\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;{{ .Page.Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{ block \u0026#34;main\u0026#34; . }} {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\r/layouts/_default/list.html\r{{ define \u0026#34;main\u0026#34; }} {{ .Content }} {{ end }}\rAdd CSS styles /layouts/_default/baseof.html\r{{ $style := resources.Get \u0026#34;sass/main.scss\u0026#34; | resources.ToCSS | resources.Minify }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ $style.Permalink }}\u0026#34;\u0026gt;\rGeneral Templates Output from front matter This is in the top of the stuff\n/layouts/_default/baseof.html\r\u0026lt;h2\u0026gt;{{ .Params.summary }}\u0026lt;/h2\u0026gt;\rConditionals (If statements) Use an if/else statement to evaludate conditional logic.\n/layouts/_default/baseof.html\r{{ if isset .Params \u0026#34;title\u0026#34; }} \u0026lt;title\u0026gt;{{ .Params.title }}\u0026lt;/title\u0026gt; {{ else }} \u0026lt;title\u0026gt;{{ .Site.title }}\u0026lt;/title\u0026gt; {{ end }}\rIn the above example, the template checks if title is set in the page\u0026rsquo;s content. If so, that becomes the title, else it falls back to the global, site-wide title.\nSet and reference variable Set and reference a variable with a $ sign.\n{{ $favorite_day := \u0026#34;Friday\u0026#34; }} {{ $favorite_day }}\rLoop {{ $states := slice \u0026#34;california\u0026#34; \u0026#34;new york\u0026#34; \u0026#34;kansas\u0026#34; \u0026#34;florida\u0026#34; }} \u0026lt;ul\u0026gt; {{ range $states }} \u0026lt;li\u0026gt;{{ . }}\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt;\rUse with for nested key values --- title: Appearance apperance: eyes: green snoot: boopable whiskers: true limbs: - claws: 5 side: left position: front ---\r{{ with .Params.appearance }} \u0026lt;h3\u0026gt;My top appearance traits\u0026lt;/h3\u0026gt; \u0026lt;dl\u0026gt; \u0026lt;dt\u0026gt;Whiskers\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt;{{ .whiskers }}\u0026lt;/dd\u0026gt; {{ with .limbs }} \u0026lt;dt\u0026gt;Claws\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt; \u0026lt;ul\u0026gt; {{ range . }} \u0026lt;li\u0026gt;{{ .position }} {{ .side }} {{ .claws }\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/dd\u0026gt; {{ end }} \u0026lt;/dl\u0026gt; {{ end }}\r","date":"2024-07-05","id":24,"permalink":"/docs/hugo/templates/","summary":"Webfonts: https://gwfh.mranftl.com/fonts/roboto?subsets=latin\nBasic /content/about.md\r--- title: About --- Basic about page.\rBasic Layout Create a single layout page\n/layouts/_default/list.html\r\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;{{ .","tags":[],"title":"Templates"},{"content":"","date":"2024-07-05","id":25,"permalink":"/docs/hugo/","summary":"","tags":[],"title":"Hugo"},{"content":"systemd is a process and service manager.\nIt can be used to start a webserver on boot, run a bash script, mount a drive automatically, and many more things.\nsystemctl is the CLI that fronts systemd. Unit files are used to find systemd processes.\nSystemctl systemctl is the CLI to manage systemd.\nsudo systemctl status docker sudo systemctl status docker.service # start a service sudo systemctl start docker # stop a service sudo systemctl stop docker # restart a service sudo systemctl restart docker # reload config without restarting sudo systemctl reload docker # start service on boot sudo systemctl enable docker # disable start on boot sudo systemctl disabled docker # reload daemon sudo systemctl daemon-reload\rUnit Files A unit is some resource that systemd knows how to operate and manage.\nTypical location: /etc/systemd/system/SERVICE_NAME.service\nTypes of Unit Files The .service unit file is the most common, but there are a variety of types:\n.service - how to manage a service or application on the box. Includes how to start/stop service, when to start it, and dependency order .socket - network or IPC socket, or FIFO buffer that systemd uses for socket-based activation. .device - manages devces with udev or sysfs filesystem .mount - manages a mountpoint. Named after the mount path .automount - manages a mountpoint to be automatically mounted .swap - manages swap space on system .path - defines a path that can be used for path-based activation .snapshot - created by systemctl snapshot command. Allows for snapshot of system state when making changes. Snapshots do not persist between sessions and are used to rollback temporary states. .slice - related to Linux Control Group nodes .scope - created automatically from info recieved from bus interfaces Example Unit File /etc/systemd/system/SERVICE_NAME.service\r[Unit] Description=Docker compose service After=docker.service Requires=docker.service [Service] User=USER_NAME Group=USER_NAME TimeoutStartSec=infinity TimeoutStopSec=5min Restart=always RestartSec=3 WorkingDirectory=/path/to/Dockerfile ExecStart=/usr/bin/docker compose -p STACK_NAME up ExecStop=/usr/bin/docker compose -p STACK_NAME down [Install] WantedBy=multi-user.target\rOverriding a unit file Override specific aspects of a unit file with a snippets in a subdirectory. For a unit named SERVICE_NAME.service, create a subdirectory SERVICE_NAME.service.d, and create a file within it ending in .conf.\nUnit File Syntax Unit file syntax definitions\r[Section] Directive1=value Directive2=value2\rSection names are case sensitive [Unit] Section The [Unit] section is the beginning of the unit file and defines the metadata and relationship to other units.\nDescription=: This directive can be used to describe the name and basic functionality of the unit. It is returned by various systemd tools, so it is good to set this to something short, specific, and informative. Documentation=: This directive provides a location for a list of URIs for documentation. These can be either internally available man pages or web accessible URLs. The systemctl status command will expose this information, allowing for easy discoverability. Requires=: This directive lists any units upon which this unit essentially depends. If the current unit is activated, the units listed here must successfully activate as well, else this unit will fail. These units are started in parallel with the current unit by default. Wants=: This directive is similar to Requires=, but less strict. Systemd will attempt to start any units listed here when this unit is activated. If these units are not found or fail to start, the current unit will continue to function. This is the recommended way to configure most dependency relationships. Again, this implies a parallel activation unless modified by other directives. BindsTo=: This directive is similar to Requires=, but also causes the current unit to stop when the associated unit terminates. Before=: The units listed in this directive will not be started until the current unit is marked as started if they are activated at the same time. This does not imply a dependency relationship and must be used in conjunction with one of the above directives if this is desired. After=: The units listed in this directive will be started before starting the current unit. This does not imply a dependency relationship and one must be established through the above directives if this is required. Conflicts=: This can be used to list units that cannot be run at the same time as the current unit. Starting a unit with this relationship will cause the other units to be stopped. Condition...=: There are a number of directives that start with Condition which allow the administrator to test certain conditions prior to starting the unit. This can be used to provide a generic unit file that will only be run when on appropriate systems. If the condition is not met, the unit is gracefully skipped. Assert...=: Similar to the directives that start with Condition, these directives check for different aspects of the running environment to decide whether the unit should activate. However, unlike the Condition directives, a negative result causes a failure with this directive. [Install] Section WantedBy=: The WantedBy= directive is the most common way to specify how a unit should be enabled. This directive allows you to specify a dependency relationship in a similar way to the Wants= directive does in the [Unit] section. The difference is that this directive is included in the ancillary unit allowing the primary unit listed to remain relatively clean. When a unit with this directive is enabled, a directory will be created within /etc/systemd/system named after the specified unit with .wants appended to the end. Within this, a symbolic link to the current unit will be created, creating the dependency. For instance, if the current unit has WantedBy=multi-user.target, a directory called multi-user.target.wants will be created within /etc/systemd/system (if not already available) and a symbolic link to the current unit will be placed within. Disabling this unit removes the link and removes the dependency relationship. RequiredBy=: This directive is very similar to the WantedBy= directive, but instead specifies a required dependency that will cause the activation to fail if not met. When enabled, a unit with this directive will create a directory ending with .requires. Alias=: This directive allows the unit to be enabled under another name as well. Among other uses, this allows multiple providers of a function to be available, so that related units can look for any provider of the common aliased name. Also=: This directive allows units to be enabled or disabled as a set. Supporting units that should always be available when this unit is active can be listed here. They will be managed as a group for installation tasks. DefaultInstance=: For template units (covered later) which can produce unit instances with unpredictable names, this can be used as a fallback value for the name if an appropriate name is not provided. Unit-specific Sections Service\nType= Can be one of:\nsimple: The main process of the service is specified in the start line. This is the default if the Type= and Busname= directives are not set, but the ExecStart= is set. Any communication should be handled outside of the unit through a second unit of the appropriate type (like through a .socket unit if this unit must communicate using sockets). forking: This service type is used when the service forks a child process, exiting the parent process almost immediately. This tells systemd that the process is still running even though the parent exited. oneshot: This type indicates that the process will be short-lived and that systemd should wait for the process to exit before continuing on with other units. This is the default Type= and ExecStart= are not set. It is used for one-off tasks. dbus: This indicates that unit will take a name on the D-Bus bus. When this happens, systemd will continue to process the next unit. notify: This indicates that the service will issue a notification when it has finished starting up. The systemd process will wait for this to happen before proceeding to other units. idle: This indicates that the service will not be run until all jobs are dispatched. Additional Settings:\nRemainAfterExit=: This directive is commonly used with the oneshot type. It indicates that the service should be considered active even after the process exits. PIDFile=: If the service type is marked as “forking”, this directive is used to set the path of the file that should contain the process ID number of the main child that should be monitored. BusName=: This directive should be set to the D-Bus bus name that the service will attempt to acquire when using the “dbus” service type. NotifyAccess=: This specifies access to the socket that should be used to listen for notifications when the “notify” service type is selected This can be “none”, “main”, or \u0026ldquo;all. The default, “none”, ignores all status messages. The “main” option will listen to messages from the main process and the “all” option will cause all members of the service’s control group to be processed. Manage Services:\nExecStart=: This specifies the full path and the arguments of the command to be executed to start the process. This may only be specified once (except for “oneshot” services). If the path to the command is preceded by a dash “-” character, non-zero exit statuses will be accepted without marking the unit activation as failed. ExecStartPre=: This can be used to provide additional commands that should be executed before the main process is started. This can be used multiple times. Again, commands must specify a full path and they can be preceded by “-” to indicate that the failure of the command will be tolerated. ExecStartPost=: This has the same exact qualities as ExecStartPre= except that it specifies commands that will be run after the main process is started. ExecReload=: This optional directive indicates the command necessary to reload the configuration of the service if available. ExecStop=: This indicates the command needed to stop the service. If this is not given, the process will be killed immediately when the service is stopped. ExecStopPost=: This can be used to specify commands to execute following the stop command. RestartSec=: If automatically restarting the service is enabled, this specifies the amount of time to wait before attempting to restart the service. Restart=: This indicates the circumstances under which systemd will attempt to automatically restart the service. This can be set to values like “always”, “on-success”, “on-failure”, “on-abnormal”, “on-abort”, or “on-watchdog”. These will trigger a restart according to the way that the service was stopped. TimeoutSec=: This configures the amount of time that systemd will wait when stopping or stopping the service before marking it as failed or forcefully killing it. You can set separate timeouts with TimeoutStartSec= and TimeoutStopSec= as well. ","date":"2024-07-02","id":26,"permalink":"/docs/linux/systemd/","summary":"systemd is a process and service manager.\nIt can be used to start a webserver on boot, run a bash script, mount a drive automatically, and many more things.","tags":[],"title":"Systemd"},{"content":"","date":"2024-07-01","id":27,"permalink":"/docs/portal/subdir/subfile/","summary":"","tags":[],"title":"Subfile"},{"content":"","date":"2024-07-01","id":28,"permalink":"/docs/portal/subdir/","summary":"","tags":[],"title":"Subdir"},{"content":"","date":"2024-07-01","id":29,"permalink":"/docs/devops/","summary":"","tags":[],"title":"Devops"},{"content":"Git CLI commands Basic git flow commands\r# see current branch and file status git status # add commits to staging git add . # add commit message git commit -m \u0026#34;fix: update date selector\u0026#34; # push git push git push -u REMOTE_NAME BRANCH_NAME git push -u origin main\rBranches Basic git branch commands\r# see current branches git branch # see all remote branches git branch -a # See what braches are tracking upstream branches git branch -vv # create new branch (does not checkout to new branch) git branch NEW_BRANCH_NAME # rename current branch git branch -m NEW_BRANCH_NAME\rDelete git branch\r# delete git branch git branch -d \u0026lt;branchName\u0026gt; # force delete git branch (with unmerged changes) git branch -D \u0026lt;branchName\u0026gt;\rCheckout to different branch Use git checkout to change to a new or existing branch.\nCheckout to different git branch\r# checkout to existing branch git checkout EXISTING_BRANCH_NAME # create and checkout to new branch git checkout -b NEW_BRANCH_NAME\rInitial repository setup git init git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin git@github.com:USERNAME/REPO_NAME.git git push -u origin main\rStaging Changes Before commiting changes, they must be staged for commit.\nStage changes for commit\r# git add all changes at current dir (and below) git add . # git add all changes in repo git add -A # git add specific files or dirs git add dir/\rUnstage changes for commit\r# unstage all changes git reset HEAD # unstage specific changes git reset HEAD dir/file.txt\rUnstage changes for commit and delete all changes\rgit reset HEAD git checkout .\rGit Config Git Config stores metadata - globally and per repository.\ngit config --list git config --global user.email \u0026#34;your_email@example.com\u0026#34; # add git config git config --add core.sshcommand \u0026#34;ssh -i ~/.ssh/github\u0026#34; # remove git config git config --unset remote.github.url\rRemotes Remotes refer to the location of a repository stored on an online tool like GitHub or GitLab, versus the local copy stored on each users machine. When pushing and pulling, the command is targeting a remote.\nMost repos push and pull from a single remote called origin. In more advanced use cases, multiple remotes are used for repos stored in multiple locations.\n# see remote origin git remote show origin # add another remote git remote add REMOTE_NAME REMOTE_URL git remote add origin git@github.com:USERNAME/REPO_NAME.git git remote add second-remote git@github.com:USERNAME/REPO_NAME.git\rRemove git remote\rgit remote remove origin\rTag Commit # create a tag git tag -a v0.1.0 -m \u0026#34;Version 0.1.0 - do a thing\u0026#34; # push tags to remote git push --tags\rPulling without pull strategy set warning: Pulling without specifying how to reconcile divergent branches is\rdiscouraged. You can squelch this message by running one of the following\rcommands sometime before your next pull:\rgit config pull.rebase false # merge (the default strategy)\rgit config pull.rebase true # rebase\rgit config pull.ff only # fast-forward only\rYou can replace \u0026#34;git config\u0026#34; with \u0026#34;git config --global\u0026#34; to set a default\rpreference for all repositories. You can also pass --rebase, --no-rebase,\ror --ff-only on the command line to override the configured default per\rinvocation.\rAuthenticate with External Git Providers Different methods for authenticating with external git providers like GitHub.\nConfigure a remote to use a PAT Use a Personal Access Token to authenticate over HTTPS with an external git provider like GitHub.\nThis is useful for private repositories that cannot use password or SSH based authentication.\nAuthenticate with PAT\r# run `git remote add` command first as normal git remote set-url origin https://USER_NAME:PAT@github.com/ORG_NAME/REPO_NAME.git\rUSER_NAME - GitHub username of user that owns/created PAT PAT - Classic GitHub PAT (Personal access token). This token can additionally be set to authenticate with SSO if within a company managed GH org. ORG_NAME - GitHub Org name, or username if not with a GH org REPO_NAME - Name of GH repository Specify a certain SSH key Use a specific SSH key to authenticate over SSH with an external git provider like GitHub.\nAuthenticate with specific SSH key\r# set given SSH key for current repo git config core.sshCommand \u0026#34;ssh -i /path/to/keyfile\u0026#34; # set given SSH key for all repos git config --global core.sshCommand \u0026#34;ssh -i /path/to/keyfile\u0026#34;\rSupport for core.sshCommand is only available in git version 2.10.0 and above.\n","date":"2024-07-01","id":30,"permalink":"/docs/devops/git/","summary":"Git CLI commands Basic git flow commands\r# see current branch and file status git status # add commits to staging git add .","tags":[],"title":"Git"},{"content":"bash \u0026lt;(curl -s https://gitlab.com/NickBusey/HomelabOS/-/raw/master/install_homelabos.sh)\n========== Preparing HomelabOS docker image ========== ========== This account is NOT in the docker group ========== Do you want to add yourself to the docker group? [y/n] y docker:x:997: ========== You must log out and back in for changes to take effect. ========== ========== You may need to reboot entirely if you still get permission denied. ========== make: *** [Makefile:38: build] Error 1 You can check the status of Organizr with \u0026#39;systemctl status organizr\u0026#39; or \u0026#39;sudo docker ps\u0026#39; To enable more services, run \u0026#39;cd /var/homelabos/install\u0026#39; then \u0026#39;make set servicename.enable true\u0026#39; where servicename is a service you would like to have. Example: \u0026#39;make set miniflux.enable true\u0026#39; Once you have enabled all the services you would like, simply run \u0026#39;make deploy\u0026#39;. ================== Done. ==================\r","date":"2024-07-01","id":31,"permalink":"/docs/portal/homelab/","summary":"bash \u0026lt;(curl -s https://gitlab.com/NickBusey/HomelabOS/-/raw/master/install_homelabos.sh)\n========== Preparing HomelabOS docker image ========== ========== This account is NOT in the docker group ========== Do you want to add yourself to the docker group?","tags":[],"title":"Homelab"},{"content":"Use the left sidebar to select a topic.\n","date":"2024-07-01","id":32,"permalink":"/docs/guides/get-started-in-wynotes/","summary":"Use the left sidebar to select a topic.","tags":[],"title":"Get Started in WyNotes"},{"content":"Use Telegraf as the agent to collect system logs and metrics.\nSetup Telegraf Monitoring Currently Telegraf was setup in the IOTstack. This runs inside the docker compose network, and collects metrics about what’s happening in the network.\nCreate Influx DB Install Telegraf Write configuration file Start and enable with systemctl Create Influxdb Databases Create a database in Influx db that Telegraf will post to. The database should be created before instantiating Telegraf or there’ll be nowhere to send the data to.\nInteract with Influxdb CLI\r# exec into influx container sudo docker exec -it influxdb influx # create new database create database machine_metrics # use database machine_metrics use machine_metrics # show all databases show databases show series show measurements\t# like tables show tag keys\t# show tag keys for each measurement show field keys\t# like column names and datatypes for each measurement\rInstall Telegraf Telegraf can be installed in a Docker container or as an apt package and managed through systemd.\nInstall telegraf from InfluxData apt repo\rcurl -s https://repos.influxdata.com/influxdata-archive.key \u0026gt; influxdata-archive.key echo \u0026#39;943666881a1b8d9b849b74caebf02d3465d6beb716510d86a39f6c8e8dac7515 influxdata-archive.key\u0026#39; | sha256sum -c \u0026amp;\u0026amp; cat influxdata-archive.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/influxdata-archive.gpg \u0026gt; /dev/null echo \u0026#39;deb [signed-by=/etc/apt/trusted.gpg.d/influxdata-archive.gpg] https://repos.influxdata.com/debian stable main\u0026#39; | sudo tee /etc/apt/sources.list.d/influxdata.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install telegraf\rConfigure Telegraf Default Location Telegraf config sits in /etc/telegraf/telegraf.conf\nThe /etc/telegraf/telegraf.conf file drives what metrics Telegraf selects and any filters or conditions. Restart Telegraf when updating this file for changes to take effect.\nList of all Telegraf inputs List of Telegraf input plugins: GitHub\nA sample Telegraf reference can be found in /etc/telegraf/telegraf.conf.sample. It is a full list of inputs and configurations for reference only. The file is read only and changes to it are overridden.\nGenerate default config file:\ntelegraf config \u0026gt; telegraf.conf\rThe Telegraf systemd service needs to be restarted after telegraf.conf is updated.\nsudo systemctl restart telegraf\rExample /etc/telegraf/telegraf.conf\r[global_tags] [agent] interval = \u0026#34;10s\u0026#34; round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \u0026#34;0s\u0026#34; flush_interval = \u0026#34;10s\u0026#34; flush_jitter = \u0026#34;0s\u0026#34; precision = \u0026#34;0s\u0026#34; hostname = \u0026#34;\u0026#34; omit_hostname = false [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = false core_tags = false [[inputs.disk]] ignore_fs = [\u0026#34;tmpfs\u0026#34;, \u0026#34;devtmpfs\u0026#34;, \u0026#34;devfs\u0026#34;, \u0026#34;iso9660\u0026#34;, \u0026#34;overlay\u0026#34;, \u0026#34;aufs\u0026#34;, \u0026#34;squashfs\u0026#34;] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] use_sudo = false [[inputs.swap]] [[inputs.system]] [[inputs.file]] files = [\u0026#34;/sys/class/thermal/thermal_zone0/temp\u0026#34;] name_override = \u0026#34;cpu_temperature\u0026#34; data_format = \u0026#34;value\u0026#34; data_type = \u0026#34;integer\u0026#34; [[inputs.net]] [[outputs.influxdb]] urls = [\u0026#34;http://localhost:8086\u0026#34;] database = \u0026#34;machine_metrics\u0026#34; timeout = \u0026#34;5s\u0026#34; # added to get TCP connections [[inputs.netstat]] # added for kubernetes [[inputs.kubernetes]] url=\u0026#34;http://192.168.1.100:10250\u0026#34; insecure_skip_verify = true bearer_token_string = \u0026#34;XXXXXXXXXXXXXXXXXXX\u0026#34;\rDocker Telegraf configs /home/user/IOTstack/volumes/telegraf/telegraf.conf\r[global_tags] [agent] interval = \u0026#34;10s\u0026#34; round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \u0026#34;0s\u0026#34; flush_interval = \u0026#34;10s\u0026#34; flush_jitter = \u0026#34;0s\u0026#34; precision = \u0026#34;0s\u0026#34; hostname = \u0026#34;\u0026#34; omit_hostname = false [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = false core_tags = false [[inputs.disk]] ignore_fs = [\u0026#34;tmpfs\u0026#34;, \u0026#34;devtmpfs\u0026#34;, \u0026#34;devfs\u0026#34;, \u0026#34;iso9660\u0026#34;, \u0026#34;overlay\u0026#34;, \u0026#34;aufs\u0026#34;, \u0026#34;squashfs\u0026#34;] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] use_sudo = false [[inputs.swap]] [[inputs.system]] [[inputs.file]] files = [\u0026#34;/sys/class/thermal/thermal_zone0/temp\u0026#34;] name_override = \u0026#34;cpu_temperature\u0026#34; data_format = \u0026#34;value\u0026#34; data_type = \u0026#34;integer\u0026#34; [[inputs.docker]] endpoint = \u0026#34;unix:///var/run/docker.sock\u0026#34; gather_services = false source_tag = false container_name_include = [] container_name_exclude = [] timeout = \u0026#34;5s\u0026#34; perdevice = false total_include = [\u0026#34;cpu\u0026#34;, \u0026#34;blkio\u0026#34;, \u0026#34;network\u0026#34;] docker_label_include = [] docker_label_exclude = [] tag_env = [\u0026#34;HEAP_SIZE\u0026#34;] [[inputs.net]] [[outputs.influxdb]] urls = [\u0026#34;http://influxdb:8086\u0026#34;] database = \u0026#34;sys_metrics\u0026#34; timeout = \u0026#34;5s\u0026#34;\r","date":"2024-06-24","id":33,"permalink":"/docs/linux/telegraf/","summary":"Use Telegraf as the agent to collect system logs and metrics.\nSetup Telegraf Monitoring Currently Telegraf was setup in the IOTstack.","tags":[],"title":"Telegraf"},{"content":"Networking Set Static IP A static IP is an IP address that doesn’t change. This allows a machine to set its own IP address, rather than be assigned a random one by the router. This means it will always be available at the same IP address, like 192.168.1.1. This is contrasted with DHCP, where the IP is assigned randomly by the router.\nNetwork Manager Network Manager is the more modern version of managing network configs like IP addresses. Network configurations are stored as YAML files and then applied using Network Manager.\nNetwork Manager needs to be installed from apt before using.\nInstalling network manager\r# Install Network Manager with apt sudo apt install network-manager -y\rThe YAML file defines the network configs like IP address, gateway, and DNS servers.\nNetwork Manager YAML example\r# scripts/network/01-static-ip.yaml network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: no addresses: - 192.168.1.42/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8 - 8.8.8.4\rhttps://wiki.archlinux.org/title/NetworkManager\nInterface File Updating the interfaces file is the \u0026ldquo;old\u0026rdquo; way of setting a static IP.\nFile locations\nIP addresses are set in /etc/network/interfaces. DNS servers are set in /etc/resolv.conf.\nDyanmic IP address in /etc/network/interfaces\rauto eth0 iface eth0 inet dhcp\rDyanmic IP address in /etc/network/interfaces\rauto eth0 iface eth0 inet static address 192.168.0.100 netmask 255.255.255.0 gateway 192.168.0.1 dns-nameservers 8.8.8.8 dns-nameservers 8.8.4.4\rDNS nameservers in /etc/resolv.conf\rnameserver 208.67.222.222 nameserver 208.67.220.220\r{ \u0026#34;title\u0026#34;: \u0026#34;SIDECAR METADATA\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Discover or synchrnoize...\u0026#34;, \u0026#34;active\u0026#34;: 0, \u0026#34;waiting\u0026#34;: 0 }\r{ \u0026#34;storage\u0026#34;: { \u0026#34;used\u0026#34;: 28.6, \u0026#34;total\u0026#34;: 45 }, \u0026#34;server\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;Online\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v1.106.4\u0026#34; } }\rUsers Manage Users Create and delete a user\r# create user sudo adduser USER_NAME # delete user sudo deluser USER_NAME # set password sudo passwd USER_NAME\rRunning adduser without arguments will create an interactive prompt for the password and full name.\nCreate user options\r# create user sudo adduser USER_NAME sudo adduser --uid USER_ID USER_NAME sudo adduser --system USER_NAME\rAvoid providing password with -p flag\nWhen using the -p flag, the password should already be encrypted. The passwd command is the preferred option.\nUser Location File Name Use /etc/passwd Location of users /etc/shadow Location of hashed user passwords /etc/passwd file syntax\rusername:password:UID:GID:comment:home:shell duke:x:1001:1001::/home/duke/:/bin/bash\rPasswords were previously stored in /etc/passwd, making them viewable by everyone. The encrypted passwords are now stored in /etc/shadow.\nManage User Groups Create a New Group Create user group\nCreate new group\rsudo groupadd GROUP_NAME\rAssign User to Group Add user to group. User and group must already exist.\nAdd existing user to group\rsudo adduser USER_NAME GROUP_NAME # alternate sudo useradd -G GROUP_NAME USER_NAME # alternate sudo usermod -a -G GROUP_NAME USER_NAME\rAdd user to multiple groups\nAdd user to multiple groups\rsudo usermod -a -G GROUP_NAME_1,GROUP_NAME_2 USER_NAME\rCreate new user and assign to group Create a new user and add to existing group. Group must already exist.\nCreate new user and assign to group\rsudo useradd -G GROUP_NAME USER_NAME # give new user a password sudo passwd USER_NAME\rChange users primary group Change users primary group\nChange user\u0026#39;s primary group\rsudo usermod -g GROUP_NAME USER_NAME\rRemove user from group Remove user from group\nRemove user from a group\rsudo gpasswd -d USER_NAME GROUP_NAME\rView Groups View all groups on the system with sudo nano /etc/groups.\nView all groups\rsudo nano /etc/groups\rView the current user\u0026rsquo;s groups with groups.\nView current user\u0026#39;s groups\rgroups\rView a given user\u0026rsquo;s groups with groups USER_NAME.\nView a given user\u0026#39;s groups\rgroups USER_NAME\rView a given user\u0026rsquo;s Id and group Ids with id USER_NAME.\nView a given user\u0026#39;s Id and group Ids\rid USER_NAME\rView all users and their group associations with getent group.\nView what users are members of which groups\rgetent group\rCommon Groups Group Allows users to\u0026hellip; sudo Use the sudo command to elevate prilieges. wheel An older way to endow sudo privlieges.. cdrom Mount the optical drive adm Monitor linux system logs lpadmin Configure printers plugdev Access external storage devices Bash Config Files Bash config files set common variables or functions that are run or set everytime a new login shell is invoked (a new terminal tab is opened). This can set frequently used variables or aliases, format the appearance of a shell, and more.\nWhen invoking a terminal shell, there are two types of shells:\nLogin/Interactive shell: This is when someone logs into the terminal and is typing commands into a terminal shell. If a command requires additional user prompts, the user can type them in. Non-interactive shell: This is when a bash shell runs in the background or in a CI/CD pipeline. If a command requires additional prompts, Bash config files are typically located in the user\u0026rsquo;s home directory (~).\nLogin shell order Bash checks for configuration files in the following order:\nConfig file order\r# config file order, starting at top: ~/.bash_profile ~/.bash_login ~/.profile\rBash uses the first, and only the first, configuration file it finds. Subdequent file are ignored.\n.bash_profile File Name When Sourced Typical Content .bash_profile Once during an interactive login shell Environment variables for login shell, startup scripts .bash_login Fallback if .bash_profile is missing Same as .bash_profile .profile All interactive shells (login \u0026amp; sub-shells) Non-login specific environment variables, aliases, functions Keep .bash_profile lean System Wide Login Config While bash config files are typically located in a user\u0026rsquo;s home directory, there are configs that are loaded for all users.\nThe system-wide bash config file is stored at /etc/profile.\nThe system-wide config for Ubuntu is located at/etc/profile. This can be used to give a login message for all users.\n","date":"2024-06-24","id":34,"permalink":"/docs/linux/linux-configuration/","summary":"Networking Set Static IP A static IP is an IP address that doesn’t change. This allows a machine to set its own IP address, rather than be assigned a random one by the router.","tags":[],"title":"Linux Configuration"},{"content":"Install deps with pip # install deps pip install -r requirements.txt # updated deps pip install --upgrade -r requirements.txt\rUsing venv virtual environment # create virtual env python -m venv \u0026lt;environment-name\u0026gt; python -m venv venv # activate environment source \u0026lt;environment-name\u0026gt;/bin/activate source venv/bin/activate # install deps pip install -r requirements.txt # deactive environment deactivate # remove environment rm -rf venv\r","date":"2024-06-22","id":35,"permalink":"/docs/python/deps/","summary":"Install deps with pip # install deps pip install -r requirements.txt # updated deps pip install --upgrade -r requirements.txt\rUsing venv virtual environment # create virtual env python -m venv \u0026lt;environment-name\u0026gt; python -m venv venv # activate environment source \u0026lt;environment-name\u0026gt;/bin/activate source venv/bin/activate # install deps pip install -r requirements.","tags":[],"title":"Deps"},{"content":"\r==\u0026gt; mosquitto mosquitto has been installed with a default configuration file. You can make changes to the configuration by editing: /opt/homebrew/etc/mosquitto/mosquitto.conf To start mosquitto now and restart at login: brew services start mosquitto Or, if you don\u0026#39;t want/need a background service you can just run: /opt/homebrew/opt/mosquitto/sbin/mosquitto -c /opt/homebrew/etc/mosquitto/mosquitto.conf\r","date":"2024-06-22","id":36,"permalink":"/docs/portal/mosquitto/","summary":"==\u0026gt; mosquitto mosquitto has been installed with a default configuration file. You can make changes to the configuration by editing: /opt/homebrew/etc/mosquitto/mosquitto.","tags":[],"title":"Mosquitto"},{"content":"","date":"2024-06-22","id":37,"permalink":"/docs/portal/","summary":"","tags":[],"title":"Portal"},{"content":"Arrrrr\n","date":"2024-06-21","id":38,"permalink":"/docs/unicode/","summary":"Arrrrr","tags":[],"title":"Unicode"},{"content":"Arrr\n← → ↔ ↑ ↓ ↕ ↖ ↗ ↘ ↙ ⤡ ⤢ ↚ ↛ ↮ , ⇷ ⇸ ⇹ ⤉ ⤈ , ⇺ ⇻ ⇼ ⇞ ⇟\nq 0 1 2 3 4 5 6 7 8 9 A B C D E F U+219x ← ↑ → ↓ ↔ ↕ ↖ ↗ ↘ ↙ ↚ ↛ ↜ ↝ ↞ ↟ U+21Ax ↠ ↡ ↢ ↣ ↤ ↥ ↦ ↧ ↨ ↩ ↪ ↫ ↬ ↭ ↮ ↯ U+21Bx ↰ ↱ ↲ ↳ ↴ ↵ ↶ ↷ ↸ ↹ ↺ ↻ ↼ ↽ ↾ ↿ U+21Cx ⇀ ⇁ ⇂ ⇃ ⇄ ⇅ ⇆ ⇇ ⇈ ⇉ ⇊ ⇋ ⇌ ⇍ ⇎ ⇏ U+21Dx ⇐ ⇑ ⇒ ⇓ ⇔ ⇕ ⇖ ⇗ ⇘ ⇙ ⇚ ⇛ ⇜ ⇝ ⇞ ⇟ U+21Ex ⇠ ⇡ ⇢ ⇣ ⇤ ⇥ ⇦ ⇧ ⇨ ⇩ ⇪ ⇫ ⇬ ⇭ ⇮ ⇯ U+21Fx ⇰ ⇱ ⇲ ⇳ ⇴ ⇵ ⇶ ⇷ ⇸ ⇹ ⇺ ⇻ ⇼ ⇽ ⇾ ⇿ Supplemental Arrows A U+27Fx ⟰ ⟱ ⟲ ⟳ ⟴ ⟵ ⟶ ⟷ ⟸ ⟹ ⟺ ⟻ ⟼ ⟽ ⟾ ⟿ Supplemental Arrows B U+290x ⤀ ⤁ ⤂ ⤃ ⤄ ⤅ ⤆ ⤇ ⤈ ⤉ ⤊ ⤋ ⤌ ⤍ ⤎ ⤏ U+291x ⤐ ⤑ ⤒ ⤓ ⤔ ⤕ ⤖ ⤗ ⤘ ⤙ ⤚ ⤛ ⤜ ⤝ ⤞ ⤟ U+292x ⤠ ⤡ ⤢ ⤣ ⤤ ⤥ ⤦ ⤧ ⤨ ⤩ ⤪ ⤫ ⤬ ⤭ ⤮ ⤯ U+293x ⤰ ⤱ ⤲ ⤳ ⤴ ⤵ ⤶ ⤷ ⤸ ⤹ ⤺ ⤻ ⤼ ⤽ ⤾ ⤿ U+294x ⥀ ⥁ ⥂ ⥃ ⥄ ⥅ ⥆ ⥇ ⥈ ⥉ ⥊ ⥋ ⥌ ⥍ ⥎ ⥏ U+295x ⥐ ⥑ ⥒ ⥓ ⥔ ⥕ ⥖ ⥗ ⥘ ⥙ ⥚ ⥛ ⥜ ⥝ ⥞ ⥟ U+296x ⥠ ⥡ ⥢ ⥣ ⥤ ⥥ ⥦ ⥧ ⥨ ⥩ ⥪ ⥫ ⥬ ⥭ ⥮ ⥯ U+297x ⥰ ⥱ ⥲ ⥳ ⥴ ⥵ ⥶ ⥷ ⥸ ⥹ ⥺ ⥻ ⥼ ⥽ ⥾ ⥿ U+292x U+292x 27F0 27F1 27F2 27F3 27F4 27F5 27F6 27F7 27F8 27F9 27FA 27FB 27FC 27FD 27FE 27F\n","date":"2024-06-21","id":39,"permalink":"/docs/unicode/arrows/","summary":"Arrr\n← → ↔ ↑ ↓ ↕ ↖ ↗ ↘ ↙ ⤡ ⤢ ↚ ↛ ↮ , ⇷ ⇸ ⇹ ⤉ ⤈ , ⇺ ⇻ ⇼ ⇞ ⇟","tags":[],"title":"Arrows"},{"content":"Snippets of various python code.\nTime Things of time.\nConvert datetime to string Convert datetime to string - strftime() from datetime import datetime now = datetime.now() now.strftime(\u0026#34;%Y\u0026#34;)\rConvert string to datetime (strptime()) from datetime import datetime date_str = \u0026#34;\u0026#34; datetime.strptime(date_str, \u0026#34;%d\u0026#34;)\rRandom Handle subprocess.run import subprocess import logging def call_subprocess(cmd, capture_output=True): \u0026#34;\u0026#34;\u0026#34; Runs a subprocess command and captures errors with logging. Args: cmd: A list containing the command and its arguments. capture_output: Whether to capture standard output and error (default: True). Returns: A CompletedProcess object on success, None on failure. \u0026#34;\u0026#34;\u0026#34; try: # Call subprocess.run with desired arguments result = subprocess.run(cmd, capture_output=capture_output) return result except subprocess.CalledProcessError as error: # Log error with specific message for CalledProcessError logging.error(f\u0026#34;Subprocess failed with exit code: {error.returncode}\u0026#34;) logging.error(f\u0026#34;Command: {\u0026#39; \u0026#39;.join(cmd)}\u0026#34;) if capture_output: logging.error(f\u0026#34;Standard Error: {error.stderr.decode(\u0026#39;utf-8\u0026#39;)}\u0026#34;) except (FileNotFoundError, PermissionError) as error: # Log more specific errors for common exceptions logging.error(f\u0026#34;Error: {type(error).__name__} - {error}\u0026#34;) logging.error(f\u0026#34;Command: {\u0026#39; \u0026#39;.join(cmd)}\u0026#34;) # No return here if there\u0026#39;s an exception # Example usage command = [\u0026#34;ls\u0026#34;, \u0026#34;/nonexistent/directory\u0026#34;] try: result = call_subprocess(command) if result: print(f\u0026#34;Subprocess output: {result.stdout.decode(\u0026#39;utf-8\u0026#39;)}\u0026#34;) except Exception as e: # Catch any other unexpected exceptions logging.error(f\u0026#34;Unexpected error: {e}\u0026#34;)\rOpen File Modes r - read only. I/O exception raised if file doesn’t exist. Default mode. Handle location is start of file. r+ - read and write. I/O exception raised if file doesn’t exist. Default mode. Handle location is start of file. w - write only. Existing files overwritten. Creates new file if doesn’t exist. Fails if writing to new directory that doesn’t exist. Handle location is start of file. w+ - ready and write. Overwrites existing text. a - append only. Writes new text at end of file. Creates new file if doesn’t exist. Fails if writing to new directory that doesn’t exist. Handle location is end of file. a+ - read and write. Read and append behavior. try: with open(filename, \u0026#34;a\u0026#34;) as file_obj: file_obj.write(string_to_append) print(f\u0026#34;Appended {string_to_append}\u0026#34;) except FileNotFoundError: print(\u0026#34;Error\u0026#34;)\rCreate a directory When using os.makedirs() if not using the aboslute path, e.g., calling the python script from a different path than the code is in, it will fail. Install Deps with Pip # install deps pip install -r requirements.txt # updated deps pip install --upgrade -r requirements.txt\rStart Virtual Environment with venv # create virtual env python -m venv \u0026lt;environment-name\u0026gt; python -m venv venv # activate environment source \u0026lt;environment-name\u0026gt;/bin/activate source venv/bin/activate # install deps pip install -r requirements.txt # deactive environment deactivate # remove environment rm -rf venv\rDate / Time from datetime import datetime now = datetime.now() year = now.strftime(\u0026#34;%Y\u0026#34;) # =\u0026gt; 2018 week = now.strftime(\u0026#34;%a\u0026#34;) # =\u0026gt; Sun week = now.strftime(\u0026#34;%A\u0026#34;) # =\u0026gt; Sunday week = now.strftime(\u0026#34;%w\u0026#34;) # =\u0026gt; 0 day = now.strftime(\u0026#34;%d\u0026#34;) # =\u0026gt; 01 day = now.strftime(\u0026#34;%-d\u0026#34;) # =\u0026gt; 1 month = now.strftime(\u0026#34;%b\u0026#34;) # =\u0026gt; Jan month = now.strftime(\u0026#34;%B\u0026#34;) # =\u0026gt; January month = now.strftime(\u0026#34;%m\u0026#34;) # =\u0026gt; 01 month = now.strftime(\u0026#34;%-m\u0026#34;) # =\u0026gt; 1 hour = now.strftime(\u0026#34;%H\u0026#34;) # =\u0026gt; 00..23 hour = now.strftime(\u0026#34;%-H\u0026#34;) # =\u0026gt; 0..23 hour = now.strftime(\u0026#34;%I\u0026#34;) # =\u0026gt; 01...12 hour = now.strftime(\u0026#34;%=I\u0026#34;) # =\u0026gt; 1..12 meridian = now.strftime(\u0026#34;%H\u0026#34;) # =\u0026gt; AM..PM minute = now.strftime(\u0026#34;%M\u0026#34;) # =\u0026gt; 00..59 minute = now.strftime(\u0026#34;%-M\u0026#34;) # =\u0026gt; 0..59 second = now.strftime(\u0026#34;%S\u0026#34;) # =\u0026gt; 00..59 second = now.strftime(\u0026#34;%-S\u0026#34;) # =\u0026gt; 0..59 microsecond = now.strftime(\u0026#34;%f\u0026#34;) # =\u0026gt; 00000..99999 utc_offset = now.strftime(\u0026#34;%z\u0026#34;) # =\u0026gt; 00..59 time_zone_name = now.strftime(\u0026#34;%Z\u0026#34;) # =\u0026gt; 0..59ode local_timestamp = now.strftime(\u0026#34;%c\u0026#34;) # =\u0026gt; Mon Jun 3 17:40:16 2024\rConvert Times from datetime import datetime # convert datetime to string strftime(\u0026#34;%Y\u0026#34;) # convert string to datetime datetime.strptime(date_string, \u0026#34;%d\u0026#34;)\rTerminal Subprocess Keys: Terminal, Bash, run shell command, run command\nimport subprocess\rClasses Good Guide: https://realpython.com/python-classes/#getting-started-with-python-classes\nClass with state manager class State: def __init__(self): self.current_menu = \u0026#34;main\u0026#34; self.user_data = {} # Store any relevant user data def update_state(self, selection): # Update state based on user selection # ... def get_next_action(self): # Determine next action based on current state # ... class UI: def display_menu(self, menu_name): # Display menu options from state # ... def get_user_input(self): # Get user input and validate it # ... class Logic: def perform_action1(self, data): # Business logic for action 1 # ... def perform_action2(self, data): # Business logic for action 2 # ... class App: def __init__(self): self.state = State() self.ui = UI() self.logic = Logic() def run(self): while True: self.ui.display_menu(self.state.current_menu) user_input = self.ui.get_user_input() self.state.update_state(user_input) next_action = self.state.get_next_action() if next_action == \u0026#34;action1\u0026#34;: self.logic.perform_action1(self.state.user_data) elif next_action == \u0026#34;action2\u0026#34;: self.logic.perform_action2(self.state.user_data) # ... handle other actions ... elif next_action == \u0026#34;exit\u0026#34;: break if __name__ == \u0026#34;__main__\u0026#34;: app = App() app.run()\rGet user input, translate input value class UI: def display_menu(self, menu_name): # ... def get_user_input(self): # Get user input user_choice = input(\u0026#34;Enter your choice: \u0026#34;) # Simple translation based on a dictionary menu_options = { \u0026#34;1\u0026#34;: \u0026#34;payments\u0026#34;, \u0026#34;2\u0026#34;: \u0026#34;settings\u0026#34;, } return menu_options.get(user_choice, user_choice) # Handle invalid choices\rFind in array my_array = [1, 2, 3, 4, 5] value_to_find = 3 if value_to_find in my_array: print(\u0026#34;Value exists in the array\u0026#34;) else: print(\u0026#34;Value not found\u0026#34;)\rCheck if env variable exists, otherwise get input source_account = os.environ.get(\u0026#39;SOME_VAR\u0026#39;) if not KeyError else input(\u0026#34;Enter input:\u0026#34;)\rKnow, working\ndef check_var(var, prompt): res = os.environ.get(var) if res is None: print(\u0026#34;ERROR: Environment variable {var} not supplied\u0026#34;) res = input(prompt) print(f\u0026#34;INFO: Got {var} from environment variable\u0026#34;) return res check_var(\u0026#34;SOME_VAR\u0026#34;, \u0026#34;Enter some variable:\u0026#34;)\rCheck if multiple variables are all populated variable1 = 10 variable2 = \u0026#34;hello\u0026#34; variable3 = None # Set one variable to None if all(variable1, variable2, variable3): print(\u0026#34;All variables are populated\u0026#34;) else: print(\u0026#34;At least one variable is None\u0026#34;)\rString Interpolation some_string = \u0026#34;Hello there\u0026#34; print(f\u0026#34;The string is: {some_string}\u0026#34;\rFor Loop and For Loop with index/iteration for x in options: print(x) for index, x in enumerate(options): print(index, x)\rStarts with Find item that starts with the same text, fuzzy matching:\nmy_list = [\u0026#34;--source\u0026#34;, \u0026#34;--target\u0026#34;, \u0026#34;--config=default\u0026#34;] my_string = \u0026#34;--source=123\u0026#34; # Use enumerate to get both the item and its index for index, item in enumerate(my_list): # Check if the string starts with the list item if my_string.startswith(item): print(f\u0026#34;Partial match for \u0026#39;{my_string}\u0026#39; found at index {index}\u0026#34;) break # Exit the loop after finding the first match # If the loop completes without a break, no match is found else: print(f\u0026#34;String \u0026#39;{my_string}\u0026#39; not found in the list\u0026#34;)\rDynamically update class variables def update_config(self, config_name, value): print(config_name) if hasattr(self, config_name): setattr(self, config_name, value) else: print(f\u0026#34;Error: Attribute \u0026#39;{config_name}\u0026#39; does not exist.\u0026#34;)\rRand Snip msg = f\u0026#34;Enter source account id [{existing_configs[\u0026#39;source_account\u0026#39;]}]: \u0026#34;\rArgument Names def getPerson(name, age, state): print(name, age, state) getPerson(\u0026#34;Greg\u0026#34;, 90, \u0026#34;VA\u0026#34;) getPerson(\u0026#34;Greg\u0026#34;, state=\u0026#34;VA\u0026#34;, age=90)\rError Handling ValueError: Raised when a correct argument type but an incorrect value is supplied to a function. For example, if you try to pass a string to a function that expects an integer, a ValueError will be raised. TypeError: Raised when an argument is of the wrong type. For example, if you try to pass a string to a function that expects an integer, a TypeError will be raised. IndexError: Raised when an index is out of range. For example, if you try to access the 10th element of a list that only has 5 elements, an IndexError will be raised. KeyError: Raised when a key is not found in a dictionary. For example, if you try to access the key \u0026ldquo;foo\u0026rdquo; in a dictionary that does not have a key \u0026ldquo;foo\u0026rdquo;, a KeyError will be raised. NameError: Raised when a name is not defined. For example, if you try to use the variable \u0026ldquo;foo\u0026rdquo; but the variable \u0026ldquo;foo\u0026rdquo; has not been defined, a NameError will be raised. Check to see if attribute exists: def configureWireless(self): # check setPipes was set if not hasattr(self, \u0026#34;transmit\u0026#34;): print(\u0026#34;Error message for you to see.\u0026#34;) print(\u0026#34;ERROR: Call setPipes() to define transmit address\u0026#34;) raise ValueError(\u0026#34;Transmit not set. See more information above.\u0026#34;)\rSet Variables in try catch A variable can be set in a try/except assuming the variable is also defined in the except or an error is raised.\ntry: spi = SPI(0, sck=Pin(2), mosi=Pin(7), miso=Pin(4)) cfg = {\u0026#34;spi\u0026#34;: spi, \u0026#34;csn\u0026#34;: 3, \u0026#34;ce\u0026#34;: 0} except ValueError as e: print(\u0026#34;Upstream error: \u0026#34;, e) raise ValueError(\u0026#34;Last error rendered to user\u0026#34;) # spi can be defined in \u0026#34;try\u0026#34; # spi can be defined in \u0026#34;except\u0026#34; or raise Error print(spi)\rKargs with methods METHOD”\ndef configureWireless(self, **kwargs): transmit = kwargs.get(\u0026#34;transmit\u0026#34;) receive = kwargs.get(\u0026#34;receive\u0026#34;) if not transmit or not receive: print(\u0026#34;ERROR: Transmit or recieve not set\u0026#34;) raise ValueError(\u0026#34;Transmit or recieve not set\u0026#34;)\rCalling Method\nmy_object = MyObject() my_object.configureWireless(transmit=\u0026#34;192.168.1.1\u0026#34;, receive=\u0026#34;192.168.1.2\u0026#34;)\rConverting Hex # convert hex to bytes print(\u0026#34;TO BYTES\u0026#34;, bytes.fromhex(\u0026#34;d2f0f0f0f0\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; TO BYTES b\u0026#39;\\xd2\\xf0\\xf0\\xf0\\xf0\u0026#39; # Convert hex to decimal print(\u0026#34;TO DECIMAL\u0026#34;, int(\u0026#34;d2f0f0f0f0\u0026#34;, 16)) \u0026gt;\u0026gt;\u0026gt; TO DECIMAL 905985454320 print(\u0026#34;TO HEX\u0026#34;, bytes.hex(tx)) \u0026gt;\u0026gt;\u0026gt; TO HEX d2f0f0f0f0\rArrays JQ Convert Each Line to Array sudo systemctl status docker | head | jq --raw-input --slurp \u0026#39;split(\u0026#34;\\n\u0026#34;)\u0026#39;\rSplit Columns into JSON sudo docker ps | jq -nR \u0026#39;[inputs | split(\u0026#34; \u0026#34;) | { \u0026#34;containter_id\u0026#34;: .[0] } ]\u0026#39;\rRandomly select element in array import random random.choice([1, 2, 3]) # Get random int between two given ints random_int = random.randint(0, len(tickets)) random_int = random.randint(0, 9)\rLoop though a range - repeat a given number of times for x in range(5): print(x)\r","date":"2024-06-03","id":40,"permalink":"/docs/python/python-snippets/","summary":"Snippets of various python code.\nTime Things of time.\nConvert datetime to string Convert datetime to string - strftime() from datetime import datetime now = datetime.","tags":[],"title":"Python Snippets"},{"content":"","date":"2024-06-03","id":41,"permalink":"/docs/python/","summary":"","tags":[],"title":"Python"},{"content":"Link to valuable, relevant resources.\n","date":"2024-02-27","id":42,"permalink":"/docs/resources/","summary":"Link to valuable, relevant resources.","tags":[],"title":"Resources"},{"content":"","date":"2023-09-07","id":43,"permalink":"/docs/","summary":"","tags":[],"title":"Docs"},{"content":"Variables # set a variable VARIABLE_NAME=some_text # reference a variable echo $VARIABLE_NAME # set variable equal to a command output VARIABLE_NAME=$(echo \u0026#34;Hello\u0026#34;) # set default variable VARIABLE_NAME=\u0026#34;${1:-HELLO}\u0026#34;\rBash Builtin Variables Builtin bash variables\r$0 # Name of the Bash script $1 - $9 # First 9 arguments to the script $# # Number of arguments were passed to script $@ # All the arguments supplied to the Bash script. $? # The exit status of the most recently run process. $$ # The process ID of the current script. $_ # Last argument of the previous command $USER # The username of the user running the script. $HOSTNAME # The hostname of the machine the script is running on. $SECONDS # The number of seconds since the script was started. $RANDOM # Returns a different random number each time is it referred to. $LINENO # Returns the current line number in the Bash script.\rCheck if Variable Exists Bash scripts that expect environment variables can be enriched by checking to see if the variables exist before running the remainder of the script. Help text can be provided and an exit code can be thrown if the conditions are not met.\n#check if variable exists if [ -n \u0026#34;$1\u0026#34; ]; then echo \u0026#34;You supplied the first parameter!\u0026#34; else echo \u0026#34;First parameter not supplied. Exiting.\u0026#34; exit 1 fi # variable does not exist if [ -z \u0026#34;$1\u0026#34; ]; then echo \u0026#34;Does not exists\u0026#34; fi # check if exists, otherwise set if [ -z ${var+x} ]; then echo \u0026#34;var is unset\u0026#34;; else echo \u0026#34;var is set to \u0026#39;$var\u0026#39;\u0026#34;; fi\rDefault Variables Check if a variable is unset or null, then expand to default string value.\n$ foo=\u0026#34;bar\u0026#34; $ echo \u0026#34;${foo:-defaultValue}\u0026#34; ↪ bar $ echo \u0026#34;${fizz:-defaultValue}\u0026#34; ↪ defaultValue\rSet a variable to default if a given variable is unset or null.\nSet a default variable if the first argument is not supplied\r# set default variable if first argument not supplied VARIABLE_NAME=\u0026#34;${1:-DEFAULT VALUE}\u0026#34; # example usage MESSAGE=\u0026#34;${1:-Hello localhost}\u0026#34;\rReading User Input Read is used to get input from a user which can be set to a variable.\nGet user input and set to variable\r# set a variable read VARIABLE_NAME # set a variable, with prompt statement read -p \u0026#39;Enter name: \u0026#39; VARIABLE_NAME # set a variable, with prompt statement, password read -sp \u0026#39;Enter password: \u0026#39; VARIABLE_NAME\rExporting Varibles Use export to make a variable available to a subprocess.\nUse export to set a variable\rexport VAR_NAME=VALUE\rIf Statements if [[ $1 -gt 100]] then echo \u0026#34;Some text\u0026#34; fi\rUse semicolons (;) to condense if statements down to one line.\n# check if exists, otherwise set if [ -z ${var+x} ]; then echo \u0026#34;var is unset\u0026#34;; else echo \u0026#34;var is set to \u0026#39;$var\u0026#39;\u0026#34;; fi\rExtend logic with if, elif, and else.\nExtend logic with if, elif, and else.\nif [ $1 -gt 100] then echo \u0026#34;Some text\u0026#34; elif [ $2 == \u0026#39;no\u0026#39;] then echo \u0026#34;Alternate matching condition\u0026#34; else echo \u0026#34;Final catch condition\u0026#34; fi\rWildcards Use the asterisk wildcard (*) for partial matches of one or more occurences of any charachter. Can match beginning, end, or both.\nif [[ $1 == *\u0026#34;$SUBSTRING\u0026#34;* ]] then echo \u0026#34;Matches substring\u0026#34; fi\rUse the question mark (?) to match a single occurence of any chatachter.\nif [[ $1 == ??\u0026#34;log.txt\u0026#34; ]] then echo \u0026#34;Matches substring\u0026#34; fi\rBoolean Operators \u0026amp;\u0026amp; # and || # or\rif [[ $1 -gt 100 \u0026amp;\u0026amp; $2 == \u0026#39;yes\u0026#39;]] then echo \u0026#34;And condition met\u0026#34; elif [[ $1 -gt 50 || $2 == \u0026#39;no\u0026#39;]] then echo \u0026#34;Or condition met\u0026#34; fi\rEquality Operators = == # string comparisons -eq # equals, numeric comparisons -gt # greater than -ge # greater than or equal to -lt # less than -le # less than or equal to -ne # not equal\rBracket Syntax Bash has two types of syntax when dealing with if statements: [single] and [[double]]. Single is an older supported syntax and double allow for more features.\nSingle Bracket Notation File based conditions if [-L symlink]; then String-based conditions if [\u0026quot;$some_var\u0026quot; == \u0026quot;foo\u0026quot;]; then Numeric-based conditions if [\u0026quot;$some_var\u0026quot; -gt 100]; then Double Bracket Notation Equality operators: introduces and (\u0026amp;\u0026amp;), or (||) if [[ \u0026quot;$some_var\u0026quot; == \u0026quot;test\u0026quot; \u0026amp;\u0026amp; $1 -gt 2 ]]; then Regex pattern matching: introduces and (\u0026amp;\u0026amp;), or (||) if [[ \u0026quot;$some_var\u0026quot; == \u0026quot;test\u0026quot; \u0026amp;\u0026amp; $1 -gt 2 ]]; then Shell globbing: asterisk will expand to anything if [[ \u0026quot;$some_var\u0026quot; == *[Ff]oo ]]; then Word Splitting is prevented: so quotes can be omitted around variables if [[ $some_var == *[Ff]oo ]]; then Filenames are not expanded: With the older single bracked notation if [ -a *.sh ]; then. This will return true if there\u0026rsquo;s a single matching shell file. Return false if there are no files. Return an error if With double bracket notation, it will return true if there are one or more shell files if [[ $some_var == *[Ff]oo ]]; then Redirection # redirect stdout to file command \u0026gt;file # same as above command 1\u0026gt;file # redirect stdout to file (append to end of file) command \u0026gt;\u0026gt;file # redirect stderr to file command 2\u0026gt;file # redirect stdout \u0026amp; stderr to file command \u0026amp;\u0026gt;file # same as above # redirect stdout out to file, redirect stderr to stdout, which outputs to file command \u0026gt;file 2\u0026gt;\u0026amp;1 # discard stdout of command command \u0026gt; /dev/null # discard stdout \u0026amp; stderr of command command \u0026amp;\u0026gt; /dev/null # redirect contents of file to stdin of command command \u0026lt;file\rRedirect Multiple Lines command \u0026amp;lt;\u0026amp;lt;EOL multi line text goes here EOL\rFunctions Defining and Calling Functions Define and call a function\r# define a function that takes an argument function_name() { echo \u0026#34;Hello $1\u0026#34; } # call a function and pass an argument function_name first_argument\rReturning Values Set variable equal to return value of function\rfunction_name() { local some_var=\u0026#39;some value\u0026#39; echo \u0026#34;$(some_var)\u0026#34; } result=$(function_name)\rAlternate: set variable equal to return value of function\rfunction_name() { return 42 } function_name new_var=$?\rHandling Errors Handling errors in function results\rfunction_name() { return 1 } if function_name; then echo \u0026#34;Great success\u0026#34; else echo \u0026#34;Failure\u0026#34; fi\rLoops Loop there it is.\nfor i in TEST; do echo \u0026#34;$i\u0026#34; done\rRanges While true\u0026hellip;\nfor i in {1..5}; do ... done\rWith step size\nfor i in {1..100..2}; do echo \u0026#34;Hello $i\u0026#34; done\rForever While true\u0026hellip;\nwhile true; do ... done\rReading Lines while read -r line; do echo $line done \u0026lt;file_name\rIterate Loop for ((i = 0 ; i \u0026lt; 100 ; i++)) do echo $i done\rMisc echo \u0026#34;You\u0026#39;re in $(pwd)\u0026#34;\rEcho output and write to file\necho_and_write() { echo $* | tee -a file_name } echo_and_write \u0026#34;Some string to echo/write\u0026#34;\r","date":"2023-06-26","id":44,"permalink":"/docs/bash/bash-scripting/","summary":"Variables # set a variable VARIABLE_NAME=some_text # reference a variable echo $VARIABLE_NAME # set variable equal to a command output VARIABLE_NAME=$(echo \u0026#34;Hello\u0026#34;) # set default variable VARIABLE_NAME=\u0026#34;${1:-HELLO}\u0026#34;\rBash Builtin Variables Builtin bash variables\r$0 # Name of the Bash script $1 - $9 # First 9 arguments to the script $# # Number of arguments were passed to script $@ # All the arguments supplied to the Bash script.","tags":[],"title":"Bash Scripting"},{"content":"Domain Name System (DNS) is the phone book of the internet. It translates human friendly domain names, e.g., www.exmaple.com, to computer friendly addresses like 192.168.1.1.\nBasic Overview Domain names (like github.com) are made for humans. They\u0026rsquo;re nice and easy for us to use and remember. Websites, however, live at IP addresses (like 140.82.114.4), which are not easy to remember. DNS is the system that tells the browser to fetch 140.82.114.4 when the user requests github.com.\nDNS is distributed DNS is hierarchical DNS Record Types DNS record types provide information about the domain.\nType Name Description A A Record Points domain name to IP address AAAA AAAA Record Points domain name to IPv6 address CNAME Canonical name Points domain name to another domain name NS Nameserver Specifies the authoritative DNS server for a given domain MX mail exchange Specifies where emails for a domain should be routed to SOA Start of Authority Stores admin information about a domain Basic overview of DNS for website resolution Browser requests DNS record from DNS resolver. DNS Resolver is typically your ISP. If the DNS Resolver comes up dry (does not have it in cache) it will reach out to the root server. Root server is the top level of the DNS heirarchy. 13 servers are strategically placed around the world. If the Resolver has the record cached, the process will skip to step 8 Root server does not contain the IP address, but instead directs the Resolver to the TLD server The resolver asks the TLD server fot the IP address. The TLD server stores the IP addresses for top level domains (like .com). The TLD server does not know the IP address, but will direct the Resolver to the Authoratative Name Server. The Resolver will ask the name server for the IP address. The name server should contain everything about the domain, including its IP address. The Name Server return the IP address for github.com to the Resolver, which caches it in memory. The resolver returns the IP address to the client. The browser can now make the request to the web page. The browser requests the webpage from github.com\u0026rsquo;s web server The web server returns the age to the client TTL (Time to Live) TTL is a parameter on a DNS record that tells a resolver how long to cache the record before requesting a new one.\nCaching a record is a double edged sword. When a DNS record is cached in a resolver, it does not need to go looking for the DNS record, increasing the speed for the end user. However, if the DNS record changes, the resolver may continue to send traffic to the old, cached address.\nDNS propogration is waiting for the cached DNS records to expire (where the resolver will request new DNS records with the updated answer).\nSplit Horizon DNS Information to follow.\n","date":"2023-02-13","id":45,"permalink":"/docs/networking/dns/","summary":"Domain Name System (DNS) is the phone book of the internet. It translates human friendly domain names, e.g., www.exmaple.com, to computer friendly addresses like 192.","tags":[],"title":"DNS"},{"content":"A port is an endpoint of communication; it\u0026rsquo;s the docking point where data flows through. Ports are a logical contrcut, but are commonly used in the TCP and UDP protocols.\nPorts are a way to multiplex. The IP address 192.168.1.1 can be extended by adding ports: 192.168.1.1:22, 192.168.1.1:80, ect.\nA port number is a 16-bit unsigned integer (includes 0 and positive numbers), this ranging from 0 to 65535. Ports can range from\nCommon port numbers Port numbers 0 to 1024 are reserved for privileged services and are considered well known ports. Registered Ports are 1024 to 49151 Dynamic ports (aka: private ports): 49152 to 65535\nPort Protocol Desscrption 1 TMUX Used as a multiplexer 20, 21 FTP 20 as data and 21 as control 22 SSH 23 Telnet 25 SMTP Simple Mail Transfer Protocol 29 MSG IGP 37 Time 42 Hostname 43 Whois 53 DNS Domain Name Service 80 HTTP 115 SFTP 123 NTP Network Time Protocol 143 IMAP 179 BGP Border Gateway Protocol 443 HTTPS 3306 MySQL* Default MySQL port 3389 RDP Remote Desktop Protocol 3456 Postgres* Default Postgres port ","date":"2023-02-13","id":46,"permalink":"/docs/networking/ports/","summary":"A port is an endpoint of communication; it\u0026rsquo;s the docking point where data flows through. Ports are a logical contrcut, but are commonly used in the TCP and UDP protocols.","tags":[],"title":"Ports"},{"content":"","date":"2023-02-13","id":47,"permalink":"/docs/networking/","summary":"","tags":[],"title":"Networking"},{"content":"kubectl is a command line interface tool that interacts with the kube-apiserver/control plane.\nAPI resources can generally use with get, describe, delete. See the full list of API resources with kubectl api-resources.\nCreating Resrouces Use the kubectl apply command to create or update a resource. The -f flag specifies a file or directory name.\n# Specify file or directory $ kubectl apply -f ./deploy-dev.yaml # create resrouce from file $ kubectl apply -f ./dev.yaml ./prod.yaml # from two files $ kubectl apply -f ./dir # from an entire directory # Apply YAML from stdin cat deploy.json | kubectl apply -f -\rViewing, Updating, and Deleting Resrouces Kuberentes API resoruces can be generally be used with get, describe, edit, and delete.\nSee full list of API Resources with kubectl api-resources.\nViewing Resources View resources with the kubectl get command.\n# get services in default namespace $ kubectl get pods # get services in dev namespace $ kubectl get pods -n dev # get pods all namespaces $ kubectl get pods -A # list pods with more details $ kubectl get pods -o wide # get deployment dev-dep $ kubectl get pods customer-service-ae892934-482da2f # get pod\u0026#39;s YAML $ kubectl get pod ui-pod -o yaml #list pod without cluster specific information $ kubectl get --export # get all k8s resoruces in given namespace kubectl get all -n NAMESPACE # get all k8s resources in all namespaces kubectl get all -A # Can get, describe, and delete resources $ k get pods -n tick $ k desribe pods/influx-influxdb-j3ilso93-ae82kdl2 -n tick $ k delete pods/influx-influxdb-j3ilso93-ae82kdl2 -n tick\rDescribing, Editing, and Deleting # delete deployment kubectl delete deployments applications-service -n applications-dev # shorthand kubectl describe po/payment-service-3829aec9-cea2831af -n dev kubectl describe svc/payment-service -n dev kubectl describe deploy/payment-service -n dev\rGet logs Logs can be grabbed from a pod or a service.\nOne advantage of service level logs is faster debugging: the service name is predictable while the pod name is randomly generated and there\u0026rsquo;s an extra step to get the pod name. The service level logs will show logs from all pods it handles, which may or may not be desirable.\n# get logs associated with a pod kubectl logs CONTINER-NAME -n NAMESPACE kubectl logs payment-service-39a03f2-ac624fe -n dev # get logs associated with other services kubectl logs RESOURCE/RESOURCE-NAME -n NAMESPACE kubectl logs svc/payment-service -n dev kubectl logs jobs/payment-service -n dev kubectl logs deploy/payment-service -n dev # tail/follow logs kubectl logs svc/payment-service -n dev --follow kubectl logs svc/payment-service -n dev -f # limit logs based on time kubectl logs svc/payment-service -n dev --since 10m\rExec into running container Exec into a running container to get shell access or run a command.\nkubectl exec po/pod-name -n namespace-name -- bash\rPort forwarding Use port forwarding to map a port on your local machine to a running kubernetes pod or service.\nThis is useful when an engineer is working on one service and needs to interact with another service for development or testing.\n$ k port-forward services/applications-service -n dev 8080:80\rNamespaces Namespaces are a way to segment and organize resources running in a cluster.\n# create a namespace kubectl create namespace finance-qa # delete a namespace kubectl delete namepsaces finance-qa # note extra \u0026#34;s\u0026#34; # use \u0026#34;ns\u0026#34; shorthand for namespaces kubectl delete ns finance-qa\rAdd a user to EKS Config Map Give an IAM user access to an EKS cluster by editing its Config Map.\nkubectl edit -n kube-system configmap/aws-auth\rAdd EKS cluster to your local kubeconfig Add an AWS EKS to a machine\u0026rsquo;s kubeconfig using the AWS CLI.\n# add EKS cluster to local kubeconfig aws eks --region us-east-1 update-kubeconfig --name cluster-name-in-eks-ce548264\rUnorganized commands # Current meximum pods per node kubectl get nodes -o yaml | grep pods # Current pods per node $ kubectl get pods --all-namespaces | grep Running | wc -l\r# Get cluster info $ kubectl cluster-info $ kubectl component-status # shorthand commands $ k get services $ k get svc $ k get pods $ k get po # exec into container in pod k exec --stdin --tty employer-lookup-service-78cb8c8c7d-hhltf -- sh # see pod limit per node $ kubectl get nodes -o yaml | grep pods # see total running pods $ kubectl get pods --all-namespaces | grep Running | wc -l # Check node CPU and memory usage kubectl get nodes --no-headers | awk \u0026#39;{print $1}\u0026#39; | xargs -I {} sh -c \u0026#39;echo {}; kubectl describe node {} | grep Allocated -A 5 | grep -ve Event -ve Allocated -ve percent -ve -- ; echo\u0026#39;\rkubectl short hand\n$ k get all k get pod/applications-service-6ffbc48d6d-bpchh $ k get service/applications-ui $ k get deployment.apps/token-service $ k get replicaset.apps/applications-service-558c6b579d\rGenerate admin token for dashboard #!/bin/bash EKS_ADMIN_TOKEN_KEY=$(kubectl -n kube-system get secret | grep eks-admin | awk \u0026#39;{print $1}\u0026#39;) kubectl -n kube-system describe secret ${EKS_ADMIN_TOKEN_KEY} | grep \u0026#39;token:\u0026#39;\r","date":"2023-02-13","id":48,"permalink":"/docs/kubernetes/kubectl-cli/","summary":"kubectl is a command line interface tool that interacts with the kube-apiserver/control plane.\nAPI resources can generally use with get, describe, delete.","tags":[],"title":"Kubectl CLI"},{"content":"","date":"2023-02-13","id":49,"permalink":"/docs/kubernetes/","summary":"","tags":[],"title":"Kubernetes"},{"content":"Kubernetes Troubleshooting NOTE: TL;DR\nPrerequisite Understanding It\u0026rsquo;s helpful to have some shared terminology. I encourage you to read the Kubernetes and Helm documentation first. If it looks complicated, that\u0026rsquo;s because it is.\nUnderstand what a deployment and service mean (in k8s lingo): Service: abstraction of k8s routing/networking: defines port mappings, external DNS name (Ambassador annotation). Specifies how traffic from outside the container will reach the pod. Services usually get created once on creation and are infrequently if ever updated. Deployment: defines the pod, Docker image tag (version), environment variables, secrets, ect. When deleting a pod, Kubernetes will immediately start a new one; when deleting a deployment, the record of the pod and its history are removed from k8s entirely. Understand how Kubernetes does blue/green deployments k8s will not direct traffic to a new pod until it\u0026rsquo;s alive and ready. If the new pod fails in some way, kubnernetes will continue to direct traffic to the good, old pod. No matter how many additional deployments occur, if they all fail in some way, Kubernetes will continue to direct traffic to the last good pod (no matter how old that is). All kubectl and helm commands require the -n namespace flag. This flag will be omitted in this guide for simplicity. Troubleshooting Steps NOTE: all k8s commands require the -n namespace flag. This flag will be omitted in this guide for simplicity.\n0. Check Pipeline Output NOTE: Successful pipelines don\u0026rsquo;t guarantee successful deployments\nIf the pipeline fails - there\u0026rsquo;s probably a syntax issue with the k8s configuration files. Your configuration failed spectacularly and Kubernetes rejected the operation. Read the pipeline output to help determine the issue - while noisy, the pipeline output will usually identify the issue.\nIf the pipeline succeeds - your k8s files are syntactically correct, but the values are incorrect.\nIf the pipeline hangs - and you\u0026rsquo;re using the helm-deplov2.sh script, the new container has failed to enter a ready state. Inspect the pod in Kubernetes to debug the issue.\nConnectivity failures? Have you tried turning it on an off again? If the pipeline output implies a connectivity or authentication issue, try rerunning the job.\nMisleading Successful Pipelines Depending on how your pipeline deploys, a successful GitLab pipeline doesn\u0026rsquo;t guarantee a successful deployment.\nWhen using kubectl apply, a successful pipeline indicated the command was sent to Kubernetes successfully, not that the release was successful. In this case, the syntax of the configuration was valid and not rejected outright by k8s, the values of the configuration files may need refining.\nUse the helm-deploy-v2.sh script NOTE: Link to Helm docs\nThis script uses kubectl rollout to verify the new pod is running and accepting traffic. This means confirmation of a successful deployment in the pipeline.\n# Gitlab Pipeline Output Helm completed. Querying cluster to verify release was successful... Waiting for deployment \u0026#34;cat-facts-ui\u0026#34; rollout to finish: 0 of 1 updated replicas are available... deployment \u0026#34;cat-facts-ui\u0026#34; successfully rolled out\rLook for the text successfully rolled out. When you see this output, Kubernetes has confirmed the new pod is running and accepting traffic.\n1. Get Pods Get list of all pods in a given namespace.\nk get pods -n dev\rChecking pod status helps you quickly triage an issue.\nwyatt.munson $ k get pods\rNAME READY STATUS RESTARTS AGE\radverse-action-loan-544cd9b7b4-sks9c 1/1 Running 0 33d\rapplication-edge-service-68c897b7c-2q69d 1/1 CrashLoopBackOff 45 33d\rapplications-service-6664d8bdb4-hpch8 1/1 Error Starting 0 32d\rRunning - indicates the container was started and the pod is running fine and is considered ready by Kubernetes. Diagnose: inspect the describe service for misconfiguration. Problems exist at the networking level, e.g., bad port mappings, bad Ambassador annotation (DNS name configuration). CrashLoopBackOff - K8s has detected the container is crashing on start. Application may be panicking, or failing a healthcheck. Diagnose: check the application logs for the pod. Problems exist at the application level, e.g., secret value was incorrect and the app couldn\u0026rsquo;t authenticate with the database. Kubernetes will repeatedly attempt to restart any dead container, but will wait before doing so at an exponentially slower rate when in this state. CreateContainerConfigError - Kubernetes hasn\u0026rsquo;t event attempted to start the container. Any problems exist outside of the application. Diagnose: use describe pod and check events. Problems exist at the kubernetes level, e.g., you referenced a non-existant k8s secret. 2. Describe Pod Get details about a given pod.\n# Get unique pod name using `kubectl get pods` k describe pod/application-edge-service-68c897b7c-2q69d # You can also describe the service k describe service/application-edge-service # Or, the deployment k describe deployment/application-edge-service\rTake note of the following fields in the output:\nNamespace - is it correct? Containers.ContainerID - is the version tag of the image correct (or present)? Containers.PortID - is the port correct (this is the port the application is listening on)? Containers.Environment - are expected environment variables present? ----- - is the healthcheck route correct? Events - check here for any k8s error output. Issues with pulling the image or finding secrets will display here. 3. Get Logs Applications logs can be pulled from the pod or the service.\nGet application logs.\nk logs po/\u0026lt;POD-NAME\u0026gt; k logs po/adverse-action-loan-544cd9b7b4-sks9c # get logs for last 10 minutes k logs po/adverse-action-loan-544cd9b7b4-sks9c --since 10m # tail logs k logs po/adverse-action-loan-544cd9b7b4-sks9c --follow\rThis will show the logs from a given container. Logs will persist until the pod is deleted.\nGet Service Logs Service logs usually persist over several versions of a pod, meaning you can access logs from previous pods through the service.\nk get logs service/adverse-action-loan\rWith each release, usually only the deployment is updated (e.g., new application image) and the service remains intact. The service is usually created on project creation and is infrequently updated.\nOften times, a missing environment variable or failed database connection is the culprit for a CrashLoopBackOff. Good application logging can greatly help troubleshooting efforts.\nRemember, if the container hasn\u0026rsquo;t started, there will be no application logs. In this case, the problem is.\n4. Check Secret Values NOTE: A secret must exist in AWS before it can be created in k8s.\n# list all secrets k get secrets # get a given secret\u0026#39;s values # secret values are represented in base64 k get secret SECRET_NAME -o yaml # decode a base64 encoded string echo \u0026#34;secret-text\u0026#34; | base64 -d\rDoes the secret exist in Kubenrtes? Does the secret exist in AWS. It must be created here first. Even if some values are in AWS and some are not, if any are missing the secret will not be created in k98s Are you referencing the secret key and value correctly? k describe pod/....\rEnvironment:\rSERVER_ENV: dev\rNEUSTAR_PASSWORD: \u0026lt;set to the key \u0026#39;NEUSTAR_PASSWORD\u0026#39; in secret \u0026#39;non-existant-secret\u0026#39;\u0026gt; Optional: false\r5. Get Events Get all events in a given namespace.\nk get events\rThis output will give you events across all pods in the namespace. Take note of the First occured field FIX THIS.\n6. Port Forwarding Use port forwarding to isolate networking problems.\nk port-forward pod/... curl localhost:4545/healthcheck k port-forward service/cat-facts-ui curl localhost:4545/healthcheck\rCannot hit pod - port mapping issue\nCannot hit service - service cannot hit pod.\n7. Ambassador Configuration NOTE: This step is only necessary if the service needs to be accessable externally from a domain name like (service.dev.westcreekfin.com).\nIf your pod is available via port forwarding, but cannot be accessed via its DNS name the issue lies with\nThis is a two step process:\nA Route 53 entry points a DNS entry to the Ambassador load balancer An ambassador annotation create a route table entry that routs a DNS name to a Kubernetes pod Debug DNS Entry dig apply.dev.westcreekfin.com\rThis should return the IP addresses of the AWS load balancers (10.2.x.x or 10.3.x.x addresses).\nCheck Ambassador Console # get ambassador pod name k get pod -n default # port forward to pod k port-forward pod/ambassador-68c8c8fd54-tbcmb 8877\rAccess http://localhost:8877/ambassador/v0/diag/ in your web browser.\nIf y\nRolling Back with Helm Roll backs. What\u0026rsquo;s in your wallet?\nRolling back vs. deploying a revert commit vs. redeploying The short answer is that rolling back guarantees you immediately revert to the last known working version; however, evidence of this is not captured in GitLab or git.\nRerunning the deployment stage for a previous pipeline is the fastest way to redeploy a older version using only the GitLab UI. Depending on the age of the pipeline, artifacts may have expired which will invalidate this option. History of re-deployment would be captured in GitLab, but not in git. Remember not to rerun the entire pipeline.\nReverting commits can be a slower option depending on the runtime of the pipeline. Repos may have merged, but undeployed code hoarded in master - it may be undesirable or confusing to remove it. This option, however, cements the operation in the GitLab and git histories, if such a record is desired.\nRolling back with Helm List Helm Release helm ls A Helm release is a deployed version of your application. Each subsequent version is a new release.\nhelm ls\rNAME NAMESPACE\tREVISION\tUPDATED STATUS CHART APP VERSION\rapplication-edge-service dev 2 2020-07-01 17:49:36.624466184 +0000 UTC\tfailed application-edge-service-1 0.5.0\rcat-facts-ui dev 1 2020-07-11 21:50:44.54027864 +0000 UTC deployed\tcat-facts-ui-bleeding-edge 1.1.22\rSee status of Helm Release helm status wyatt.munson@WCF-0047 $ helm status cat-facts-ui\rNAME: cat-facts-ui\rLAST DEPLOYED: Sat Jul 11 21:50:44 2020\rNAMESPACE: dev\rSTATUS: deployed\rREVISION: 1\rTEST SUITE: None\rSee full output with manifest that will include all the helm values. Use the command below to verify all values are set correctly.\n# see full output of helm release including all values $ helm status cat-facts-ui -o yaml\rSee history of Helm releases helm history NOTE: kubectl equivalent is kubectl rollout history\n$ helm history cat-facts-ui\rwyatt.munson@WCF-0047 $ helm history document-generator REVISION\tUPDATED STATUS CHART APP VERSION\tDESCRIPTION 29 Mon Sep 14 16:36:24 2020\tsuperseded\tdocument-generator-bleeding-edge\t0.29.30 Upgrade complete 30 Mon Sep 14 17:08:07 2020\tdeployed\tdocument-generator-bleeding-edge\t0.29.31 Upgrade complete 31 Mon Sep 14 18:40:24 2020\tfailed\tdocument-generator-bleeding-edge\t0.29.34 Upgrade complete 32 Mon Sep 14 19:20:36 2020\tfailed\tdocument-generator-bleeding-edge\t0.29.35 Upgrade complete\rUsing helm history you can see the status of all previous deployments. Note\nNOTE: It\u0026rsquo;s intended Kubernetes behavior that the last known good version will continue to be served.\nRollback with helm rollback $ helm rollback RELEASE [REVISION] # rollback to the previous release $ helm rollback cat-facts-ui # rollback to revision number 12 # use `helm history` to get revision numbers $ helm rollback cat-facts-ui 12\rRollback with kubectl WARNING: Do not use kubectl rollback commands if Helm was used for the deploy\nIf you did not use Helm, kubectl has built in commands for handling rollbacks.\nSee release history (kubectl rollout history) kubectl rollout history deployment/DEPLOYMENT_NAME kubectl rollout history deployment/cat-facts-ui\r# See deployment history for a given deployment $ k rollout history deploy/cat-facts-ui deployment.extensions/cat-facts-ui REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; # See details including image name and tag $ k rollout history deploy/cat-facts-ui --revision=1\rRollback (kubectl rollout undo) WARNING: Don\u0026rsquo;t use these commands if pod was deployed using Helm\nkubectl rollout undo deployment/DEPLOYMENT_NAME # Rollback to the previous deployment kubectl rollout undo deployment/cat-facts-ui # Rollback to specific deployment kubectl rollout undo deployment/cat-facts-ui --to-revision=2\rHealthy Output of Running Container wyatt.munson@WCF-0047 $ k describe po/cat-facts-ui-849f888dbc-lwwfh\rName: cat-facts-ui-849f888dbc-lwwfh\rNamespace: dev\rPriority: 0\rNode: ip-10-3-2-9.ec2.internal/10.3.2.9\rStart Time: Thu, 13 Aug 2020 16:00:59 -0400\rLabels: app=cat-facts-ui\rapp.kubernetes.io/instance=cat-facts-ui\rapp.kubernetes.io/name=cat-facts-ui\rpod-template-hash=849f888dbc\rrelease=cat-facts-ui\rAnnotations: kubernetes.io/psp: eks.privileged\rStatus: Running\rIP: 10.3.2.160\rIPs: \u0026lt;none\u0026gt;\rControlled By: ReplicaSet/cat-facts-ui-849f888dbc\rContainers:\rcat-facts-ui:\rContainer ID: docker://ca7160614b34032cdd729cfd1a2c46ed0f551c7e5d2a568e72663775f2da574f\rImage: docker.internal.westcreekfin.com/westcreek/example-projects/cat-facts-ui:1.1.22\rImage ID: docker-pullable://docker.internal.westcreekfin.com/westcreek/example-projects/cat-facts-ui@sha256:8fd506a8844991c1ecb6bc202f025be5a340f4e2d79a6e4d7ff0dead68c4ed8a\rPort: 80/TCP\rHost Port: 0/TCP\rState: Running\rStarted: Thu, 13 Aug 2020 16:01:57 -0400\rReady: True\rRestart Count: 0\rLiveness: http-get http://:5000/ delay=15s timeout=15s period=10s #success=1 #failure=3\rReadiness: http-get http://:5000/ delay=5s timeout=3s period=10s #success=1 #failure=3\rEnvironment:\rSERVER_ENV: dev\rMounts:\r/var/run/secrets/kubernetes.io/serviceaccount from default-token-gzq4z (ro)\rConditions:\rType Status\rInitialized True\rReady True\rContainersReady True\rPodScheduled True\rVolumes:\rdefault-token-gzq4z:\rType: Secret (a volume populated by a Secret)\rSecretName: default-token-gzq4z\rOptional: false\rQoS Class: BestEffort\rNode-Selectors: \u0026lt;none\u0026gt;\rTolerations: node.kubernetes.io/not-ready:NoExecute for 300s\rnode.kubernetes.io/unreachable:NoExecute for 300s\rEvents: \u0026lt;none\u0026gt;\rBad Container Deployed After Good Container Here a\ncat-facts-ui-6886467ffc-rcb84 0/1 ContainerCreating 0 16s\rcat-facts-ui-849f888dbc-lwwfh 1/1 Running 0 36d\rk get pod/cat-facts-ui-6886467ffc-rcb84\rNAME READY STATUS RESTARTS AGE\rcat-facts-ui-6886467ffc-rcb84 0/1 CreateContainerConfigError 0 29s\rEvent Ouput of bad container Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 81s default-scheduler Successfully assigned dev/cat-facts-ui-6886467ffc-rcb84 to ip-10-3-1-212.ec2.internal Normal Pulling 7s (x6 over 80s) kubelet, ip-10-3-1-212.ec2.internal Pulling image \u0026#34;docker.internal.westcreekfin.com/westcreek/example-projects/cat-facts-ui:1.1.23\u0026#34; Normal Pulled 7s (x6 over 61s) kubelet, ip-10-3-1-212.ec2.internal Successfully pulled image \u0026#34;docker.internal.westcreekfin.com/westcreek/example-projects/cat-facts-ui:1.1.23\u0026#34; Warning Failed 7s (x6 over 61s) kubelet, ip-10-3-1-212.ec2.internal Error: secret \u0026#34;non-existant-secret\u0026#34; not found\rTest TL;DR NOTE: Remember the -n namespace flag for every command\n# git list of pods in a given namespace k get pods # describe pod and service for detailed config k describe pod/cat-facts-ui-849f888dbc-lwwfh k describe service/cat-facts-ui # see application logs from pod or service k logs pod/cat-facts-ui-849f888dbc-lwwfh k logs service/cat-facts-ui # get namespace-wide events k get events -n dev # get secrets in a given namespace k get secrets # see base-64 encoded values for a given secret k get secret some-secret-name -o yaml # decode a base64 string echo \u0026#34;BASE_64_ENCODED_STRING\u0026#34; | base64 -d # port forward to pod, access on you machine as localhost:4545 k port-forward pod/cat-facts-ui-849f888dbc-lwwfh 4545:80 # port forward to serivce, access on you machine as localhost:4545 k port-forward service/cat-facts-ui 4545:80\r","date":"2023-02-13","id":50,"permalink":"/docs/kubernetes/kubernetes-troubleshooting/","summary":"Kubernetes Troubleshooting NOTE: TL;DR\nPrerequisite Understanding It\u0026rsquo;s helpful to have some shared terminology. I encourage you to read the Kubernetes and Helm documentation first.","tags":[],"title":"Kubernetes Troubleshooting"},{"content":"Postgres flavored SQL Postgres implements the SQL language as expected for the most part. Postgres layers on additional datatypes and functions.\nCreating tables Postgres datatype reference\nBasic Example CREATE TABLE cars ( car_id INTEGER PRIMARY KEY SERIAL, manufacturer INTEGER REFERENCES manufacturers(manufacturer_id) awd BOOLEAN, description VARCHAR(500), notes VARCHAR, create_date TIMESTAMP WITH TIMEZONE, update_date TIMESTAMP, );\rThe table above creates:\nA car_id primary key, using SERIAL to automatically generate and populate the primary key A manufacturer foreign key reference to the manufacturer_id column in the manufacturers table. It is stored as an INTEGER in the cars table, the same datatype as manufacturer_id. A awd boolean field that can store true, false, or null A description field with a fixed 500 charachter limit A notes field with no fixed charachter limit A create_date timestamp (time and date) including timezone information update_date - timestamp without timezone information Full Example -- Create a sequence to be used in a table (e.g., id field) CREATE SEQUENCE box_sequence start 2000 increment 1; CREATE TABLE boxes ( -- sequence reference uses \u0026#39;single quotes\u0026#39; \u0026#34;boxId\u0026#34; INTEGER NOT NULL PRIMARY KEY DEFAULT nextval(\u0026#39;box_sequence\u0026#39;), -- column names use \u0026#34;double quotes\u0026#34; \u0026#34;boxName\u0026#34; VARCHAR(32), -- lower case column names don\u0026#39;t need quotes notes VARCHAR(128) fixed_text CHAR(32) long_text TEXT, can_delete BOOLEAN, price FLOAT(2), metadata JSONB, id_code UUID, create_time TIME, create_date DATE, created_on TIMESTAMP, with_timezone TIMESTAMP WITH TIME ZONE ); CREATE TABLE items ( -- SERIAL is auto-incremented auto-populated integer \u0026#34;itemId\u0026#34; SERIAL PRIMARY KEY, -- foreign key reference to another table \u0026#34;box\u0026#34; INTEGER REFERENCES boxes (\u0026#34;boxId\u0026#34;), );\rSERIAL is an auto-incrementing, auto-populating type. It starts at one and is automatically add when the record is created. It\u0026rsquo;s handy for a primary key ID when you\u0026rsquo;re not concerned about the value. Drop table Delete a table, handle errors, handle object dependencies.\n-- drop (delete) a table DROP TABLE table_name; -- prevent SQL error if talbe doesn\u0026#39;t exist DROP TABLE IF EXISTS table_name; -- drop table and dependant objects in foreign table DROP TABLE table_name CASCADE;\rAlter table and delete rows Add columns, change column data types, delete columns.\nALTER TABLE items ADD \u0026#34;itemDescription\u0026#34; VARCHAR(64); ALTER TABLE items ALTER COLUMN \u0026#34;lastMoved\u0026#34; TYPE TIMESTAMP WITH TIME ZONE; ALTER TABLE DROP COLUMN description;\rInsert, Update, and Delete rows Insert new rows, update existing rows, and delete rows.\n-- INSERT (strings use \u0026#39;single quotes\u0026#39;) INSERT INTO users (\u0026#34;userId\u0026#34;, notes) VALUES (500, \u0026#39;Greg Benish\u0026#39;); -- UPDATE ROW(S) UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition; -- DELETE ROW(S) DELETE FROM table_name WHERE condition;\rLimit and Offset Limit number of returned rows, skip a number of rows before returning the retult set.\n-- Limiting the rows returned SELECT * FROM users ORDER BY create_date LIMIT 200; -- Skip a number of rows before returning limit SELECT * FROM users LIMIT 200 OFFSET 20;\rEnums Create predefined values for a column.\n-- Create an enum CREATE TYPE item_category AS ENUM (\u0026#39;Electronic\u0026#39;, \u0026#39;Camping\u0026#39;, \u0026#39;Other\u0026#39;); -- Update existing enum ALTER TYPE item_category ADD VALUE \u0026#39;None\u0026#39;; -- Use as column type ALTER TABLE items ADD \u0026#34;itemCategory\u0026#34; ITEM_CATEGORY; -- List enum values \\dT+ ENUM_NAME \\dT+ item_category\rArray Datatypes Column data types can be arrays\nALTER TABLE items ADD \u0026#34;itemList\u0026#34; TEXT[]; INSERT INTO items (\u0026#34;itemList\u0026#34;) VALUES (\u0026#39;{foo,bar}\u0026#39;);\rSchemas Schemas are effectively namespaces that help segment data within a Postgres database.\nCREATE SCHEMA some_schema; CREATE TABLE some_schema.some_table ( ... ); SELECT * FROM some_schema.some_table;\rPSQL Tid Bits Find rows with duplicate records in itinerary, e.g., pointing to a foreign key. Matching records are combined into a JSON array. Returns each row with a unique value for itinerary and a combined JSON array of matching values.\nSELECT s.\u0026#34;itinerary\u0026#34;, jsonb_agg(s) AS segments FROM segments s GROUP BY s.\u0026#34;itinerary\u0026#34;\rSelect distinct where set may be split between multiple columns.\nselect distinct a as ab from (select \u0026#34;originCity\u0026#34; a from itineraries union all select \u0026#34;destinationCity\u0026#34; from itineraries ) AS x;\ritinerary | segments\r__________|___\r1 | [{ itinerary: 1, stops: 3 }, { itinerary: 1, stops: 6 }]\r2 | [{ itinerary: 2, stops: 7 }]\r","date":"2023-01-05","id":51,"permalink":"/docs/postgres/psql-reference/","summary":"Postgres flavored SQL Postgres implements the SQL language as expected for the most part. Postgres layers on additional datatypes and functions.","tags":[],"title":"PSQL Reference"},{"content":"Positioning position: static; - default when not specified Not affected by top, bottom, left, or right properties Not positioned in any special way; positioned in line with normal page flow position: relative; - positioned relative to its normal position When you want to move an element; use top, bottom, left, or right absolute - gets positioned relative to closest positioned ancestor (not viewport fixed) Notes moves with page scrolling, overlaps other divs If no positioned ancestors, it uses the document body and moves with page scrolling Can use top, bottom, left, or right position: fixed; - gets positioned relative to the viewport Notes: moves with page scrolling Can use top, bottom, left, or right position: sticky; - gets positioned based on the user\u0026rsquo;s scroll position Uses a combination of relative and fixed Use when you want to have an element appear, scoll up to the top, then stay sticky as you continue scrolling down Test\np { border: 1px solid black; border-radius: 3px; border-style: solid; }\rTransitions Fade In Create a fadein animation for class class_name. This uses animation that points to a @keyframe named fade_in_name. You can name the keyframe anything unique.\n.class_name {\ranimation: fade_in_name 1s;\r}\r@keyframes fade_in_name {\r0% { opacity: 0; }\r100% { opacity: 1; }\r}\r","date":"2023-01-05","id":52,"permalink":"/docs/web-dev/css/","summary":"Positioning position: static; - default when not specified Not affected by top, bottom, left, or right properties Not positioned in any special way; positioned in line with normal page flow position: relative; - positioned relative to its normal position When you want to move an element; use top, bottom, left, or right absolute - gets positioned relative to closest positioned ancestor (not viewport fixed) Notes moves with page scrolling, overlaps other divs If no positioned ancestors, it uses the document body and moves with page scrolling Can use top, bottom, left, or right position: fixed; - gets positioned relative to the viewport Notes: moves with page scrolling Can use top, bottom, left, or right position: sticky; - gets positioned based on the user\u0026rsquo;s scroll position Uses a combination of relative and fixed Use when you want to have an element appear, scoll up to the top, then stay sticky as you continue scrolling down Test","tags":[],"title":"CSS"},{"content":"Postgres Data Types Type Code Desc. Boolean BOOLEAN or BOOL 0, true, t, yes, y evaluates to true. 1, false, f, no, n evaluates to false. Charachter CHAR(n) Fixed length string. Strings less than n are padded with spaces. Charachter VARCHAR(n) Variable length string. Limited to up to n charachters. Charachter VARCHAR Variable length string. Unlimited charachters. Charachter TEXT Variable length string. Unlimited length. Small Integer SMALLINT 2-byte signed integer from -32,768 to 32,767 Integer INT or INTEGER 4-byte singed integer from -2,147,483,648 to 2,147,483,647 Serial SERIAL Similar to integer, but Postgres will automatically generate and populate the value. Float FLOAT(n) Floating-point number with precision of n. Maximum of 8 bytes. Float real or float8 4-byte floating point number Float numeric or numeric(p,s) Real number with p digits and s number after the decimal place. Date DATE Date only. Date TIME Time only. (without timezone) Date TIME WITH TIMEZONE Time only. Includes timezone. Date TIMESTAMP Time and date. Date TIMESTAMPZ Time and date with timezone. Date INTERVAL Periods of time. UUID UUID RFC 4122 compliant UUIDs. JSON JSON Plain JSON types that require for each processing. JSON JSONB Plain JSON types that are faster to inster but slower to insert. Supports indexing. Array ??? Plain JSON types that are faster to inster but slower to insert. Supports indexing. Boolean Choosing boolean is easy because there\u0026rsquo;s only one BOOLEAN dataype. Choose boolean or do not.\nBoolean can represent TRUE, FALSE, or NULL 0, true, t, yes, y evaluates to true 1, false, f, no, n evaluates to false String Data Types Datatypes VARCHAR(n), CHARACTER(n), TEXT Accepts Text: hello, Hello!!1!, 2412 Fidelity Stored exactly Use When storing numbers and letters Avoid When storing numbers only Which VARCHAR(n) for fixed length, TEXT for unlimited length VARCHAR vs CHAR vs TEXT Datatype Length VARCHAR(n) Variable length, with maximum length specified VARCHAR Variable length, unlimited size CHAR(n) Fixed length, with maximum length specified, blank spaces padded to reach charachter limit CHAR Equal to CHAR(1) TEXT Variable length, unlimited size (same as VARCHAR) VARCHAR(n) allows for a variable length string VARCHAR(n) where n is a positive integer that sepcifies the maximum number of charachters (not bytes) Inputs over the charachter length are rejected by the database Inputs under the charachter length are added at the same length as the string CHAR(n) allows for a fixed length string CHAR(n) where n is a positive integer that sepcifies the maximum number of charachters (not bytes) Inputs over the charachter length are rejected by the database Inputs under the charachter length are padded with blank spaces to reach the charachter limit, then stored and displayed that way TEXT allows for a variable length string with no charachter limit General Notes:\nThe longest possible charachter string that can be stored is about 1GB. If storing massive strings, omit the maximum charachter limit and use VARCHAR or TEXT. VARCHAR(n), CHAR(n), and TEXT are all stored the same way under the hood. Using VARCHAR over TEXT is a matter of preference; there is no functional difference between the two. Number Data Types Integers Integer datatypes allow for storing of integers (whole numbers), with variable precision, and are stored exactly.\nSMALLINT, INTEGER, BIGINT Integers only: -64, 0, 2214555746 W a a a a When storing integers (no decimals) When storing decimals INTEGER is good, safe option\nDatatypes SMALLINT, INTEGER, BIGINT Accepts Integers only: -64, 0, 2214555746 Fidelity Stored exactly Use When storing integers (no decimals) Avoid When storing decimalsy values Which INTEGER is good, safe option` If storing integers, choosing the INTERGER datatype is the best option. SMALLINT can be reservered where disk space is urgent.\ntype short code SMALLINT -32768 to 32767 INTEGER -2147483648 to 2147483647 BIGINT -9223372036854775808 to 9223372036854775807` Arbitrary Precision Arbitrary datatypes like NUMERIC can accept decimals, with variable-precision, and are stored exactly.\nDatatypes NUMERIC / DECIMAL Accepts integers and decimals: -832.23, 0, 4.99, 45 Fidelity are stored exactly, at the expense of compute and calculation time Use when exactness is required, like monetary amounts Avoid when accuracy and precision is not required as calculations are much slower compared to other number datatypes Which NUMERIC is the only option here Specifiy limits on precision The NUMERIC dataype allows for arbitrary precision, but the maximum scale and precision can also be defined.\nNUMERIC(precision, scale) - where precision is total number of digits and scale is the number of digits after the decimal place\nIf precision is ommited, any precision and scale is allowed up to bounds of NUMERIC dataype NUMERIC is bounded to 131,072 digits (precision) and 16,383 after the decimal (scale). Floating Point Types Floating point datatypes like REAL and DOUBLE PRECISION are inexact, variable-precision datatypes.\nDatatypes REAL/FLOAT4, DOUBLE PRECISION/FLOAT8 Accepts integers and decimals: -832.23, 0, 4.99, 45 Values are stored exactly, at the expense of compute and calculation time Use When exactness is not required and data contains decimals Avoid When accuracy and precision is required, e.g., monetary values Which REAL for less precision, otherwise DOUBLE PRECISION Variable-precision mean the precision, or length, or the data can vary, e.g., (123, 3.5121, 2999410248746)\nAvoid when exactness is important Avoid using floating point when the exactness of the values is important, like dealing with money.\nInexact means the data is stored as an approximation to reduce size and boost calculation speed. The inexactness of the data used in the calculations means you start get stuff like 2 + 2 = 4.0000000001.\nDo not use if performing complex calculations for anything of importance. Equality comparisons between floating point data may not always operate as expected.\nSpecify limits on precision FLOAT(p) where p is precision in binary digits (max of 8) Floating point dataypes can also store 'Inifinity', '-Infinity', 'Nan' REAL has minimum precision of 6 digits and DP has minimum of 15 Float:\nExamples: REAL/FLOAT4, DOUBLE PRECISION/FLOAT8 Range: The range varies with precision, as precision increases the range drops test\nShort Codes type short code SMALLINT INT2 INTEGER INT4, INT BIGINT INT8 SMALLSERIAL SERIAL2 SERIAL SERIAL4 BIGSERIAL SERIAL8 NUMERIC DECIMAL REAL FLOAT4 DOUBLE PRECISION FLOAT8 BOOLEAN BOOL TIME WITH TIMEZONE TIMEZ TIMESTAMP WITH TIMEZONE TIMESTAMPZ ","date":"2023-01-05","id":53,"permalink":"/docs/postgres/postgres-dataypes/","summary":"Postgres Data Types Type Code Desc. Boolean BOOLEAN or BOOL 0, true, t, yes, y evaluates to true. 1, false, f, no, n evaluates to false.","tags":[],"title":"Postgres Dataypes"},{"content":"SQL Structured Query Language (SQL) is a language for interacting with databases. It defines the syntax for querying data and mantaining the database using statements like SELECT FROM and ADD COLUMN.\nSQL is used in Relational Database Management Systems (RDMS) that uses \u0026ldquo;structured\u0026rdquo; or \u0026ldquo;relational\u0026rdquo; data, unlike the unstructured data set seen in NoSQL databases. SQL is a language; PostgreSQL is a RDMS.\nThe SQL standard defines the core language; however, many SQL implementations add additional functionality and are thus incompatable with one another.\nStructured Data Is a data model where real world entities and heiarchies are mapped to an abstract model. For example, a database can contain a list of students and a list of classes and use a third table to map a student to class.\nRelational database management systems A RDMS is require to actually run a databse. Each RDMS implements the core SQL standard, but vendor\u0026rsquo;s procedural extensions add additional functionalities that are often incompatable with each other.\nSQL Server is Microsoft\u0026rsquo;s RDMS offering. SQL Server transact-SQL, which extends SQL and provides additional query commands. These commands would be incompatable on, say, PostgreSQL. SQL Server also requires a paid license to use where Postgres\nPostgres\nSELECT column FROM table LIMIT 5;\rSQL Server\nSELECT TOP 5 column FROM table;\rCommon relational database management systems PostgreSQL - common FOSS SQL Server - Microsoft\u0026rsquo;s RDMS using tSQL, Microsoft\u0026rsquo;s implementation of SQL MySQL - open source, aquired by Oracle MariaDB - fork of MySQL from 2009 when it was aquired by Oracle Basic SQL Create Table CREATE TABLE table_name ( column datatype );\rCREATE TABLE students ( studentId INT, lastName VARCHAR(255), firstName VARCHAR(255), graduated BOOLEAN );\rDrop Table DROP TABLE table_name;\rAlter Table Add Column ALTER TABLE table_name ADD column_name datatype;\r","date":"2023-01-04","id":54,"permalink":"/docs/postgres/sql-crash-course/","summary":"SQL Structured Query Language (SQL) is a language for interacting with databases. It defines the syntax for querying data and mantaining the database using statements like SELECT FROM and ADD COLUMN.","tags":[],"title":"SQL Crash Course"},{"content":"psql is a CLI front end for postgres. Use it to view and maintain databases or run SQL commands directly.\nInstall PSQL psql is not installed natively.\nMacOS brew update brew install libpq # create symlink for psql and related tools to /usr/local/bin brew link --force libpq\rLogging into psql psql -h localhost -U postgres -p 5432 # login if you don\u0026#39;t know any db name psql -U postgres # login if you know the db name (login as current user) psql databse_name # login to given database as given user psql databse_name -U user_name # all of the flags psql -h hostname -U username -p port -d db_name # use connection string to login psql postgres://USERNAME:PASSWORD@DATABASE_URL:5432/DATABSE_NAME psql postgres://some_user@localhost:5432/db_name psql postgres://some_user:secret_pass@10.0.0.5:5432/db_name # specify username and database when logging in psql -U USERNAME DATABASE_NAME\rBe default, when specifying a username, Postgres will attempt to connect a database with the same name. If the database does not exist, the command will fail saying FAIL: database \u0026lt;USER\u0026gt; does not exist.\nIn this case specify a database name using the command above. If you do not know the database names, log in using the postgres user first, with psql -u postgres.\nBasic psql commands # LISTING DATABASES # list databases \\l # list database with additional info \\l+ # CONNECTING TO DATABASES # connect to database \u0026#39;users\u0026#39; \\c users # see current connection (auto connects to postgres if not connected) \\c # TABLES # list tables \\dt # list tables and sequences \\d # list tables and sequences with additional info \\d+ # describe a table \\d orders # describe table with add;t into \\d+ orders # list a tables relations \\dt orders # list schemas \\dn orders \\dn+ orders # quit shell \\q # open text editors \\e # see users \\du CREATE USER username;\rUsing psql with schemas # list schemas \\dn # list table within a schema \\dt schema_name.\u0026lt;TABLE NAME\u0026gt; # list all tables within a schema \\dt schema_name.* # List all tables in all schemas \\dt *.* # select data from a table in schema SELECT * FROM schema_name.table_name;\rHomebrew Postgres brew info postgres brew info postgresql brew services start postgresql@14\r","date":"2023-01-04","id":55,"permalink":"/docs/postgres/psql-cli/","summary":"psql is a CLI front end for postgres. Use it to view and maintain databases or run SQL commands directly.","tags":[],"title":"PSQL CLI"},{"content":"","date":"2023-01-04","id":56,"permalink":"/docs/postgres/","summary":"","tags":[],"title":"Postgres"},{"content":"ES6 Things The Spread Operator const updatedArray = [...previousState, newElement];\rThe Optional Operator // Deeply nested fields got ya down? const value = deeply.nested.field; // Keep it optional const value = deeply?.nested?.field;\rBuiltin Methods JavaScript (ES6) has a wealth of builtin method helpers.\nNumber Methods Returning numbers:\nMethod Purpose parseInt(str) accept string, return whole number; rounds down, spaces ok but only returns first number parseFloat(str) accepts string, returns number; spaces ok but only returns first number Number(n) number method returns a number converted from its argument Returning string:\nMethod Purpose num.toFixed(n) called on number, returns string of number, accepts optional argument to set number of decimals. returns string formatted with supplied number of decimals to the right; rounds properly, good for currency num.toPrecision(n) called on number, returns string, accepts optional number to specify significant figures num.toExponential(n) called on number, returns string, accepts optional argument to set exponential notation num.toString() called on number, returns string String Methods Method Purpose str[n] return nth characther of str at given index str.chartAt(n) returns charachter at the specified index str.length returns legnth of string Modifying strings\nMethod Purpose str.concat(str2, strN) combines text of two or more strings str.toLowerCase() lowercase entire string str.toUpperCase() - uppercase entire string str.split(\u0026quot;-\u0026quot;) split string into array of substrings separated by supplied argument Finding things in a string\nMethod Purpose str.replace(\u0026quot;stringForDeletion\u0026quot;, \u0026quot;replacementString\u0026quot;) finds match between regex and a string, replaces matched subscrting with new string. str.includes(\u0026quot;search text\u0026quot;) returns true if search text is found in searched string str.indexOf(\u0026quot;search text\u0026quot;) returns the index of the first occurance of specified string str.lastIndexOf(\u0026quot;search text\u0026quot;) returns the index of thelast occurance of specified string ot including) position str.slice(5, 10) extracts part of string from old string. Takes two arguments start and end (but not including) position Array Methods Method Purpose arr.pop() remove last element from array arr.push(x) adds element to end of array arr.shift() removes (or shifts out) first element from arrray arr.unshift(x) adds element to beginning of array Changing elements\nMethod Purpose arr[5] = \u0026quot;somethingNew\u0026quot; set or change sixth element of array arr[arr.length] = \u0026quot;anotherOne\u0026quot; use length to easily append new elements to the end of an array delete arr[5] delete element at index five, length unchanged; dirty arr.splice(5, 1) remove element 5 from array, reduce length by one; clean Sorting \u0026amp; Interating Arrays\nMethod Purpose arr.find(x) return first element in array that matches supplied criteria arr.reduce() runs a function on each element in array to reduce it to single value arr.filter() creates a new array with elements that pass a test arr.map() creates a new array by performing function on each array element arr.forEach() calls function, specifically a callback function, once for each array element arr.sort() sorts an array alphabetically; mutates existing array arr.reverse() reverses an array; mutates existing array arr.every() checks if every element in array passes a test, returns true/false arr.some() checks if some elements in an array pass a test, returns true/false Flush out laster\nindexOf str.substr(7, 6) str.substring(5, 10) str.trim() - removes whitespace on boths sides of string match toString Number Methods: Full Reference ParseInt parseInt(n) - parse string to integer\nParses a string and returns a whole number. Accepts string, returns whole number.\nIf passed multiple numbers, only first number returned.\nparseInt(\u0026#34;10\u0026#34;); // returns 10 parseInt(\u0026#34;10.25\u0026#34;); // returns 10 parseInt(\u0026#34;10 200 330\u0026#34;); // returns 10 ToFixed numb.toFixed(n) - formats number with supplied number of decimals to the right\nRounds in the right direction. Called on number, returns string, with optionally supplied number of decimals.\n`toFixed(2) is perfect for working with money\nString Method Reference Length str.length - returns legnth of string\nlet string = \u0026#34;someString\u0026#34;; let stringLength = string.length;\rConcat str.concat(str2) - combines text of two or more strings\nconst combined = string.concat(string2, string3, stringN);\rReplace .replace() - finds match between regex and a string, replaces matched subscrting with new string.\nFind and replace. Returns a new string, doesn\u0026rsquo;t mutate existing array. Can accept regex.\nReplace Single Instance\nlet string = \u0026#34;the good, the bad, and the ugly\u0026#34;; string.replace(\u0026#34;the\u0026#34;, \u0026#34;some\u0026#34;); // =\u0026gt; some good, the bad, and the ugly Replace Multiple Instances\nlet string = \u0026#34;the good, the bad, and the ugly\u0026#34;; string.replace(/the/g, \u0026#34;some\u0026#34;); // =\u0026gt; some good, some bad, and some ugly Regex Tricks\nUse /i flag to remove case sensitivity indexOf and lastIndexOf str.indexOf(\u0026quot;searchText\u0026quot;)- returns the index of the first occurance of specified string\nstr.lastIndexOf(\u0026quot;searchText\u0026quot;)- returns the index of thelast occurance of specified string\nBoth return -1 if text is not found.\nconst str = \u0026#34;green blue orange\u0026#34;; const position = str.indexOf(\u0026#34;orange\u0026#34;); // =\u0026gt; 11 Search Search method is similar to indexOf method.\nconst str = \u0026#34;green blue orange\u0026#34;; const position = str.search(\u0026#34;orange\u0026#34;); // 11 Search vs indexOf\nThey return same value but are not equal.\nsearch() cannot take second position argument indexOf() cannot take powerful search values, e.g., regex Split str.split(\u0026quot;-\u0026quot;) - split string into array of substrings separated by supplied charachter\nlet string = \u0026#34;212-505-1289\u0026#34;; string.split(\u0026#34;-\u0026#34;); // [\u0026#34;212\u0026#34;, \u0026#34;505\u0026#34;, \u0026#34;1289\u0026#34;] Slice str.slice(5, 10) - extracts part of string from old string. Takes two arguments start and end (but not including) position\nlet string = \u0026#34;the good, the bad, and the ugly\u0026#34;; string.slice(5); // =\u0026gt; \u0026#34;od, the bad, and the ugly\u0026#34; string.slice(5, 15); // =\u0026gt; \u0026#34;od, the b\u0026#34; string.slice(-20); // =\u0026gt; \u0026#34;he bad, and the ugly\u0026#34; Match str.match(/regex/g) - search string for given text, returns matches in an array\nUse match to search for instances of something in a string, returns results as array.\nconst matcher = \u0026#34;green green green green blue\u0026#34;; console.log(matcher.match(/gree/g)); // =\u0026gt; [\u0026#34;gree\u0026#34;, \u0026#34;gree\u0026#34;, \u0026#34;gree\u0026#34;, \u0026#34;gree\u0026#34;] Array Methods Delete Method delete arr[0] crudely deletes element out of array\nDeletes an element in an array, leaves an undefined crater behind.\nconst arr = [1, 2, 3]; delete arr[0]; // arr = [undefined, 2, 3] Use pop() or shift() instead.\nSplice (Array) arr.splice(x, y) - add new items to an array\nCalled on array, returns the deleted element(s) in the array, chosen by spplied parameters. Mutates existing array.\nIf splice deletes no elements, an empty array is returned.\nconst codes = [\u0026#34;A\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;E\u0026#34;]; codes.splice(2, 0, \u0026#34;G\u0026#34;, \u0026#34;Z\u0026#34;); // =\u0026gt; [] // Returns empty array because nothing was deleted Splice arguments:\n1st arg: (2) position where new elements should be added (spliced in) 2nd arg (0): how many elements should be removed Remaining args (\u0026ldquo;G\u0026rdquo;, \u0026ldquo;Z\u0026rdquo;): new elements to be added Remove single element at given index with Splice // where index is x someArray.splice(x, 1);\rConcat (Array) arr.concat() - merging arrays (concatenating arrays)\nCalled on array, accepts unlimited arrays, returns new array, does not modify existing arrays.\nconst mergedArr = arr1.concat(arr2);\rSlice arr.slice() - cut slice out of existing array without changing it\nCalled on array, returns slice of array from specified params. Does not modify existing array.\nCut slices out of strings. Supplied argument keeps everything from that position to the right.\nThe slice() method creates a new array. Nothing is removed from the source array.\nconst arr = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;]; const slicedArr = arr.slice(2); // =\u0026gt; [\u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;] Sort arr.sort() best effort attempt at sorting elements; mutates array it\u0026rsquo;s called on\nCalled on array, returns array; mutates array it\u0026rsquo;s called on.\nAlphabetical sorting\nlet arr = [\u0026#34;A\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;V\u0026#34;, \u0026#34;D\u0026#34;]; arr.sort(); // ==\u0026gt; [\u0026#34;A\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;V\u0026#34;] Numberic Sorting\nSorting numbers requires more work. Remember this function.\nvar scores = [30, 2, 99, 87, 1, 100, 54]; scores.sort(function (a, b) { return a - b; });\rRandom Order Sorting\nvar points = [40, 100, 1, 5, 25, 10]; points.sort(function (a, b) { return 0.5 - Math.random(); });\rReduce (Array) Count occurances in an array Create an object to display how many times a value occures in an object in an array.\nconst flights = [ { departureAirport: \u0026#34;JFK\u0026#34; } { departureAirport: \u0026#34;IAD\u0026#34; } { departureAirport: \u0026#34;JFK\u0026#34; } ] const airportMap = flights .map((char) =\u0026gt; char.departureAirport) .reduce(reducer, {}); // =\u0026gt; { JFK: 2, IAD: 1 } The Compare Function // sort order, works well for numbers points.sort((a, b) =\u0026gt; a - b ) // another definition function(a, b){return a - b}\rIIFE Immediately nvoked Function Expression (IIFE) is a way to execute code immediately when it\u0026rsquo;s defined.\nThey don\u0026rsquo;t pollute the global object and they\u0026rsquo;re a simple was to isoldate variable declarations.\nIIFE Syntax (function () { // code }); (() =\u0026gt; { // code }){}\rNamed IIFE\nIIFEs can be named - they cannot leak out into the global scope and they cannot be called again.\nOnly work for regular named functions, not arrow functions.\n(function aThing() { // })();\rSnippets Debounce const debounce = (func, wait) =\u0026gt; { let timeout; return function executedFunction(...args) { const later = () =\u0026gt; { clearTimeout(timeout); func(...args); }; clearTimeout(timeout); timeout = setTimeout(laster, wait); }; };\rvar returnedFunction = debounce(function () { // taxing stuff }, 250);\rLegnthy but with Pictures Splice Over Delete The delete method removes an element from an array, but not it\u0026rsquo;s history. An undefined crater is left behind.\nIf you want to remove a single element from an array (not first or last or know).\n","date":"2023-01-04","id":57,"permalink":"/docs/js/js-reference/","summary":"ES6 Things The Spread Operator const updatedArray = [...previousState, newElement];\rThe Optional Operator // Deeply nested fields got ya down? const value = deeply.","tags":[],"title":"Js Reference"},{"content":"Creating Characther Map // List of everything /** * CHARACHTER MAP * * Creating a char map */ const createCharMap = (string) =\u0026gt; { const map = {}; for (let item of string) { if (map[item]) { map[item]++; } else { map[item] = 1; } } return map; }; // fast char map const createCharMap = (string) =\u0026gt; { const map = {}; for (let item of string) { map[item] = map[item] + 1 || 1; } return map; }; // or another way Array Helpers /** * LENGTH OF OBJECT */ const someObject = {}; Object.keys(someObject).length; /** * ARRAY HELPERS */ const students = [ { teacher: \u0026#34;Johnson\u0026#34;, course: \u0026#34;English\u0026#34;, grade: 98, credits: 3 }, { teacher: \u0026#34;Burns\u0026#34;, course: \u0026#34;History\u0026#34;, grade: 92, credits: 3 }, { teacher: \u0026#34;Burns\u0026#34;, course: \u0026#34;Geography\u0026#34;, grade: 84, credits: 3 }, { teacher: \u0026#34;Thatcher\u0026#34;, course: \u0026#34;Southern Etiquitte\u0026#34;, grade: 98, credits: 3 }, ]; /** * ARRAY HELPERS: FIND * * Find returns the first match */ const findMatch = students.find((item) =\u0026gt; { return item.teacher === \u0026#34;Johnson\u0026#34;; }); // return: { teacher: \u0026#34;Johnson\u0026#34;, course: \u0026#34;English\u0026#34;, grade: 98, credits: 3 } // FIND: return teacher only const findCourseByTeacher = () =\u0026gt; { const match = students.find((x) =\u0026gt; { return x.teacher === \u0026#34;Johnson\u0026#34;; }); return match.course; }; // return: \u0026#34;English\u0026#34; /** * ARRAY HELPERS: FILTER * * Finds all matches */ const filter = students.filter((item) =\u0026gt; { return item.teacher === \u0026#34;Burns\u0026#34;; }); // output: // returns [ // { teacher: \u0026#34;Burns\u0026#34;, course: \u0026#34;History\u0026#34;, grade: 92, credits: 3 }, // { teacher: \u0026#34;Burns\u0026#34;, course: \u0026#34;Geography\u0026#34;, grade: 84, credits: 3 } // ] // REDUCE: Get multiple matching values const getCourseArray = (students, teacher) =\u0026gt; { const courses = students.filter((total, item) =\u0026gt; { return item. }) } /** * ARRAY HELPERS: REDUCE * * Explain this better */ // Sum values in array of object // Remember second argument of \u0026#34;0\u0026#34; in this case const sum = students.reduce((total, item) =\u0026gt; { return total + item.credits; }, 0); // WEIGHTED AVERAGE // Average grades based on credit weight const weightedAverage = students.reduce((total, item) =\u0026gt; { return total + item.credits * item.grade; }, 0) / sum; /** * DATA STRUCTURES: QUEUE */ class Queue { constructor() { this.data = [] } add(i) { this.data.unshift(i); } remove() { return this.data.pop(); } } // instantiate the class with const q = new Queue(); q.add(1) q.remove();\r","date":"2023-01-04","id":58,"permalink":"/docs/js/js-cheatsheet/","summary":"Creating Characther Map // List of everything /** * CHARACHTER MAP * * Creating a char map */ const createCharMap = (string) =\u0026gt; { const map = {}; for (let item of string) { if (map[item]) { map[item]++; } else { map[item] = 1; } } return map; }; // fast char map const createCharMap = (string) =\u0026gt; { const map = {}; for (let item of string) { map[item] = map[item] + 1 || 1; } return map; }; // or another way Array Helpers /** * LENGTH OF OBJECT */ const someObject = {}; Object.","tags":[],"title":"Js Cheatsheet"},{"content":"Topics to Learn ES6 ES6, or ES6 compatible, javascript is considered modern javascript. ES6 is a specification of JavaScipt that defines the syntax that is considered valid javascript.\n// old function syntax function convertToCelcius(temp) { return (temp - 32) / 1.8; } // ES6 \u0026#34;fat arrow\u0026#34; syntax const convertToCelcius = (temp) =\u0026gt; { return (temp - 32) / 1.8; };\rES6 introduces a way to\nThe basics Declaring variables When to use let and const Don\u0026rsquo;t use var - this is an older syntax and can lead to \u0026ldquo;scope\u0026rdquo; issues Data Types Defining and calling functions Know what a return value is and how to use it Know how to declare a function, understand the different types of function declarations Flow control (if statements) Know the parts of an if statement if, if else, else Loops Understan Throwing and catching errors Declaring Variables // declare a variable that cannot change const speedLimit = 60; // declare a variable that can change let currentSpeed = 45; // currentSpeed can be changed currentSpeed = 35; currentSpeed = currentSpeed + 10; // constants cannot be reassigned speedLimit = 70; // ⚠️ this will throw a TypeError speedLimit = speedLimit + 10; // ⚠️ this will throw a TypeError Variables can be scoped, meaning they can be seen (at not seen) at various levels in the code.\nlet printValue; doubleNumber(7); function doubleNumber(numb) { // declare new variable in function let result = numb * 2; // set previously declared variable to `result` // `printValue` was defined at the global scope and can be read inside this function printValue = result; // `result` can be accessed inside this function console.log(result); // returns 14 } // `printValue` is set at the same scope and be read here console.log(printValue); // returns: 14 // `result` only exists in the function scope and cannot be read here console.log(result); // returns: undefined Javascript data types // \u0026#34;primitive\u0026#34; data types const aString = \u0026#34;hello\u0026#34;; const aNumber = 1; const aBoolean = true; // can be \u0026#39;true\u0026#39;, \u0026#39;false\u0026#39;, or \u0026#39;null\u0026#39; // more advnaced data types const anArray = [\u0026#34;apple\u0026#34;, 1, true]; console.log(anArray); // [\u0026#34;apple\u0026#34;, 1, true] console.log(anArray[0]); // \u0026#34;apple\u0026#34; console.log(anArray[1]); // 1 console.log(anArray[2]); // true // objects const anObject = { size: \u0026#34;large\u0026#34;, color: \u0026#34;green\u0026#34;, inStock: true, }; console.log(anObject.size); // \u0026#34;large\u0026#34; // an array of objects const shirtList = [ { size: \u0026#34;large\u0026#34;, color: \u0026#34;green\u0026#34;, inStock: true, }, { size: \u0026#34;medium\u0026#34;, color: \u0026#34;blue\u0026#34;, inStock: false, }, ]; console.log(shirtList[0].inStock); // true Mutable data types: In JavsScript, arrays and objects are mutable.\nlet firstArray = [1, 2, 3]; let secondArray = firstArray; // remove last element with JavaScript builtin pop firstArray.pop(); console.log(firstArray); // [1, 2] console.log(secondArray); // [1, 2] /** * when creating `secondArray` JS merely saved a reference to `firstArray` * when `firstArray` changed, `secondArray` did too. * Use destructuring assignment of ES6 to make a clone of the array */ let thirdArray = [1, 2, 3]; let fourthArray = [...thirdArray]; // remove last element with JavaScript builtin pop thirdArray.pop(); console.log(thirdArray); // [1, 2] console.log(fourthArray); // [1, 2, 3] /** * Save the output of pop */ // pop element of array and save to variable let fifthArray = [1, 2, 3]; let lastValue = fifthArray.pop(); console.log(fifthArray); // [1, 2] console.log(lastValue); // 3 /** * More array functions * pop - remove from the end * push - append to the end * shift - remove first element * unshift - append to the beginning */ let sixthArray = [1, 2, 3]; sixthArray.pop(); // [1, 2] sixthArray.push(4); // [1, 2, 4] sixthArray.shift(); // [2, 4] sixthArray.unshift(5); // [5, 2, 4] // Remember, the order of arrays are important // count array length console.log(sixthArray.length()); // 3 Functions // declaring a function function somFunction(text) { console.log(test); } // calling a function someFunction(\u0026#34;Hello world\u0026#34;);\rFunctions will normally return a value.\nfunction doubleNumber(int) { return int * 2; } // set variable equal to function result. let myNumber = doubleNumber(3); console.log(myNumber); // should print \u0026#39;6\u0026#39;; Code Snippets Get last element after given character.\nconst name = link.split(\u0026#34;/\u0026#34;).pop(); // Get last three elements of array const path = link.split(\u0026#34;/\u0026#34;).slice(Math.max(link.legnth - 3, 0));\r/** * */\r","date":"2023-01-04","id":59,"permalink":"/docs/js/basic-js/","summary":"Topics to Learn ES6 ES6, or ES6 compatible, javascript is considered modern javascript. ES6 is a specification of JavaScipt that defines the syntax that is considered valid javascript.","tags":[],"title":"Basic Js"},{"content":"Stanard Linux File Structure /bin\nEssential binaries for the system to run. Applications like Chrome would be stored in /usr/bin, while something like Bash would be stored in /bin.\n/boot\nStatic boot files. The GRUB boot loader\u0026rsquo;s files and Linux kernel are stored here. Boot loader config file are stored in /etc with the other configuration files.\n/cdrom - Old mount point for CD drives\nWhile not part of the FHS standard, it still appears on Ubuntu and other distros. The standard location for media, however, is the /media directory.\n/dev - Device files\nLinux exposes devices like hard drives as files. They\u0026rsquo;re not traditional files in the technical sense, but they appear to the user as files. E.g, /dev/sda is the first SATA drive in the system.\n/etc - Configuration files System wide condiguration files. User specific configurations are in the user\u0026rsquo;s home directory.\n/home - Home folder The /home directory contains the home folder for each user. Greg\u0026rsquo;s home folder is found at /home/benish. It contains all the data files and config files for the user. Each user has wirte access to only their home folder and must elevate access to touch other files on the system.\n/lib - Essential shared libraries Contains libraries needed by resources in /bin and /sbin.\n/lost+found - Recovered files Corrupted files recovered if the system crashes.\n/media - Removeable media Contains subdirectories where removeable media devices are mounted.\n/mnt - Temporary mount points Where file systems where typically mounted. They can be mounted anywhere.\n/opt - Optional packages Contains subdirectories for optional software packages. Used by proprietary software that doesn\u0026rsquo;t use the standard file system heiarchy. Programs may dump their files in opt/applicationName when installed.\n/proc - Kernel \u0026amp; process files Similar to the /dev directory, it does not contain standard files. They represent system and process information.\n/root - Root home directory The home directory of the root user. Not located at /home/root. Nor is it /, which is the root directory.\n/sbin - System administration binaries Similar to /bin, it contains binaries for system administration meant to be run by the root user.\n/selinux - SELinux virtual file system If the distro uses SELinux (e.g., RedHat or Fedora) it contains related files.\n/srv - Service data Contains data for services provided by the system. If you were running an Apache web server, you could store the files in the /srv directory.\n/tmp - Temporary files Contains temporary files stored by applications. Generally removed on restart, my be deleted at anytime by utilities like tmpwatch.\n/usr - User binaries and read only data Contains applicaions and files used by the user (instead of by the system). Non-essential applications are located in /usr/bin. Libraries are located in /use/lib. There are a few others.\n/var - Variable data files Contains\n","date":"2023-01-04","id":60,"permalink":"/docs/linux/file-structure/","summary":"Stanard Linux File Structure /bin\nEssential binaries for the system to run. Applications like Chrome would be stored in /usr/bin, while something like Bash would be stored in /bin.","tags":[],"title":"File Structure"},{"content":"Dockerfiles are the instruction set to create a Docker Image.\nDockerfile Basics Dockerfiles are named Dockerfile with no extenstions.\nDockerfiles are built with the docker run command.\nDockerfile crash course: Dockerfile starts with a FROM command Dockerfile ends with a single CMD or ENTRYPOINT command to be executed when the container starts ENTRYPOINT - use for a command that should always be executed when the container starts CMD - use for a command that can be easily overriden when the container starts RUN commands are executed when the Dockerfile is built (used for downloading packages, installing applications, ect.) EXPOSE are used to expose port for process inside the container COPY files from the local file system to the container\u0026rsquo;s file system WORKDIR set working directory execute commands from Simple Dockerfile FROM node:12.4 COPY . /app RUN make /app CMD node /app/main.js\rEach instruction forms one layer:\nFROM - create layer from node:12.4 Docker image COPY - move files into Docker directory RUN - kick off build process using make CMD - specify command to run within container Environment Variables Can be supplied on run\ndocker run -e MY_VAR=hello -it ubuntu:latest bash\rCan be supplied with .env file\nDB_HOST=localhost DB_PASSWORD=secret\rdocker run --env-file .env -it ubuntu:latest bash\rDockerfile Instruction Reference FROM This is the first command in Dockerfile.\n# pull files from Dockerhub registry FROM alpine FROM alpine:latest FROM node:19.12-slim\rSee Docker Hub for all available images\nRUN Executes the commands as a new layer during the building of the image (docker build).\nThis is used for things like downloading packages or installing applications during the building of the image.\nThere can be and often are multiple RUN commands in a single Dockerfile.\nRUN echo \u0026#34;This happens during the build\u0026#34; RUN npm install RUN ls -lah \u0026amp;\u0026amp;\\ echo \u0026#34;multi line run statement\u0026#34;\rCMD This is the default command executed when running a container (docker run); it is not executed during the docker build phase when the image is built.\nFor example, if the image was a webserver, the CMD command would start the server when the container is run.\nIt will execute by default, and can be overridden with docker run, unlike ENTRYPOINT Dockerfile\u0026rsquo;s should only include one CMD instruction at the end of the file CMD is used for when a container is started (not during the docker build phase) The CMD command is desgined to be easily overridable.\n# Shell form CMD echo \u0026#34;Hello World\u0026#34; # exec form CMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello World\u0026#34;]\rThe CMD command is overridden when another command is specified with docker run.\n# sample Dockerfile for sample-container\rFROM alpine\rCMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello World\u0026#34;]\r# file running with default `echo Hello World` docker run sample-container =\u0026gt; Hello World # overriding `echo Hello World` with `uname` command docker run sample-container uname =\u0026gt; Darwin\rENTRYPOINT The ENTRYPOINT is used as the executable for the container, executed when running a container (docker run); it is not executed during the docker build phase when the image is built.\nIt will always execute when the container is run, unlike CMD which can be overridden with the docker run command. Dockerfile\u0026rsquo;s should only include one ENTRYPOINT instruction at the end of the file ENTRYPOINT is used for when a container is started (not during the docker build phase). Commands can be passed from CMD to ENTRYPOINT\nCMD [\u0026#34;-v\u0026#34;] ENTRYPOINT [\u0026#34;node\u0026#34;]\rIf additional arguments are added with docker run, the ENTRYPOINT is not ignored. It will always execute.\n# sample Dockerfile FROM alpine CMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello World\u0026#34;]\r# file running with default `echo Hello World` docker run sample-container =\u0026gt; Hello World # `echo Hello World` is not overridden with `uname` command docker run sample-container uname =\u0026gt; HelloWorld Darwin\rWORKDIR Set the working direcotry for all subsequent commands. Multiple WORKDIR commands can be used to update the working direcotry.\n# set working dir to \u0026#34;/opt/north\u0026#34; for subsequent commands WORKDIR /opt/north RUN make north-app # change working dir to \u0026#34;/app\u0026#34; for subsequent commands WORKDIR /app RUN ./app\rEXPOSE Exposes a port for a process inside the container.\nFor exmaple, if a Node app is running inside the container on port 5000, you can expose that port.\nEXPOSE 5000\rENV Set environment variables that can be used during the building of an image or inside a running container.\nTypically API keys, database connection strings, secrets, ect. They can be used for building the image or for a running container Environment variables specified in the Dockerfile are overridden by the docker run -e command Contrast with ARG where vaiables can be used in the build phase only Set environment variables in a Dockerfile:\n# set \u0026#34;workingdirectory\u0026#34; variable to \u0026#34;/opt/north\u0026#34; ENV workingdirectory /opt/north # consume \u0026#34;workingdirectory\u0026#34; variable WORKDIR $workingdirecotry # set build arg ARG NODE_VERSION=18 # set env variable equal to build argument ENV ENV_NODE_VERSION=$NODE_VERSION\rPass environment variables with docker run:\ndocker run -e USERNAME=wyatt -e PASSWORD=hunter2 nginx\rARG Build argments are parameters only available during the build phase.\nTypically package version numbers or other details relevant to the build They are not available when the container is running or once the image is built Build arguments can be specified in the Dockerfile or the docker build command Contrast with ENV where vaiables can be used in the build and run phase ARG NODE_VERSION=18.04 RUN echo \u0026#34;Node version is ${NODE_VERSION}\u0026#34;\rPass environment variables with docker run:\ndocker build --build-args USERNAME=mike --build-args PASSWORD=bestpassword .\rCOPY Copy files from local machine to the container\u0026rsquo;s file system.\n# format COPY location/location/machine ./container/file/system # sample COPY package.json .\rADD Add files or\nADD server.js .\rLABEL Add metadata to an image.\nLABEL \u0026#34;author\u0026#34;=\u0026#34;Greg Benish\u0026#34;\rDocker Concepts Shell Form vs Exec Form The two forms are different.\nShell Form INSTRUCTION COMMAND RUN apt-get update -y COPY ./some/directory CMD echo \u0026#34;Hello Local Host\u0026#34;\rExec Form INSTRUCTION [\u0026#34;EXECUTABLE\u0026#34;, \u0026#34;PARAM_1\u0026#34;, \u0026#34;PARAM_2\u0026#34;] RUN [\u0026#34;apt-get\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;-y\u0026#34;] COPY [\u0026#34;./some/directory\u0026#34;] CMD [\u0026#34;echo\u0026#34;, \u0026#34;Hello Local Host\u0026#34;] # propmt shell processing to substitute variable ENV tester Greg CMD [\u0026#34;echo\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;Hello $tester\u0026#34;]\rCMD vs RUN vs ENTRYPOINT Containers are built to run a single process. When that process completes the container exits.\nRUN is used to run commands like downloading software packages and installing applications. These are executed when the image is built.\nCMD and ENTRYPOINT are the two commands that define what process is running in the container. These are executed when the image is run.\nRUN - executing a command and creates a new layer. Do something like installing software package CMD - default command to execute when running a container without supplying a command. It can be easily overriden. ENTRYPOINT - specify when making a container an executable. This is only overriden by the --entrypoint flag. CMD and ENTRYPOINT - combine when using a container with an executable that needs easily overrideable default values. Using ENV and CMD together How to use environment variables in a CMD command.\n# docker run command (with env variable) $ docker run -e URL_PATH=\u0026#39;example.com\u0026#39;\r# Dockerfile ENV URL_PATH $URL_PATH CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sh ./script.sh ${URL_PATH}\u0026#34; ]\rDockerfiles are named Dockerfile (with no extension) Building an Image from a Dockerifle # build Dockerfile in current directory docker build . # add tag to image docker built -t vendor/image:0.0.1 . # add build-time arguments docker build --build-arg KEY=VALUE . # multiple build-time arguments docker build --build-arg KEY=VALUE --build-arg KEY=VALUE .\r","date":"2023-01-04","id":61,"permalink":"/docs/docker/creating-dockerfiles/","summary":"Reference for writing Dockerfiles to build Docker images.","tags":[],"title":"Creating Dockerfiles"},{"content":"","date":"2024-07-05","id":62,"permalink":"/casino/guides/","summary":"","tags":[],"title":"Guides"},{"content":"","date":"2024-07-05","id":63,"permalink":"/casino/","summary":"","tags":[],"title":"Casino"},{"content":"","date":"2024-07-05","id":64,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"2024-07-05","id":65,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"2024-07-05","id":66,"permalink":"/categories/guides/","summary":"","tags":[],"title":"Guides"},{"content":"","date":"2024-07-05","id":67,"permalink":"/tags/observability/","summary":"","tags":[],"title":"Observability"},{"content":"","date":"2024-07-05","id":68,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"},{"content":"","date":"2024-07-05","id":69,"permalink":"/tags/telegraf/","summary":"","tags":[],"title":"Telegraf"},{"content":"","date":"2024-07-05","id":70,"permalink":"/contributors/wyatt/","summary":"","tags":[],"title":"Wyatt"},{"content":"Test inof\n","date":"2024-06-20","id":71,"permalink":"/casino/faq/","summary":"This is a nested file","tags":[],"title":"Faq"},{"content":"Test inof\n","date":"2024-06-20","id":72,"permalink":"/casino/guides/total/","summary":"This is a summary","tags":[],"title":"Total"},{"content":"","date":"2023-09-07","id":73,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":74,"permalink":"/","summary":"","tags":[],"title":"WyNotes"},{"content":"","date":"2023-01-05","id":75,"permalink":"/docs/web-dev/","summary":"","tags":[],"title":"Web Dev"}]